#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ€§èƒ½ä¼˜åŒ–æ¨¡å— - GPUåŠ é€Ÿå’Œå¹¶è¡Œå¤„ç†ä¼˜åŒ–
"""

import os
import time
import psutil
import multiprocessing as mp
from typing import Dict, List, Any, Optional
import numpy as np


class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å™¨ - GPUåŠ é€Ÿå’Œå¹¶è¡Œå¤„ç†"""
    
    def __init__(self):
        self.cpu_count = mp.cpu_count()
        self.memory_info = psutil.virtual_memory()
        self.gpu_available = self._check_gpu_availability()
        
        print(f"ğŸ”§ æ€§èƒ½ä¼˜åŒ–å™¨åˆå§‹åŒ–")
        print(f"   CPUæ ¸å¿ƒæ•°: {self.cpu_count}")
        print(f"   å†…å­˜æ€»é‡: {self.memory_info.total / (1024**3):.1f}GB")
        print(f"   GPUå¯ç”¨: {'æ˜¯' if self.gpu_available else 'å¦'}")
    
    def _check_gpu_availability(self) -> bool:
        """æ£€æŸ¥GPUå¯ç”¨æ€§"""
        try:
            import torch
            if torch.cuda.is_available():
                gpu_count = torch.cuda.device_count()
                gpu_name = torch.cuda.get_device_name(0) if gpu_count > 0 else "Unknown"
                print(f"   GPUè®¾å¤‡: {gpu_name} (æ•°é‡: {gpu_count})")
                return True
            else:
                print("   GPUè®¾å¤‡: æœªæ£€æµ‹åˆ°CUDAè®¾å¤‡")
                return False
        except ImportError:
            print("   GPUè®¾å¤‡: PyTorchæœªå®‰è£…")
            return False
    
    def optimize_for_algorithm(self, algorithm_name: str, data_size: int) -> Dict[str, Any]:
        """ä¸ºç‰¹å®šç®—æ³•ä¼˜åŒ–æ€§èƒ½é…ç½®"""
        config = {
            'use_gpu': False,
            'batch_size': 32,
            'num_workers': 1,
            'memory_limit': None,
            'parallel_processes': 1
        }
        
        # åŸºäºç®—æ³•ç±»å‹ä¼˜åŒ–
        if algorithm_name in ['transformer', 'gnn', 'lstm']:
            # æ·±åº¦å­¦ä¹ ç®—æ³•
            config['use_gpu'] = self.gpu_available
            config['batch_size'] = min(64, max(16, data_size // 10))
            config['num_workers'] = min(4, self.cpu_count)
            
        elif algorithm_name in ['monte_carlo', 'advanced_ensemble']:
            # è®¡ç®—å¯†é›†å‹ç®—æ³•
            config['parallel_processes'] = min(self.cpu_count, 8)
            config['memory_limit'] = self.memory_info.available * 0.7
            
        elif algorithm_name in ['clustering', 'bayesian']:
            # ä¸­ç­‰è®¡ç®—é‡ç®—æ³•
            config['parallel_processes'] = min(self.cpu_count // 2, 4)
            config['batch_size'] = min(128, data_size)
            
        elif algorithm_name == 'super_predictor':
            # è¶…çº§é¢„æµ‹å™¨ - ç»¼åˆä¼˜åŒ–
            config['use_gpu'] = self.gpu_available
            config['parallel_processes'] = min(self.cpu_count, 6)
            config['memory_limit'] = self.memory_info.available * 0.8
            config['batch_size'] = 32
        
        print(f"ğŸ¯ {algorithm_name} æ€§èƒ½é…ç½®: {config}")
        return config
    
    def monitor_performance(self, func, *args, **kwargs):
        """æ€§èƒ½ç›‘æ§è£…é¥°å™¨"""
        start_time = time.time()
        start_memory = psutil.virtual_memory().used
        
        # æ‰§è¡Œå‡½æ•°
        result = func(*args, **kwargs)
        
        end_time = time.time()
        end_memory = psutil.virtual_memory().used
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        execution_time = end_time - start_time
        memory_used = (end_memory - start_memory) / (1024**2)  # MB
        
        performance_info = {
            'execution_time': execution_time,
            'memory_used': memory_used,
            'cpu_usage': psutil.cpu_percent(),
            'memory_percent': psutil.virtual_memory().percent
        }
        
        print(f"ğŸ“Š æ€§èƒ½ç›‘æ§ç»“æœ:")
        print(f"   æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
        print(f"   å†…å­˜ä½¿ç”¨: {memory_used:.1f}MB")
        print(f"   CPUä½¿ç”¨ç‡: {performance_info['cpu_usage']:.1f}%")
        print(f"   å†…å­˜ä½¿ç”¨ç‡: {performance_info['memory_percent']:.1f}%")
        
        return result, performance_info
    
    def optimize_memory_usage(self, data_size: int) -> Dict[str, Any]:
        """å†…å­˜ä½¿ç”¨ä¼˜åŒ–"""
        available_memory = self.memory_info.available
        
        # è®¡ç®—æ¨èçš„æ‰¹å¤„ç†å¤§å°
        if data_size * 1000 < available_memory:  # å‡è®¾æ¯ä¸ªæ•°æ®ç‚¹1KB
            batch_size = data_size
            use_chunking = False
        else:
            batch_size = max(10, available_memory // (1000 * 10))
            use_chunking = True
        
        config = {
            'batch_size': batch_size,
            'use_chunking': use_chunking,
            'memory_limit': available_memory * 0.8,
            'gc_frequency': 100 if use_chunking else 1000
        }
        
        return config
    
    def parallel_process_optimization(self, task_count: int, cpu_intensive: bool = True) -> int:
        """å¹¶è¡Œå¤„ç†ä¼˜åŒ–"""
        if cpu_intensive:
            # CPUå¯†é›†å‹ä»»åŠ¡
            optimal_processes = min(self.cpu_count, task_count)
        else:
            # I/Oå¯†é›†å‹ä»»åŠ¡
            optimal_processes = min(self.cpu_count * 2, task_count)
        
        # è€ƒè™‘å†…å­˜é™åˆ¶
        memory_per_process = 500  # MB
        max_processes_by_memory = self.memory_info.available // (memory_per_process * 1024 * 1024)
        
        return min(optimal_processes, max_processes_by_memory, 16)  # æœ€å¤š16ä¸ªè¿›ç¨‹


class SystemBenchmark:
    """ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def __init__(self):
        self.optimizer = PerformanceOptimizer()
    
    def run_benchmark(self) -> Dict[str, float]:
        """è¿è¡Œç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("ğŸƒ å¼€å§‹ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•...")
        
        results = {}
        
        # CPUè®¡ç®—æ€§èƒ½æµ‹è¯•
        results['cpu_performance'] = self._test_cpu_performance()
        
        # å†…å­˜è®¿é—®æ€§èƒ½æµ‹è¯•
        results['memory_performance'] = self._test_memory_performance()
        
        # å¹¶è¡Œå¤„ç†æ€§èƒ½æµ‹è¯•
        results['parallel_performance'] = self._test_parallel_performance()
        
        # GPUæ€§èƒ½æµ‹è¯•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.optimizer.gpu_available:
            results['gpu_performance'] = self._test_gpu_performance()
        
        print("âœ… ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ")
        return results
    
    def _test_cpu_performance(self) -> float:
        """CPUæ€§èƒ½æµ‹è¯•"""
        print("æµ‹è¯•CPUè®¡ç®—æ€§èƒ½...")
        
        start_time = time.time()
        
        # çŸ©é˜µè¿ç®—æµ‹è¯•
        size = 1000
        a = np.random.rand(size, size)
        b = np.random.rand(size, size)
        c = np.dot(a, b)
        
        end_time = time.time()
        cpu_score = 1000 / (end_time - start_time)  # åˆ†æ•°è¶Šé«˜è¶Šå¥½
        
        print(f"   CPUæ€§èƒ½åˆ†æ•°: {cpu_score:.1f}")
        return cpu_score
    
    def _test_memory_performance(self) -> float:
        """å†…å­˜æ€§èƒ½æµ‹è¯•"""
        print("æµ‹è¯•å†…å­˜è®¿é—®æ€§èƒ½...")
        
        start_time = time.time()
        
        # å¤§æ•°ç»„åˆ›å»ºå’Œè®¿é—®æµ‹è¯•
        size = 10000000  # 10M elements
        arr = np.random.rand(size)
        result = np.sum(arr)
        
        end_time = time.time()
        memory_score = size / (end_time - start_time) / 1000000  # MB/s
        
        print(f"   å†…å­˜æ€§èƒ½åˆ†æ•°: {memory_score:.1f} MB/s")
        return memory_score
    
    def _test_parallel_performance(self) -> float:
        """å¹¶è¡Œå¤„ç†æ€§èƒ½æµ‹è¯•"""
        print("æµ‹è¯•å¹¶è¡Œå¤„ç†æ€§èƒ½...")

        # ä¸²è¡Œæµ‹è¯•
        start_time = time.time()
        serial_results = [self._cpu_task(10000) for _ in range(100)]
        serial_time = time.time() - start_time

        # å¹¶è¡Œæµ‹è¯•
        start_time = time.time()
        with mp.Pool() as pool:
            parallel_results = pool.map(self._cpu_task, [10000] * 100)
        parallel_time = time.time() - start_time

        speedup = serial_time / parallel_time if parallel_time > 0 else 1

        print(f"   å¹¶è¡ŒåŠ é€Ÿæ¯”: {speedup:.2f}x")
        return speedup

    def _cpu_task(self, n):
        """CPUå¯†é›†å‹ä»»åŠ¡"""
        return sum(i * i for i in range(n))
    
    def _test_gpu_performance(self) -> float:
        """GPUæ€§èƒ½æµ‹è¯•"""
        print("æµ‹è¯•GPUè®¡ç®—æ€§èƒ½...")
        
        try:
            import torch
            
            device = torch.device('cuda')
            
            start_time = time.time()
            
            # GPUçŸ©é˜µè¿ç®—æµ‹è¯•
            size = 2000
            a = torch.rand(size, size, device=device)
            b = torch.rand(size, size, device=device)
            c = torch.mm(a, b)
            torch.cuda.synchronize()
            
            end_time = time.time()
            gpu_score = 2000 / (end_time - start_time)
            
            print(f"   GPUæ€§èƒ½åˆ†æ•°: {gpu_score:.1f}")
            return gpu_score
            
        except Exception as e:
            print(f"   GPUæµ‹è¯•å¤±è´¥: {e}")
            return 0.0


def optimize_system_performance():
    """ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–...")
    
    optimizer = PerformanceOptimizer()
    benchmark = SystemBenchmark()
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    benchmark_results = benchmark.run_benchmark()
    
    # ç”Ÿæˆä¼˜åŒ–å»ºè®®
    recommendations = []
    
    if benchmark_results.get('cpu_performance', 0) < 100:
        recommendations.append("å»ºè®®å‡çº§CPUæˆ–ä¼˜åŒ–ç®—æ³•å¤æ‚åº¦")
    
    if benchmark_results.get('memory_performance', 0) < 1000:
        recommendations.append("å»ºè®®å¢åŠ å†…å­˜æˆ–ä½¿ç”¨å†…å­˜æ˜ å°„")
    
    if benchmark_results.get('parallel_performance', 1) < 2:
        recommendations.append("å»ºè®®ä¼˜åŒ–å¹¶è¡Œç®—æ³•æˆ–æ£€æŸ¥CPUæ ¸å¿ƒæ•°")
    
    if optimizer.gpu_available and benchmark_results.get('gpu_performance', 0) < 500:
        recommendations.append("å»ºè®®ä¼˜åŒ–GPUç®—æ³•æˆ–å‡çº§æ˜¾å¡")
    
    print("\nğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
    for i, rec in enumerate(recommendations, 1):
        print(f"   {i}. {rec}")
    
    if not recommendations:
        print("   ç³»ç»Ÿæ€§èƒ½è‰¯å¥½ï¼Œæ— éœ€ç‰¹åˆ«ä¼˜åŒ–")
    
    return optimizer, benchmark_results


if __name__ == "__main__":
    optimize_system_performance()
