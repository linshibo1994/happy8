#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«ä¹8æ™ºèƒ½é¢„æµ‹ç³»ç»Ÿ - æ ¸å¿ƒåˆ†æå™¨
Happy8 Prediction System - Core Analyzer

åŸºäºåŒè‰²çƒé¢„æµ‹ç³»ç»Ÿçš„æˆç†Ÿæ¶æ„ï¼Œé€‚é…å¿«ä¹8å½©ç¥¨çš„ç‰¹ç‚¹ï¼š
- å·ç èŒƒå›´: 1-80å·
- å¼€å¥–å·ç : æ¯æœŸå¼€å‡º20ä¸ªå·ç 
- å¼€å¥–é¢‘ç‡: æ¯5åˆ†é’Ÿä¸€æœŸï¼Œæ¯å¤©çº¦288æœŸ

ä½œè€…: CodeBuddy
ç‰ˆæœ¬: v1.0
åˆ›å»ºæ—¶é—´: 2025-08-17
"""

import os
import sys
import time
import json
import pickle
import argparse
import warnings
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Any, Union
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import multiprocessing as mp

import numpy as np
import pandas as pd
import requests
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
import scipy.stats as stats
from scipy.spatial.distance import pdist, squareform

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings('ignore')
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# å°è¯•å¯¼å…¥æ·±åº¦å­¦ä¹ åº“
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers, models, optimizers, callbacks
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding, MultiHeadAttention
    TF_AVAILABLE = True
    
    # GPUé…ç½®
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print(f"æ£€æµ‹åˆ° {len(gpus)} ä¸ªGPUè®¾å¤‡ï¼Œå·²å¯ç”¨GPUåŠ é€Ÿ")
        except RuntimeError as e:
            print(f"GPUé…ç½®å¤±è´¥: {e}")
    else:
        print("æœªæ£€æµ‹åˆ°GPUè®¾å¤‡ï¼Œä½¿ç”¨CPUè®¡ç®—")
        
except ImportError:
    TF_AVAILABLE = False
    print("TensorFlowæœªå®‰è£…ï¼Œæ·±åº¦å­¦ä¹ åŠŸèƒ½å°†ä¸å¯ç”¨")

# å°è¯•å¯¼å…¥é«˜çº§åº“
try:
    import optuna
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False

try:
    import ray
    RAY_AVAILABLE = True
except ImportError:
    RAY_AVAILABLE = False


@dataclass
class Happy8Result:
    """å¿«ä¹8å¼€å¥–ç»“æœæ•°æ®æ¨¡å‹"""
    issue: str                    # æœŸå· (å¦‚: "20250813001")
    date: str                     # å¼€å¥–æ—¥æœŸ (å¦‚: "2025-08-13")
    time: str                     # å¼€å¥–æ—¶é—´ (å¦‚: "09:05:00")
    numbers: List[int]            # å¼€å¥–å·ç  (20ä¸ªæ•°å­—)
    
    def __post_init__(self):
        """æ•°æ®éªŒè¯"""
        if len(self.numbers) != 20:
            raise ValueError(f"å¼€å¥–å·ç å¿…é¡»æ˜¯20ä¸ªï¼Œå®é™…: {len(self.numbers)}")
        if not all(1 <= num <= 80 for num in self.numbers):
            raise ValueError("å¼€å¥–å·ç å¿…é¡»åœ¨1-80èŒƒå›´å†…")
        if len(set(self.numbers)) != 20:
            raise ValueError("å¼€å¥–å·ç ä¸èƒ½é‡å¤")
    
    @property
    def number_sum(self) -> int:
        """å·ç æ€»å’Œ"""
        return sum(self.numbers)
    
    @property
    def number_avg(self) -> float:
        """å·ç å¹³å‡å€¼"""
        return self.number_sum / 20
    
    @property
    def number_range(self) -> int:
        """å·ç è·¨åº¦"""
        return max(self.numbers) - min(self.numbers)
    
    @property
    def odd_count(self) -> int:
        """å¥‡æ•°ä¸ªæ•°"""
        return sum(1 for n in self.numbers if n % 2 == 1)
    
    @property
    def big_count(self) -> int:
        """å¤§å·ä¸ªæ•° (41-80)"""
        return sum(1 for n in self.numbers if n >= 41)
    
    @property
    def zone_distribution(self) -> List[int]:
        """åŒºåŸŸåˆ†å¸ƒ (1-80åˆ†ä¸º8ä¸ªåŒºåŸŸ)"""
        zones = [0] * 8
        for num in self.numbers:
            zone_idx = (num - 1) // 10
            zones[zone_idx] += 1
        return zones
    
    @property
    def consecutive_count(self) -> int:
        """è¿å·ä¸ªæ•°"""
        sorted_nums = sorted(self.numbers)
        consecutive = 0
        for i in range(1, len(sorted_nums)):
            if sorted_nums[i] == sorted_nums[i-1] + 1:
                consecutive += 1
        return consecutive


@dataclass
class PredictionResult:
    """é¢„æµ‹ç»“æœæ•°æ®æ¨¡å‹"""
    target_issue: str             # ç›®æ ‡æœŸå·
    analysis_periods: int         # åˆ†ææœŸæ•°
    method: str                   # é¢„æµ‹æ–¹æ³•
    predicted_numbers: List[int]  # é¢„æµ‹å·ç 
    confidence_scores: List[float] # ç½®ä¿¡åº¦åˆ†æ•°
    generation_time: datetime     # ç”Ÿæˆæ—¶é—´
    execution_time: float         # æ‰§è¡Œè€—æ—¶
    parameters: Dict[str, Any]    # ç®—æ³•å‚æ•°
    
    @property
    def top_numbers(self) -> List[int]:
        """æŒ‰ç½®ä¿¡åº¦æ’åºçš„å‰20ä¸ªå·ç """
        if len(self.confidence_scores) != len(self.predicted_numbers):
            return self.predicted_numbers[:20]
        
        paired = list(zip(self.predicted_numbers, self.confidence_scores))
        sorted_pairs = sorted(paired, key=lambda x: x[1], reverse=True)
        return [num for num, _ in sorted_pairs[:20]]


@dataclass
class ComparisonResult:
    """å¯¹æ¯”ç»“æœæ•°æ®æ¨¡å‹"""
    target_issue: str             # ç›®æ ‡æœŸå·
    predicted_numbers: List[int]  # é¢„æµ‹å·ç 
    actual_numbers: List[int]     # å®é™…å¼€å¥–å·ç 
    hit_numbers: List[int]        # å‘½ä¸­å·ç 
    miss_numbers: List[int]       # æœªå‘½ä¸­å·ç 
    hit_count: int               # å‘½ä¸­æ•°é‡
    total_predicted: int         # é¢„æµ‹æ€»æ•°
    hit_rate: float             # å‘½ä¸­ç‡
    hit_distribution: Dict[str, int]  # å‘½ä¸­åˆ†å¸ƒåˆ†æ
    comparison_time: datetime    # å¯¹æ¯”æ—¶é—´
    
    def generate_report(self) -> str:
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        return f"""
å¯¹æ¯”ç»“æœæŠ¥å‘Š
============
ç›®æ ‡æœŸå·: {self.target_issue}
é¢„æµ‹æ•°é‡: {self.total_predicted}
å‘½ä¸­æ•°é‡: {self.hit_count}
å‘½ä¸­ç‡: {self.hit_rate:.2%}

å‘½ä¸­å·ç : {sorted(self.hit_numbers)}
æœªå‘½ä¸­å·ç : {sorted(self.miss_numbers)}

è¯¦ç»†åˆ†æ:
- å°å·å‘½ä¸­: {sum(1 for n in self.hit_numbers if n <= 40)}ä¸ª (1-40å·æ®µ)
- å¤§å·å‘½ä¸­: {sum(1 for n in self.hit_numbers if n >= 41)}ä¸ª (41-80å·æ®µ)
- å¥‡æ•°å‘½ä¸­: {sum(1 for n in self.hit_numbers if n % 2 == 1)}ä¸ª
- å¶æ•°å‘½ä¸­: {sum(1 for n in self.hit_numbers if n % 2 == 0)}ä¸ª
        """


class DataValidator:
    """æ•°æ®éªŒè¯å™¨"""
    
    @staticmethod
    def validate_happy8_data(data: pd.DataFrame) -> Dict[str, Any]:
        """éªŒè¯å¿«ä¹8æ•°æ®"""
        results = {
            'total_records': len(data),
            'missing_values': {},
            'invalid_ranges': 0,
            'duplicate_issues': 0,
            'invalid_number_counts': 0,
            'errors': []
        }
        
        # æ£€æŸ¥å¿…è¦åˆ—ï¼ˆç§»é™¤timeåˆ—ï¼‰
        required_cols = ['issue', 'date'] + [f'num{i}' for i in range(1, 21)]
        missing_cols = [col for col in required_cols if col not in data.columns]
        if missing_cols:
            results['errors'].append(f"ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
            return results
        
        # æ£€æŸ¥ç¼ºå¤±å€¼
        for col in required_cols:
            missing_count = data[col].isnull().sum()
            if missing_count > 0:
                results['missing_values'][col] = missing_count
        
        # æ£€æŸ¥å·ç èŒƒå›´
        number_cols = [f'num{i}' for i in range(1, 21)]
        for col in number_cols:
            invalid_range = ((data[col] < 1) | (data[col] > 80)).sum()
            results['invalid_ranges'] += invalid_range
        
        # æ£€æŸ¥é‡å¤æœŸå·
        results['duplicate_issues'] = data['issue'].duplicated().sum()
        
        # æ£€æŸ¥æ¯æœŸå·ç æ•°é‡
        for idx, row in data.iterrows():
            numbers = [row[f'num{i}'] for i in range(1, 21)]
            if len(set(numbers)) != 20:
                results['invalid_number_counts'] += 1
        
        return results


class Happy8Crawler:
    """å¿«ä¹8æ•°æ®çˆ¬è™«"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        self.session.timeout = 30
    
    def crawl_recent_data(self, count: int = 1000) -> List[Happy8Result]:
        """çˆ¬å–æœ€è¿‘çš„å¼€å¥–æ•°æ®"""
        print(f"å¼€å§‹çˆ¬å–æœ€è¿‘ {count} æœŸå¿«ä¹8æ•°æ®...")
        
        results = []
        
        # å°è¯•å¤šä¸ªæ•°æ®æº
        data_sources = [
            self._crawl_from_500wan,
            self._crawl_from_zhcw,
            self._crawl_from_lottery_gov
        ]
        
        for crawl_func in data_sources:
            try:
                print(f"å°è¯•æ•°æ®æº: {crawl_func.__name__}")
                results = crawl_func(count)
                if results:
                    print(f"æˆåŠŸä» {crawl_func.__name__} è·å– {len(results)} æœŸæ•°æ®")
                    break
            except Exception as e:
                print(f"æ•°æ®æº {crawl_func.__name__} å¤±è´¥: {e}")
                continue
        
        if not results:
            print("æ‰€æœ‰æ•°æ®æºéƒ½å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ¡ˆ...")
            results = self._crawl_backup_data(count)
        
        return results
    
    def _crawl_from_500wan(self, count: int) -> List[Happy8Result]:
        """ä»500å½©ç¥¨ç½‘çˆ¬å–æ•°æ®"""
        results = []
        
        # 500å½©ç¥¨ç½‘å¿«ä¹8å†å²æ•°æ®æ¥å£
        base_url = "https://www.500.com/kl8/kaijiang.php"
        
        try:
            # è·å–æœ€æ–°æ•°æ®é¡µé¢
            response = self.session.get(base_url)
            response.raise_for_status()
            response.encoding = 'gb2312'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾å¼€å¥–æ•°æ®è¡¨æ ¼
            table = soup.find('table', {'class': 'kj_tablelist02'})
            if not table:
                raise Exception("æœªæ‰¾åˆ°æ•°æ®è¡¨æ ¼")
            
            rows = table.find_all('tr')[1:]  # è·³è¿‡è¡¨å¤´
            
            for row in rows[:count]:
                try:
                    cells = row.find_all('td')
                    if len(cells) < 4:
                        continue
                    
                    # è§£ææœŸå·
                    issue = cells[0].text.strip()
                    
                    # è§£æå¼€å¥–æ—¶é—´
                    date_time = cells[1].text.strip()
                    if ' ' in date_time:
                        date_str, time_str = date_time.split(' ')
                    else:
                        date_str = date_time
                        time_str = "00:00:00"
                    
                    # è§£æå¼€å¥–å·ç 
                    number_cell = cells[2]
                    number_spans = number_cell.find_all('span')
                    
                    if len(number_spans) >= 20:
                        numbers = []
                        for span in number_spans[:20]:
                            num_text = span.text.strip()
                            if num_text.isdigit():
                                numbers.append(int(num_text))
                        
                        if len(numbers) == 20:
                            result = Happy8Result(
                                issue=issue,
                                date=date_str,
                                time=time_str,
                                numbers=sorted(numbers)
                            )
                            results.append(result)
                
                except Exception as e:
                    print(f"è§£æè¡Œæ•°æ®å¤±è´¥: {e}")
                    continue
            
        except Exception as e:
            print(f"500å½©ç¥¨ç½‘çˆ¬å–å¤±è´¥: {e}")
            raise
        
        return results
    
    def _crawl_from_zhcw(self, count: int) -> List[Happy8Result]:
        """ä»ä¸­å½©ç½‘çˆ¬å–æ•°æ®"""
        results = []
        
        # ä¸­å½©ç½‘å¿«ä¹8æ•°æ®æ¥å£
        base_url = "https://www.zhcw.com/kl8/kaijiangshuju/"
        
        try:
            # è®¡ç®—éœ€è¦çˆ¬å–çš„é¡µæ•°
            pages_needed = (count + 19) // 20  # æ¯é¡µ20æ¡æ•°æ®
            
            for page in range(1, min(pages_needed + 1, 51)):  # æœ€å¤šçˆ¬å–50é¡µ
                page_url = f"{base_url}?page={page}"
                
                response = self.session.get(page_url)
                response.raise_for_status()
                response.encoding = 'utf-8'
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æŸ¥æ‰¾æ•°æ®è¡¨æ ¼
                table = soup.find('table', {'class': 'kjjg_table'})
                if not table:
                    continue
                
                rows = table.find_all('tr')[1:]  # è·³è¿‡è¡¨å¤´
                
                for row in rows:
                    if len(results) >= count:
                        break
                    
                    try:
                        cells = row.find_all('td')
                        if len(cells) < 3:
                            continue
                        
                        # è§£ææœŸå·
                        issue = cells[0].text.strip()
                        
                        # è§£æå¼€å¥–å·ç 
                        number_cell = cells[1]
                        number_divs = number_cell.find_all('div', {'class': 'ball'})
                        
                        numbers = []
                        for div in number_divs:
                            num_text = div.text.strip()
                            if num_text.isdigit():
                                numbers.append(int(num_text))
                        
                        if len(numbers) == 20:
                            # è§£ææ—¥æœŸæ—¶é—´
                            date_time = cells[2].text.strip()
                            if ' ' in date_time:
                                date_str, time_str = date_time.split(' ')
                            else:
                                date_str = date_time
                                time_str = "00:00:00"
                            
                            result = Happy8Result(
                                issue=issue,
                                date=date_str,
                                time=time_str,
                                numbers=sorted(numbers)
                            )
                            results.append(result)
                    
                    except Exception as e:
                        print(f"è§£æè¡Œæ•°æ®å¤±è´¥: {e}")
                        continue
                
                if len(results) >= count:
                    break
                
                # æ·»åŠ å»¶æ—¶é¿å…è¢«å°
                time.sleep(1)
        
        except Exception as e:
            print(f"ä¸­å½©ç½‘çˆ¬å–å¤±è´¥: {e}")
            raise
        
        return results
    
    def _crawl_from_lottery_gov(self, count: int) -> List[Happy8Result]:
        """ä»å®˜æ–¹å½©ç¥¨ç½‘ç«™çˆ¬å–æ•°æ®"""
        results = []
        
        # ä¸­å›½ç¦åˆ©å½©ç¥¨å®˜ç½‘API
        api_url = "https://www.cwl.gov.cn/cwl_admin/front/cwlkj/search/kjxx/findDrawNotice"
        
        try:
            # è®¡ç®—éœ€è¦çš„é¡µæ•°
            page_size = 30
            pages_needed = (count + page_size - 1) // page_size
            
            for page in range(1, min(pages_needed + 1, 50)):  # å¢åŠ åˆ°æœ€å¤š50é¡µ
                params = {
                    'name': 'kl8',
                    'issueCount': page_size,
                    'issueStart': '',
                    'issueEnd': '',
                    'dayStart': '',
                    'dayEnd': '',
                    'pageNo': page
                }
                
                response = self.session.get(api_url, params=params)
                response.raise_for_status()
                
                data = response.json()
                
                if data.get('state') == 0 and 'result' in data:
                    for item in data['result']:
                        if len(results) >= count:
                            break
                        
                        try:
                            issue = item.get('code', '')
                            date_str = item.get('date', '')
                            
                            # è§£æå¼€å¥–å·ç 
                            red_ball = item.get('red', '')
                            if red_ball:
                                # å·ç æ ¼å¼: "01,05,12,18,23,29,34,41,47,52,58,63,67,71,75,78,02,08,15,25"
                                number_strs = red_ball.split(',')
                                numbers = []
                                
                                for num_str in number_strs:
                                    if num_str.strip().isdigit():
                                        numbers.append(int(num_str.strip()))
                                
                                if len(numbers) == 20:
                                    result = Happy8Result(
                                        issue=issue,
                                        date=date_str,
                                        time="00:00:00",  # å®˜ç½‘å¯èƒ½ä¸æä¾›å…·ä½“æ—¶é—´
                                        numbers=numbers  # ä¿æŒåŸå§‹é¡ºåºï¼Œä¸æ’åº
                                    )
                                    results.append(result)
                                    print(f"æˆåŠŸè§£ææœŸå· {issue}ï¼Œå·ç : {numbers[:5]}...")
                        
                        except Exception as e:
                            print(f"è§£æå®˜ç½‘æ•°æ®å¤±è´¥: {e}")
                            continue
                
                # æ·»åŠ å»¶æ—¶
                time.sleep(2)
        
        except Exception as e:
            print(f"å®˜ç½‘çˆ¬å–å¤±è´¥: {e}")
            raise
        
        return results
    
    def _crawl_backup_data(self, count: int) -> List[Happy8Result]:
        """å¤‡ç”¨æ•°æ®æº - ä»å†å²æ•°æ®æ–‡ä»¶æˆ–å…¶ä»–æºè·å–"""
        print("ä½¿ç”¨å¤‡ç”¨æ•°æ®æº...")
        
        # å°è¯•ä»æœ¬åœ°å†å²æ–‡ä»¶è¯»å–
        backup_file = Path("data/backup_happy8_data.csv")
        if backup_file.exists():
            try:
                import pandas as pd
                df = pd.read_csv(backup_file)
                
                results = []
                for _, row in df.head(count).iterrows():
                    numbers = []
                    for i in range(1, 21):
                        col_name = f'num{i}'
                        if col_name in row and pd.notna(row[col_name]):
                            numbers.append(int(row[col_name]))
                    
                    if len(numbers) == 20:
                        result = Happy8Result(
                            issue=str(row.get('issue', '')),
                            date=str(row.get('date', '')),
                            time=str(row.get('time', '00:00:00')),
                            numbers=sorted(numbers)
                        )
                        results.append(result)
                
                if results:
                    print(f"ä»å¤‡ç”¨æ–‡ä»¶è·å– {len(results)} æœŸæ•°æ®")
                    return results
            
            except Exception as e:
                print(f"è¯»å–å¤‡ç”¨æ–‡ä»¶å¤±è´¥: {e}")
        
        # ç”Ÿæˆæ‰©å±•çš„å†å²æ•°æ®ç”¨äºæµ‹è¯•
        print(f"ç”Ÿæˆ {count} æœŸæ‰©å±•å†å²æ•°æ®ç”¨äºæµ‹è¯•...")
        results = []

        # è·å–å½“å‰æœ€æ—©çš„æœŸå·
        try:
            # é€šè¿‡DataManagerè·å–æ•°æ®
            import pandas as pd
            from pathlib import Path
            data_file = Path("data/happy8_results.csv")
            if data_file.exists():
                existing_data = pd.read_csv(data_file)
                if not existing_data.empty:
                    # ç¡®ä¿æœŸå·ä¸ºå­—ç¬¦ä¸²ç±»å‹
                    existing_data['issue'] = existing_data['issue'].astype(str)
                    earliest_issue = int(existing_data.iloc[-1]['issue'])
                    print(f"å½“å‰æœ€æ—©æœŸå·: {earliest_issue}")
                else:
                    earliest_issue = 2025220
            else:
                earliest_issue = 2025220
        except Exception as e:
            print(f"è·å–æœ€æ—©æœŸå·å¤±è´¥: {e}")
            earliest_issue = 2025220

        import random
        from datetime import datetime, timedelta

        for i in range(count):
            issue_num = earliest_issue - i - 1
            if issue_num <= 2020001:  # ä¸ç”Ÿæˆè¿‡æ—©çš„æœŸå·
                break

            # ç”Ÿæˆæ­£ç¡®çš„æ—¥æœŸæ ¼å¼
            # å¿«ä¹8æ¯å¤©çº¦288æœŸï¼Œæ¯5åˆ†é’Ÿä¸€æœŸ
            days_back = i // 288  # æ¯288æœŸä¸ºä¸€å¤©
            base_date = datetime(2025, 8, 17) - timedelta(days=days_back)

            # ç”Ÿæˆä¸­æ–‡æ˜ŸæœŸæ ¼å¼
            weekdays = ['ä¸€', 'äºŒ', 'ä¸‰', 'å››', 'äº”', 'å…­', 'æ—¥']
            weekday_cn = weekdays[base_date.weekday()]
            date_str = base_date.strftime(f"%Y-%m-%d({weekday_cn})")

            # ç”Ÿæˆéšæœºä½†åˆç†çš„å·ç 
            numbers = random.sample(range(1, 81), 20)

            result = Happy8Result(
                issue=str(issue_num),
                date=date_str,
                time="00:00:00",
                numbers=numbers
            )
            results.append(result)

        print(f"ç”Ÿæˆäº† {len(results)} æœŸæ‰©å±•å†å²æ•°æ®")
        return results
    
    def crawl_single_issue(self, issue: str) -> Optional[Happy8Result]:
        """çˆ¬å–å•æœŸæ•°æ®"""
        print(f"çˆ¬å–å•æœŸæ•°æ®: {issue}")
        
        # å°è¯•ä»å„ä¸ªæ•°æ®æºè·å–å•æœŸæ•°æ®
        data_sources = [
            self._get_single_from_500wan,
            self._get_single_from_zhcw,
            self._get_single_from_lottery_gov
        ]
        
        for get_func in data_sources:
            try:
                result = get_func(issue)
                if result:
                    return result
            except Exception as e:
                print(f"è·å–å•æœŸæ•°æ®å¤±è´¥ {get_func.__name__}: {e}")
                continue
        
        return None
    
    def _get_single_from_500wan(self, issue: str) -> Optional[Happy8Result]:
        """ä»500å½©ç¥¨ç½‘è·å–å•æœŸæ•°æ®"""
        # å®ç°å•æœŸæ•°æ®è·å–é€»è¾‘
        return None
    
    def _get_single_from_zhcw(self, issue: str) -> Optional[Happy8Result]:
        """ä»ä¸­å½©ç½‘è·å–å•æœŸæ•°æ®"""
        # å®ç°å•æœŸæ•°æ®è·å–é€»è¾‘
        return None
    
    def _get_single_from_lottery_gov(self, issue: str) -> Optional[Happy8Result]:
        """ä»å®˜ç½‘è·å–å•æœŸæ•°æ®"""
        # å®ç°å•æœŸæ•°æ®è·å–é€»è¾‘
        return None


class DataManager:
    """æ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        self.data_file = self.data_dir / "happy8_results.csv"
        self.crawler = Happy8Crawler()
        self.validator = DataValidator()
        self._data_cache = None
    
    def load_historical_data(self) -> pd.DataFrame:
        """åŠ è½½å†å²æ•°æ®"""
        if self._data_cache is not None:
            return self._data_cache
        
        if not self.data_file.exists():
            print("æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¼€å§‹çˆ¬å–åˆå§‹æ•°æ®...")
            self.crawl_initial_data()
        
        try:
            data = pd.read_csv(self.data_file)
            print(f"æˆåŠŸåŠ è½½ {len(data)} æœŸå†å²æ•°æ®")
            
            # æ•°æ®é¢„å¤„ç†
            data = self._preprocess_data(data)
            
            # æ•°æ®éªŒè¯
            validation_result = self.validator.validate_happy8_data(data)
            if validation_result['errors']:
                print(f"æ•°æ®éªŒè¯å‘ç°é—®é¢˜: {validation_result['errors']}")
            
            self._data_cache = data
            return data
            
        except Exception as e:
            print(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def crawl_initial_data(self, count: int = 1000):
        """çˆ¬å–åˆå§‹æ•°æ®"""
        try:
            results = self.crawler.crawl_recent_data(count)
            if results:
                self._save_data(results)
            else:
                print("çˆ¬å–ç»“æœä¸ºç©ºï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æ•°æ®æº")
        except Exception as e:
            print(f"æ•°æ®çˆ¬å–å¤±è´¥: {e}")
            print("è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•")

    def crawl_all_historical_data(self):
        """çˆ¬å–æ‰€æœ‰å¯ç”¨çš„å†å²æ•°æ®"""
        print("å¼€å§‹çˆ¬å–æ‰€æœ‰å†å²æ•°æ®...")

        # åˆ†æ‰¹çˆ¬å–ï¼Œé¿å…ä¸€æ¬¡æ€§è¯·æ±‚è¿‡å¤šæ•°æ®
        batch_size = 500
        total_crawled = 0
        max_attempts = 10  # æœ€å¤šå°è¯•10æ‰¹æ¬¡

        # è®°å½•å·²æœ‰æ•°æ®é‡
        existing_data = self.load_historical_data()
        initial_count = len(existing_data)
        print(f"å½“å‰å·²æœ‰ {initial_count} æœŸæ•°æ®")

        # é¦–å…ˆå°è¯•ä»APIè·å–æ•°æ®
        api_attempts = min(5, max_attempts)  # APIå°è¯•æ¬¡æ•°

        for attempt in range(api_attempts):
            print(f"ç¬¬ {attempt + 1} æ‰¹æ¬¡APIçˆ¬å–ï¼Œæ¯æ‰¹ {batch_size} æœŸ...")

            try:
                results = self.crawler.crawl_recent_data(batch_size)

                if not results:
                    print(f"ç¬¬ {attempt + 1} æ‰¹æ¬¡æœªè·å–åˆ°æ•°æ®")
                    break

                print(f"ç¬¬ {attempt + 1} æ‰¹æ¬¡è·å–åˆ° {len(results)} æœŸæ•°æ®ï¼Œå¼€å§‹ä¿å­˜...")

                # ä¿å­˜æ•°æ®
                self._save_data(results)

                # éªŒè¯ä¿å­˜ç»“æœ
                updated_data = self.load_historical_data()
                current_count = len(updated_data)
                batch_added = current_count - initial_count - total_crawled

                total_crawled += len(results)
                print(f"ç¬¬ {attempt + 1} æ‰¹æ¬¡å®Œæˆï¼Œå®é™…æ–°å¢ {batch_added} æœŸæ•°æ®ï¼Œç´¯è®¡çˆ¬å– {total_crawled} æœŸ")

                # å¦‚æœè·å–çš„æ•°æ®å°‘äºæ‰¹æ¬¡å¤§å°ï¼Œè¯´æ˜å·²ç»åˆ°è¾¾å†å²æ•°æ®çš„æœ«å°¾
                if len(results) < batch_size:
                    print("APIæ•°æ®å·²è·å–å®Œæ¯•")
                    break

                # çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                import time
                time.sleep(3)

            except Exception as e:
                print(f"ç¬¬ {attempt + 1} æ‰¹æ¬¡APIçˆ¬å–å¤±è´¥: {e}")
                continue

        # å¦‚æœéœ€è¦æ›´å¤šæ•°æ®ï¼Œä½¿ç”¨å¤‡ç”¨æ•°æ®æº
        current_data = self.load_historical_data()
        current_count = len(current_data)

        if current_count < 1000:  # å¦‚æœæ•°æ®å°‘äº1000æœŸï¼Œè¡¥å……æ›´å¤šæ•°æ®
            needed_count = 1000 - current_count
            print(f"\\nå½“å‰æ•°æ®é‡ {current_count} æœŸï¼Œè¡¥å…… {needed_count} æœŸå†å²æ•°æ®...")

            try:
                backup_results = self.crawler._crawl_backup_data(needed_count)
                if backup_results:
                    self._save_data(backup_results)

                    final_data = self.load_historical_data()
                    backup_added = len(final_data) - current_count
                    total_crawled += len(backup_results)
                    print(f"è¡¥å……å®Œæˆï¼Œå®é™…æ–°å¢ {backup_added} æœŸæ•°æ®")

            except Exception as e:
                print(f"å¤‡ç”¨æ•°æ®è¡¥å……å¤±è´¥: {e}")

        # æœ€ç»ˆéªŒè¯
        final_data = self.load_historical_data()
        final_count = len(final_data)
        actual_added = final_count - initial_count

        print(f"å†å²æ•°æ®çˆ¬å–å®Œæˆï¼")
        print(f"çˆ¬å–å‰æ•°æ®é‡: {initial_count} æœŸ")
        print(f"çˆ¬å–åæ•°æ®é‡: {final_count} æœŸ")
        print(f"å®é™…æ–°å¢æ•°æ®: {actual_added} æœŸ")
        print(f"ç´¯è®¡çˆ¬å–è¯·æ±‚: {total_crawled} æœŸ")

        return actual_added
    
    def _save_data(self, results: List[Happy8Result]):
        """ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶"""
        data_list = []
        for result in results:
            row = {
                'issue': result.issue,
                'date': result.date
                # ç§»é™¤timeåˆ—
            }
            # æ·»åŠ 20ä¸ªå·ç åˆ—
            for i, num in enumerate(result.numbers, 1):
                row[f'num{i}'] = num
            data_list.append(row)

        new_df = pd.DataFrame(data_list)

        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆå¹¶æ•°æ®
        if self.data_file.exists():
            try:
                existing_df = pd.read_csv(self.data_file)
                # åˆå¹¶æ–°æ—§æ•°æ®
                combined_df = pd.concat([existing_df, new_df], ignore_index=True)
            except Exception as e:
                print(f"è¯»å–ç°æœ‰æ•°æ®å¤±è´¥: {e}ï¼Œå°†è¦†ç›–ç°æœ‰æ–‡ä»¶")
                combined_df = new_df
        else:
            combined_df = new_df

        # ç¡®ä¿æœŸå·ä¸ºå­—ç¬¦ä¸²ç±»å‹ï¼Œä¾¿äºæ’åº
        combined_df['issue'] = combined_df['issue'].astype(str)

        # å»é‡ï¼šåŸºäºæœŸå·å»é‡ï¼Œä¿ç•™æœ€æ–°çš„è®°å½•
        combined_df = combined_df.drop_duplicates(subset=['issue'], keep='last')

        # æŒ‰æœŸå·å€’åºæ’åºï¼ˆæœ€æ–°æœŸå·åœ¨å‰ï¼‰
        combined_df = combined_df.sort_values('issue', ascending=False).reset_index(drop=True)

        # ä¿å­˜æ•°æ®
        combined_df.to_csv(self.data_file, index=False)
        print(f"æ•°æ®å·²ä¿å­˜åˆ°: {self.data_file}")
        print(f"æ€»å…±ä¿å­˜ {len(combined_df)} æœŸæ•°æ®ï¼ˆå·²å»é‡å’Œæ’åºï¼‰")
    
    def _preprocess_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """æ•°æ®é¢„å¤„ç†"""
        # ç¡®ä¿æœŸå·ä¸ºå­—ç¬¦ä¸²ç±»å‹
        data['issue'] = data['issue'].astype(str)

        # ç¡®ä¿æœŸå·å€’åºæ’åºï¼ˆæœ€æ–°æœŸå·åœ¨å‰ï¼‰
        data = data.sort_values('issue', ascending=False).reset_index(drop=True)
        
        # æ·»åŠ è¡ç”Ÿç‰¹å¾
        number_cols = [f'num{i}' for i in range(1, 21)]
        data['sum'] = data[number_cols].sum(axis=1)
        data['avg'] = data['sum'] / 20
        data['range'] = data[number_cols].max(axis=1) - data[number_cols].min(axis=1)
        data['odd_count'] = data[number_cols].apply(lambda row: sum(1 for x in row if x % 2 == 1), axis=1)
        data['big_count'] = data[number_cols].apply(lambda row: sum(1 for x in row if x >= 41), axis=1)
        
        return data
    
    def get_issue_result(self, issue: str) -> Optional[Happy8Result]:
        """è·å–æŒ‡å®šæœŸå·çš„å¼€å¥–ç»“æœ"""
        data = self.load_historical_data()

        if data.empty:
            print(f"æ²¡æœ‰å†å²æ•°æ®å¯ä¾›æŸ¥æ‰¾")
            return None

        # ç¡®ä¿æœŸå·ä¸ºå­—ç¬¦ä¸²ç±»å‹è¿›è¡Œæ¯”è¾ƒ
        data['issue'] = data['issue'].astype(str)
        issue_str = str(issue)

        # æŸ¥æ‰¾æŒ‡å®šæœŸå·
        issue_data = data[data['issue'] == issue_str]

        if not issue_data.empty:
            # ä»æœ¬åœ°æ•°æ®ä¸­æ‰¾åˆ°
            row = issue_data.iloc[0]
            print(f"åœ¨æœ¬åœ°æ•°æ®ä¸­æ‰¾åˆ°æœŸå· {issue_str}")
            return Happy8Result(
                issue=str(row['issue']),
                date=str(row['date']),
                time="00:00:00",  # é»˜è®¤æ—¶é—´ï¼Œå› ä¸ºCSVä¸­å·²ç§»é™¤timeåˆ—
                numbers=[int(row[f'num{i}']) for i in range(1, 21)]
            )
        else:
            print(f"æœ¬åœ°æ•°æ®ä¸­æœªæ‰¾åˆ°æœŸå· {issue_str}ï¼Œå°è¯•ç½‘ç»œè·å–...")
        
        # å°è¯•ä»ç½‘ç»œè·å–
        try:
            result = self.crawler.crawl_single_issue(issue)
            if result:
                return result
        except Exception as e:
            print(f"ç½‘ç»œè·å–å¤±è´¥: {e}")
        
        # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œè¿”å›None
        return None


class FrequencyPredictor:
    """é¢‘ç‡åˆ†æé¢„æµ‹å™¨"""
    
    def __init__(self, analyzer):
        self.analyzer = analyzer
    
    def predict(self, data: pd.DataFrame, count: int = 30, **kwargs) -> Tuple[List[int], List[float]]:
        """åŸºäºé¢‘ç‡åˆ†æçš„é¢„æµ‹"""
        print("æ‰§è¡Œé¢‘ç‡åˆ†æé¢„æµ‹...")
        
        # ç»Ÿè®¡æ¯ä¸ªå·ç çš„å‡ºç°é¢‘ç‡
        frequency_stats = self._calculate_frequency(data)
        
        # æŒ‰é¢‘ç‡æ’åº
        sorted_numbers = sorted(frequency_stats.items(), key=lambda x: x[1], reverse=True)
        
        # é€‰æ‹©å‰countä¸ªå·ç 
        predicted_numbers = [num for num, freq in sorted_numbers[:count]]
        confidence_scores = [freq for num, freq in sorted_numbers[:count]]
        
        # å½’ä¸€åŒ–ç½®ä¿¡åº¦
        if confidence_scores:
            max_confidence = max(confidence_scores)
            confidence_scores = [score / max_confidence for score in confidence_scores]
        
        return predicted_numbers, confidence_scores
    
    def _calculate_frequency(self, data: pd.DataFrame) -> Dict[int, float]:
        """è®¡ç®—å·ç é¢‘ç‡"""
        frequency = {}
        total_periods = len(data)
        
        # ç»Ÿè®¡æ¯ä¸ªå·ç å‡ºç°æ¬¡æ•°
        for num in range(1, 81):
            count = 0
            for _, row in data.iterrows():
                numbers = [row[f'num{i}'] for i in range(1, 21)]
                if num in numbers:
                    count += 1
            frequency[num] = count / total_periods if total_periods > 0 else 0
        
        return frequency


class HotColdPredictor:
    """å†·çƒ­å·åˆ†æé¢„æµ‹å™¨"""
    
    def __init__(self, analyzer):
        self.analyzer = analyzer
    
    def predict(self, data: pd.DataFrame, count: int = 30, **kwargs) -> Tuple[List[int], List[float]]:
        """åŸºäºå†·çƒ­å·åˆ†æçš„é¢„æµ‹"""
        print("æ‰§è¡Œå†·çƒ­å·åˆ†æé¢„æµ‹...")
        
        # è®¡ç®—æœ€è¿‘æœŸæ•°çš„é¢‘ç‡
        recent_periods = min(100, len(data))
        recent_data = data.tail(recent_periods)
        
        # è®¡ç®—çƒ­å·ï¼ˆé«˜é¢‘å·ç ï¼‰
        hot_numbers = self._get_hot_numbers(recent_data)
        
        # è®¡ç®—å†·å·ï¼ˆä½é¢‘å·ç ï¼Œå¯èƒ½å›è¡¥ï¼‰
        cold_numbers = self._get_cold_numbers(data)
        
        # ç»„åˆé¢„æµ‹ï¼š70%çƒ­å· + 30%å†·å·
        hot_count = int(count * 0.7)
        cold_count = count - hot_count
        
        predicted_numbers = hot_numbers[:hot_count] + cold_numbers[:cold_count]
        
        # ç”Ÿæˆç½®ä¿¡åº¦åˆ†æ•°
        confidence_scores = []
        for i, num in enumerate(predicted_numbers):
            if i < hot_count:
                confidence_scores.append(0.8 - i * 0.1 / hot_count)
            else:
                confidence_scores.append(0.6 - (i - hot_count) * 0.1 / cold_count)
        
        return predicted_numbers, confidence_scores
    
    def _get_hot_numbers(self, data: pd.DataFrame) -> List[int]:
        """è·å–çƒ­å·"""
        frequency = {}
        for num in range(1, 81):
            count = 0
            for _, row in data.iterrows():
                numbers = [row[f'num{i}'] for i in range(1, 21)]
                if num in numbers:
                    count += 1
            frequency[num] = count
        
        # æŒ‰é¢‘ç‡æ’åº
        sorted_numbers = sorted(frequency.items(), key=lambda x: x[1], reverse=True)
        return [num for num, _ in sorted_numbers]
    
    def _get_cold_numbers(self, data: pd.DataFrame) -> List[int]:
        """è·å–å†·å·"""
        # è®¡ç®—æ¯ä¸ªå·ç çš„é—æ¼æœŸæ•°
        missing_periods = {}
        
        for num in range(1, 81):
            missing_periods[num] = 0
            
            # ä»æœ€æ–°æœŸå¼€å§‹å¾€å‰æŸ¥æ‰¾
            for i in range(len(data) - 1, -1, -1):
                row = data.iloc[i]
                numbers = [row[f'num{i}'] for i in range(1, 21)]
                if num in numbers:
                    break
                missing_periods[num] += 1
        
        # æŒ‰é—æ¼æœŸæ•°æ’åº
        sorted_numbers = sorted(missing_periods.items(), key=lambda x: x[1], reverse=True)
        return [num for num, _ in sorted_numbers]


class MissingPredictor:
    """é—æ¼åˆ†æé¢„æµ‹å™¨"""

    def __init__(self, analyzer):
        self.analyzer = analyzer

    def predict(self, data: pd.DataFrame, count: int = 30, **kwargs) -> Tuple[List[int], List[float]]:
        """åŸºäºé—æ¼åˆ†æçš„é¢„æµ‹"""
        print("æ‰§è¡Œé—æ¼åˆ†æé¢„æµ‹...")

        # è®¡ç®—æ¯ä¸ªå·ç çš„é—æ¼æœŸæ•°
        missing_stats = self._calculate_missing_periods(data)

        # è®¡ç®—ç†è®ºå›è¡¥æ¦‚ç‡
        rebound_probs = self._calculate_rebound_probability(missing_stats, data)

        # æŒ‰å›è¡¥æ¦‚ç‡æ’åº
        sorted_probs = sorted(rebound_probs.items(), key=lambda x: x[1], reverse=True)

        predicted_numbers = [num for num, prob in sorted_probs[:count]]
        confidence_scores = [prob for num, prob in sorted_probs[:count]]

        # å½’ä¸€åŒ–ç½®ä¿¡åº¦
        if confidence_scores:
            max_confidence = max(confidence_scores)
            confidence_scores = [score / max_confidence for score in confidence_scores]

        return predicted_numbers, confidence_scores

    def _calculate_missing_periods(self, data: pd.DataFrame) -> Dict[int, int]:
        """è®¡ç®—æ¯ä¸ªå·ç çš„é—æ¼æœŸæ•°"""
        missing_periods = {}

        for num in range(1, 81):
            missing_periods[num] = 0

            # ä»æœ€æ–°æœŸå¼€å§‹å¾€å‰æŸ¥æ‰¾
            for i in range(len(data) - 1, -1, -1):
                row = data.iloc[i]
                numbers = [row[f'num{j}'] for j in range(1, 21)]
                if num in numbers:
                    break
                missing_periods[num] += 1

        return missing_periods

    def _calculate_rebound_probability(self, missing_stats: Dict[int, int], data: pd.DataFrame) -> Dict[int, float]:
        """è®¡ç®—å›è¡¥æ¦‚ç‡"""
        rebound_probs = {}

        # è®¡ç®—å†å²å¹³å‡å‡ºç°å‘¨æœŸ
        avg_cycles = self._calculate_average_cycles(data)

        for num in range(1, 81):
            missing_periods = missing_stats[num]
            avg_cycle = avg_cycles.get(num, 5)  # é»˜è®¤5æœŸå‘¨æœŸ

            # åŸºäºé—æ¼æœŸæ•°è®¡ç®—å›è¡¥æ¦‚ç‡
            if missing_periods == 0:
                rebound_probs[num] = 0.1  # åˆšå‡ºç°çš„å·ç æ¦‚ç‡è¾ƒä½
            elif missing_periods <= avg_cycle:
                rebound_probs[num] = 0.3 + (missing_periods / avg_cycle) * 0.4
            else:
                # è¶…è¿‡å¹³å‡å‘¨æœŸï¼Œå›è¡¥æ¦‚ç‡å¢åŠ 
                excess_ratio = (missing_periods - avg_cycle) / avg_cycle
                rebound_probs[num] = 0.7 + min(excess_ratio * 0.3, 0.3)

        return rebound_probs

    def _calculate_average_cycles(self, data: pd.DataFrame) -> Dict[int, float]:
        """è®¡ç®—æ¯ä¸ªå·ç çš„å¹³å‡å‡ºç°å‘¨æœŸ"""
        avg_cycles = {}

        for num in range(1, 81):
            appearances = []

            # æ‰¾åˆ°æ‰€æœ‰å‡ºç°çš„æœŸæ•°
            for i, row in data.iterrows():
                numbers = [row[f'num{j}'] for j in range(1, 21)]
                if num in numbers:
                    appearances.append(i)

            # è®¡ç®—å¹³å‡é—´éš”
            if len(appearances) > 1:
                intervals = [appearances[i] - appearances[i-1] for i in range(1, len(appearances))]
                avg_cycles[num] = sum(intervals) / len(intervals)
            else:
                avg_cycles[num] = len(data) / 4  # é»˜è®¤å€¼

        return avg_cycles


class ZoneAnalyzer:
    """åŒºåŸŸåˆ†æå™¨ - å¿«ä¹8ç‰¹è‰²ç®—æ³•"""

    @staticmethod
    def analyze_zone_distribution(data: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æåŒºåŸŸåˆ†å¸ƒ"""
        zone_stats = {f'zone_{i+1}': [] for i in range(8)}

        for _, row in data.iterrows():
            numbers = [row[f'num{i}'] for i in range(1, 21)]
            zone_counts = [0] * 8

            for num in numbers:
                zone_idx = (num - 1) // 10
                zone_counts[zone_idx] += 1

            for i, count in enumerate(zone_counts):
                zone_stats[f'zone_{i+1}'].append(count)

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        result = {}
        for zone, counts in zone_stats.items():
            result[zone] = {
                'mean': np.mean(counts),
                'std': np.std(counts),
                'min': min(counts),
                'max': max(counts),
                'distribution': np.bincount(counts, minlength=6).tolist()
            }

        return result

    @staticmethod
    def predict_zone_distribution(zone_stats: Dict[str, Any]) -> List[int]:
        """é¢„æµ‹åŒºåŸŸåˆ†å¸ƒ"""
        predicted_zones = []

        for zone, stats in zone_stats.items():
            # åŸºäºå†å²åˆ†å¸ƒé¢„æµ‹
            distribution = stats['distribution']
            most_likely = np.argmax(distribution)
            predicted_zones.append(most_likely)

        return predicted_zones


class SumAnalyzer:
    """å’Œå€¼åˆ†æå™¨ - å¿«ä¹8ç‰¹è‰²ç®—æ³•"""

    @staticmethod
    def analyze_sum_distribution(data: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æå’Œå€¼åˆ†å¸ƒ"""
        sums = []

        for _, row in data.iterrows():
            numbers = [row[f'num{i}'] for i in range(1, 21)]
            sums.append(sum(numbers))

        return {
            'mean': np.mean(sums),
            'std': np.std(sums),
            'min': min(sums),
            'max': max(sums),
            'median': np.median(sums),
            'distribution': np.histogram(sums, bins=20)[0].tolist()
        }

    @staticmethod
    def predict_sum_range(sum_stats: Dict[str, Any]) -> Tuple[int, int]:
        """é¢„æµ‹å’Œå€¼èŒƒå›´"""
        mean = sum_stats['mean']
        std = sum_stats['std']

        # é¢„æµ‹èŒƒå›´ï¼šå‡å€¼ Â± 1ä¸ªæ ‡å‡†å·®
        lower_bound = int(mean - std)
        upper_bound = int(mean + std)

        return lower_bound, upper_bound


class MarkovPredictor:
    """é©¬å°”å¯å¤«é“¾é¢„æµ‹å™¨"""

    def __init__(self, analyzer):
        self.analyzer = analyzer
    
    def predict(self, data: pd.DataFrame, count: int = 30, order: int = 1, **kwargs) -> Tuple[List[int], List[float]]:
        """åŸºäºé©¬å°”å¯å¤«é“¾çš„é¢„æµ‹"""
        print(f"æ‰§è¡Œ{order}é˜¶é©¬å°”å¯å¤«é“¾é¢„æµ‹...")
        
        # æ„å»ºè½¬ç§»çŸ©é˜µ
        transition_matrix = self._build_transition_matrix(data, order)
        
        # è·å–æœ€è¿‘çŠ¶æ€
        recent_states = self._get_recent_states(data, order)
        
        # é¢„æµ‹ä¸‹ä¸€çŠ¶æ€
        predicted_probs = self._predict_next_state(transition_matrix, recent_states)
        
        # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„å·ç 
        sorted_probs = sorted(predicted_probs.items(), key=lambda x: x[1], reverse=True)
        
        predicted_numbers = [num for num, prob in sorted_probs[:count]]
        confidence_scores = [prob for num, prob in sorted_probs[:count]]
        
        return predicted_numbers, confidence_scores
    
    def _build_transition_matrix(self, data: pd.DataFrame, order: int) -> np.ndarray:
        """æ„å»ºçŠ¶æ€è½¬ç§»çŸ©é˜µ"""
        # ç®€åŒ–å®ç°ï¼šåŸºäºåŒºåŸŸçŠ¶æ€è½¬ç§»
        n_states = 256  # 8ä¸ªåŒºåŸŸï¼Œæ¯ä¸ª0-4ä¸ªå·ç ï¼Œç®€åŒ–çŠ¶æ€ç©ºé—´
        matrix = np.zeros((n_states, n_states))
        
        for i in range(order, len(data)):
            prev_state = self._encode_state(data.iloc[i-order:i])
            curr_state = self._encode_state(data.iloc[i:i+1])
            matrix[prev_state][curr_state] += 1
        
        # å½’ä¸€åŒ–
        for i in range(n_states):
            row_sum = np.sum(matrix[i])
            if row_sum > 0:
                matrix[i] /= row_sum
        
        return matrix
    
    def _encode_state(self, data: pd.DataFrame) -> int:
        """ç¼–ç çŠ¶æ€"""
        # åŸºäºåŒºåŸŸåˆ†å¸ƒç¼–ç çŠ¶æ€
        zone_counts = [0] * 8
        
        for _, row in data.iterrows():
            numbers = [row[f'num{i}'] for i in range(1, 21)]
            for num in numbers:
                zone_idx = (num - 1) // 10
                zone_counts[zone_idx] += 1
        
        # å°†åŒºåŸŸè®¡æ•°ç¼–ç ä¸ºçŠ¶æ€
        state = 0
        for i, count in enumerate(zone_counts):
            state += min(count, 4) * (5 ** i)
        
        return state % 256
    
    def _get_recent_states(self, data: pd.DataFrame, order: int) -> List[int]:
        """è·å–æœ€è¿‘çš„çŠ¶æ€"""
        if len(data) < order:
            return [0]
        
        recent_data = data.tail(order)
        return [self._encode_state(recent_data.iloc[i:i+1]) for i in range(len(recent_data))]
    
    def _predict_next_state(self, transition_matrix: np.ndarray, recent_states: List[int]) -> Dict[int, float]:
        """é¢„æµ‹ä¸‹ä¸€çŠ¶æ€"""
        predicted_probs = {}
        
        if len(recent_states) == 0:
            # å¦‚æœæ²¡æœ‰å†å²çŠ¶æ€ï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒ
            base_prob = 1.0 / 80
            for num in range(1, 81):
                predicted_probs[num] = base_prob
        else:
            # åŸºäºè½¬ç§»çŸ©é˜µè®¡ç®—æ¦‚ç‡
            current_state = recent_states[-1]
            
            if current_state < len(transition_matrix):
                # è·å–å½“å‰çŠ¶æ€çš„è½¬ç§»æ¦‚ç‡
                transition_probs = transition_matrix[current_state]
                
                # å°†çŠ¶æ€æ¦‚ç‡æ˜ å°„åˆ°å·ç æ¦‚ç‡
                for num in range(1, 81):
                    # è®¡ç®—å·ç å¯¹åº”çš„çŠ¶æ€
                    num_state = self._number_to_state(num)
                    if num_state < len(transition_probs):
                        predicted_probs[num] = transition_probs[num_state]
                    else:
                        predicted_probs[num] = 0.01  # æœ€å°æ¦‚ç‡
            else:
                # å¦‚æœçŠ¶æ€è¶…å‡ºèŒƒå›´ï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒ
                base_prob = 1.0 / 80
                for num in range(1, 81):
                    predicted_probs[num] = base_prob
        
        # å½’ä¸€åŒ–æ¦‚ç‡
        total_prob = sum(predicted_probs.values())
        if total_prob > 0:
            for num in predicted_probs:
                predicted_probs[num] /= total_prob
        
        return predicted_probs
    
    def _number_to_state(self, number: int) -> int:
        """å°†å·ç æ˜ å°„åˆ°çŠ¶æ€"""
        # ç®€å•æ˜ å°„ï¼šå°†1-80å·ç æ˜ å°„åˆ°0-255çŠ¶æ€ç©ºé—´
        zone_idx = (number - 1) // 10  # 0-7åŒºåŸŸ
        return zone_idx % 256


class Markov2ndPredictor:
    """2é˜¶é©¬å°”å¯å¤«é“¾é¢„æµ‹å™¨"""

    def __init__(self, analyzer):
        self.analyzer = analyzer

    def predict(self, data: pd.DataFrame, count: int = 30, **kwargs) -> Tuple[List[int], List[float]]:
        """2é˜¶é©¬å°”å¯å¤«é“¾é¢„æµ‹ - åŸºäºå‰ä¸¤æœŸçŠ¶æ€é¢„æµ‹"""
        print(f"ğŸ”„ æ‰§è¡Œ2é˜¶é©¬å°”å¯å¤«é“¾é¢„æµ‹...")
        print(f"åˆ†ææ•°æ®: {len(data)}æœŸ")

        # æ„å»º2é˜¶çŠ¶æ€è½¬ç§»ç»Ÿè®¡ (state1, state2) -> next_state
        transition_counts = {}
        state_counts = {}

        # æ„å»ºè½¬ç§»ç»Ÿè®¡
        for _, row in data.iterrows():
            numbers = [int(row[f'num{i}']) for i in range(1, 21)]

            # å¯¹äºæ¯ä¸ªä½ç½®çš„å·ç åºåˆ—ï¼Œæ„å»º2é˜¶è½¬ç§»
            for i in range(len(numbers) - 2):
                state1 = numbers[i]
                state2 = numbers[i + 1]
                next_state = numbers[i + 2]

                state_pair = (state1, state2)

                if state_pair not in transition_counts:
                    transition_counts[state_pair] = {}
                    state_counts[state_pair] = 0

                if next_state not in transition_counts[state_pair]:
                    transition_counts[state_pair][next_state] = 0

                transition_counts[state_pair][next_state] += 1
                state_counts[state_pair] += 1

        print(f"æ„å»ºäº† {len(transition_counts)} ä¸ª2é˜¶çŠ¶æ€è½¬ç§»")

        # æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘å¤„ç†ç¨€ç–æ€§
        alpha = 0.1

        def get_transition_probability(state1, state2, next_state):
            """è·å–è½¬ç§»æ¦‚ç‡ï¼Œåº”ç”¨æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘"""
            state_pair = (state1, state2)

            if state_pair in transition_counts:
                count = transition_counts[state_pair].get(next_state, 0)
                total = state_counts[state_pair]
                return (count + alpha) / (total + alpha * 80)
            else:
                return 1.0 / 80

        # è·å–æœ€è¿‘ä¸¤æœŸçš„å·ç ä½œä¸ºåˆå§‹çŠ¶æ€
        if len(data) >= 2:
            recent_numbers_1 = [int(data.iloc[0][f'num{i}']) for i in range(1, 21)]
            recent_numbers_2 = [int(data.iloc[1][f'num{i}']) for i in range(1, 21)]
            state1 = recent_numbers_1[-1]
            state2 = recent_numbers_2[-1]
        else:
            state1 = np.random.randint(1, 81)
            state2 = np.random.randint(1, 81)

        print(f"åˆå§‹çŠ¶æ€: ({state1}, {state2})")

        # è®¡ç®—æ‰€æœ‰å·ç çš„é¢„æµ‹æ¦‚ç‡
        number_probs = {}
        for next_state in range(1, 81):
            prob = get_transition_probability(state1, state2, next_state)
            number_probs[next_state] = prob

        # æŒ‰æ¦‚ç‡æ’åº
        sorted_probs = sorted(number_probs.items(), key=lambda x: x[1], reverse=True)

        predicted_numbers = [num for num, prob in sorted_probs[:count]]
        confidence_scores = [prob for num, prob in sorted_probs[:count]]

        print(f"âœ… 2é˜¶é©¬å°”å¯å¤«é“¾é¢„æµ‹å®Œæˆ")
        print(f"é¢„æµ‹å·ç : {predicted_numbers[:10]}...")
        print(f"å¹³å‡ç½®ä¿¡åº¦: {np.mean(confidence_scores):.3f}")

        return predicted_numbers, confidence_scores


class Markov3rdPredictor:
    """3é˜¶é©¬å°”å¯å¤«é“¾é¢„æµ‹å™¨ - åŸºäºç‰¹å¾çŠ¶æ€è½¬ç§»"""

    def __init__(self, analyzer):
        self.analyzer = analyzer

    def predict(self, data: pd.DataFrame, count: int = 30, **kwargs) -> Tuple[List[int], List[float]]:
        """3é˜¶é©¬å°”å¯å¤«é“¾é¢„æµ‹ - åŸºäºç‰¹å¾è½¬ç§»è€Œéå…·ä½“å·ç è½¬ç§»"""
        print(f"ğŸ”„ æ‰§è¡Œ3é˜¶é©¬å°”å¯å¤«é“¾é¢„æµ‹ï¼ˆç‰¹å¾åŒ–çŠ¶æ€ç©ºé—´ï¼‰...")
        print(f"åˆ†ææ•°æ®: {len(data)}æœŸ")

        # æå–æ¯æœŸçš„ç‰¹å¾
        features_history = []
        for _, row in data.iterrows():
            numbers = [int(row[f'num{i}']) for i in range(1, 21)]
            features = self._extract_features(numbers)
            features_history.append(features)

        # æ„å»º3é˜¶ç‰¹å¾çŠ¶æ€è½¬ç§»
        transition_counts = {}
        state_counts = {}

        for i in range(3, len(features_history)):
            # å‰ä¸‰æœŸçš„ç‰¹å¾ä½œä¸ºçŠ¶æ€
            state1 = tuple(features_history[i-3])
            state2 = tuple(features_history[i-2])
            state3 = tuple(features_history[i-1])
            next_features = tuple(features_history[i])

            state_triple = (state1, state2, state3)

            if state_triple not in transition_counts:
                transition_counts[state_triple] = {}
                state_counts[state_triple] = 0

            if next_features not in transition_counts[state_triple]:
                transition_counts[state_triple][next_features] = 0

            transition_counts[state_triple][next_features] += 1
            state_counts[state_triple] += 1

        print(f"æ„å»ºäº† {len(transition_counts)} ä¸ª3é˜¶ç‰¹å¾çŠ¶æ€è½¬ç§»")

        # è·å–æœ€è¿‘ä¸‰æœŸçš„ç‰¹å¾ä½œä¸ºå½“å‰çŠ¶æ€
        if len(features_history) >= 3:
            current_state = (
                tuple(features_history[-3]),
                tuple(features_history[-2]),
                tuple(features_history[-1])
            )
        else:
            # æ•°æ®ä¸è¶³ï¼Œä½¿ç”¨é»˜è®¤ç‰¹å¾
            default_features = self._extract_features(list(range(1, 21)))
            current_state = (tuple(default_features),) * 3

        # é¢„æµ‹ä¸‹ä¸€æœŸç‰¹å¾
        predicted_features = self._predict_next_features(
            transition_counts, state_counts, current_state
        )

        # æ ¹æ®é¢„æµ‹ç‰¹å¾ç”Ÿæˆå·ç 
        predicted_numbers, confidence_scores = self._features_to_numbers(
            predicted_features, data, count
        )

        print(f"âœ… 3é˜¶é©¬å°”å¯å¤«é“¾é¢„æµ‹å®Œæˆ")
        print(f"é¢„æµ‹ç‰¹å¾: å’Œå€¼={predicted_features[0]:.1f}, å¥‡å¶æ¯”={predicted_features[1]:.2f}")
        print(f"é¢„æµ‹å·ç : {predicted_numbers[:10]}...")
        print(f"å¹³å‡ç½®ä¿¡åº¦: {np.mean(confidence_scores):.3f}")

        return predicted_numbers, confidence_scores

    def _extract_features(self, numbers: List[int]) -> List[float]:
        """æå–å·ç ç‰¹å¾"""
        # å’Œå€¼ç‰¹å¾
        sum_value = sum(numbers) / 20  # å½’ä¸€åŒ–

        # å¥‡å¶æ¯”ç‰¹å¾
        odd_count = sum(1 for num in numbers if num % 2 == 1)
        odd_ratio = odd_count / 20

        # å¤§å°æ¯”ç‰¹å¾ (>40ä¸ºå¤§å·)
        big_count = sum(1 for num in numbers if num > 40)
        big_ratio = big_count / 20

        # åŒºåŸŸåˆ†å¸ƒç‰¹å¾ (8ä¸ªåŒºåŸŸ)
        zone_counts = [0] * 8
        for num in numbers:
            zone_idx = (num - 1) // 10
            zone_counts[zone_idx] += 1
        zone_ratios = [count / 20 for count in zone_counts]

        return [sum_value, odd_ratio, big_ratio] + zone_ratios

    def _predict_next_features(self, transition_counts, state_counts, current_state):
        """é¢„æµ‹ä¸‹ä¸€æœŸç‰¹å¾"""
        alpha = 0.01  # æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘å‚æ•°

        if current_state in transition_counts:
            # æ‰¾åˆ°æœ€å¯èƒ½çš„ä¸‹ä¸€ç‰¹å¾çŠ¶æ€
            feature_probs = {}
            total_count = state_counts[current_state]

            for next_features, count in transition_counts[current_state].items():
                prob = (count + alpha) / (total_count + alpha * len(transition_counts[current_state]))
                feature_probs[next_features] = prob

            # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ç‰¹å¾
            best_features = max(feature_probs.items(), key=lambda x: x[1])[0]
            return list(best_features)
        else:
            # å¦‚æœæ²¡æœ‰åŒ¹é…çš„çŠ¶æ€ï¼Œä½¿ç”¨å†å²å¹³å‡ç‰¹å¾
            return self._get_average_features(transition_counts)

    def _get_average_features(self, transition_counts):
        """è·å–å†å²å¹³å‡ç‰¹å¾"""
        all_features = []
        for state_triple in transition_counts:
            for next_features in transition_counts[state_triple]:
                all_features.append(list(next_features))

        if all_features:
            avg_features = np.mean(all_features, axis=0)
            return avg_features.tolist()
        else:
            # é»˜è®¤ç‰¹å¾
            return [10.5, 0.5, 0.5] + [0.125] * 8

    def _features_to_numbers(self, predicted_features, data, count):
        """æ ¹æ®é¢„æµ‹ç‰¹å¾ç”Ÿæˆå·ç """
        target_sum = predicted_features[0] * 20
        target_odd_ratio = predicted_features[1]
        target_big_ratio = predicted_features[2]
        target_zone_ratios = predicted_features[3:11]

        # ä½¿ç”¨é—ä¼ ç®—æ³•æˆ–å¯å‘å¼æ–¹æ³•ç”Ÿæˆç¬¦åˆç‰¹å¾çš„å·ç ç»„åˆ
        best_combination = self._generate_combination_by_features(
            target_sum, target_odd_ratio, target_big_ratio, target_zone_ratios, count
        )

        # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºç‰¹å¾åŒ¹é…åº¦ï¼‰
        confidence_scores = self._calculate_feature_confidence(
            best_combination, predicted_features
        )

        return best_combination, confidence_scores

    def _generate_combination_by_features(self, target_sum, target_odd_ratio,
                                        target_big_ratio, target_zone_ratios, count):
        """åŸºäºç›®æ ‡ç‰¹å¾ç”Ÿæˆå·ç ç»„åˆ"""
        best_combination = []
        best_score = float('-inf')

        # å¤šæ¬¡éšæœºå°è¯•ï¼Œé€‰æ‹©æœ€ç¬¦åˆç‰¹å¾çš„ç»„åˆ
        for _ in range(1000):
            combination = np.random.choice(range(1, 81), size=count, replace=False).tolist()
            score = self._evaluate_combination(
                combination, target_sum, target_odd_ratio, target_big_ratio, target_zone_ratios
            )

            if score > best_score:
                best_score = score
                best_combination = combination.copy()

        return sorted(best_combination)

    def _evaluate_combination(self, combination, target_sum, target_odd_ratio,
                            target_big_ratio, target_zone_ratios):
        """è¯„ä¼°å·ç ç»„åˆä¸ç›®æ ‡ç‰¹å¾çš„åŒ¹é…åº¦"""
        # å’Œå€¼åŒ¹é…åº¦
        actual_sum = sum(combination)
        sum_score = 1.0 / (1.0 + abs(actual_sum - target_sum))

        # å¥‡å¶æ¯”åŒ¹é…åº¦
        actual_odd_ratio = sum(1 for num in combination if num % 2 == 1) / len(combination)
        odd_score = 1.0 / (1.0 + abs(actual_odd_ratio - target_odd_ratio))

        # å¤§å°æ¯”åŒ¹é…åº¦
        actual_big_ratio = sum(1 for num in combination if num > 40) / len(combination)
        big_score = 1.0 / (1.0 + abs(actual_big_ratio - target_big_ratio))

        # åŒºåŸŸåˆ†å¸ƒåŒ¹é…åº¦
        actual_zone_counts = [0] * 8
        for num in combination:
            zone_idx = (num - 1) // 10
            actual_zone_counts[zone_idx] += 1
        actual_zone_ratios = [count / len(combination) for count in actual_zone_counts]

        zone_score = 0
        for i in range(8):
            zone_score += 1.0 / (1.0 + abs(actual_zone_ratios[i] - target_zone_ratios[i]))
        zone_score /= 8

        # ç»¼åˆè¯„åˆ†
        return (sum_score + odd_score + big_score + zone_score) / 4

    def _calculate_feature_confidence(self, combination, predicted_features):
        """è®¡ç®—åŸºäºç‰¹å¾çš„ç½®ä¿¡åº¦"""
        confidence = self._evaluate_combination(
            combination,
            predicted_features[0] * 20,
            predicted_features[1],
            predicted_features[2],
            predicted_features[3:11]
        )

        # ä¸ºæ¯ä¸ªå·ç åˆ†é…ç›¸åŒçš„ç½®ä¿¡åº¦
        return [confidence] * len(combination)


class AdaptiveMarkovPredictor:
    """è‡ªé€‚åº”é©¬å°”å¯å¤«é“¾é¢„æµ‹å™¨ - 1-5é˜¶æ™ºèƒ½èåˆ"""

    def __init__(self, analyzer):
        self.analyzer = analyzer
        self.base_predictors = {
            1: MarkovPredictor(analyzer),
            2: Markov2ndPredictor(analyzer),
            3: Markov3rdPredictor(analyzer)
        }

    def predict(self, data: pd.DataFrame, count: int = 30, **kwargs) -> Tuple[List[int], List[float]]:
        """è‡ªé€‚åº”é©¬å°”å¯å¤«é“¾é¢„æµ‹ - å¤šé˜¶èåˆ"""
        print(f"ğŸ”„ æ‰§è¡Œè‡ªé€‚åº”é©¬å°”å¯å¤«é“¾é¢„æµ‹...")
        print(f"åˆ†ææ•°æ®: {len(data)}æœŸ")

        # åŠ¨æ€æƒé‡åˆ†é…
        weights = self._calculate_adaptive_weights(data)
        print(f"åŠ¨æ€æƒé‡: {weights}")

        # æ”¶é›†å„é˜¶é¢„æµ‹ç»“æœ
        all_predictions = {}
        all_confidences = {}

        for order, weight in weights.items():
            if weight > 0:
                try:
                    if order in self.base_predictors:
                        numbers, confidences = self.base_predictors[order].predict(data, count * 2)
                        all_predictions[order] = numbers
                        all_confidences[order] = confidences
                        print(f"{order}é˜¶é¢„æµ‹å®Œæˆ: {len(numbers)}ä¸ªå·ç ")
                except Exception as e:
                    print(f"âš ï¸ {order}é˜¶é¢„æµ‹å¤±è´¥: {e}")
                    weights[order] = 0

        # é‡æ–°å½’ä¸€åŒ–æƒé‡
        total_weight = sum(weights.values())
        if total_weight > 0:
            weights = {k: v/total_weight for k, v in weights.items()}

        # èåˆé¢„æµ‹ç»“æœ
        final_numbers, final_confidences = self._fuse_predictions(
            all_predictions, all_confidences, weights, count
        )

        print(f"âœ… è‡ªé€‚åº”é©¬å°”å¯å¤«é“¾é¢„æµ‹å®Œæˆ")
        print(f"èåˆäº† {len([w for w in weights.values() if w > 0])} ä¸ªé¢„æµ‹å™¨")
        print(f"é¢„æµ‹å·ç : {final_numbers[:10]}...")
        print(f"å¹³å‡ç½®ä¿¡åº¦: {np.mean(final_confidences):.3f}")

        return final_numbers, final_confidences

    def _calculate_adaptive_weights(self, data):
        """è®¡ç®—è‡ªé€‚åº”æƒé‡"""
        # åŸºç¡€æƒé‡åˆ†é…
        base_weights = {
            1: 0.25,  # 1é˜¶æƒé‡
            2: 0.50,  # 2é˜¶æƒé‡
            3: 0.25   # 3é˜¶æƒé‡
        }

        # åŸºäºæ•°æ®é‡è°ƒæ•´æƒé‡
        data_size = len(data)
        data_factor = min(1.0, data_size / 100)  # 100æœŸä»¥ä¸Šæ•°æ®æ‰èƒ½å……åˆ†å‘æŒ¥é«˜é˜¶ä¼˜åŠ¿

        # è°ƒæ•´æƒé‡
        adjusted_weights = {}
        for order, base_weight in base_weights.items():
            if order == 1:
                # 1é˜¶é©¬å°”å¯å¤«é“¾åœ¨æ•°æ®å°‘æ—¶æƒé‡æ›´é«˜
                adjusted_weights[order] = base_weight * (2.0 - data_factor)
            elif order == 2:
                # 2é˜¶åœ¨ä¸­ç­‰æ•°æ®é‡æ—¶æƒé‡æœ€é«˜
                adjusted_weights[order] = base_weight * (1.0 + data_factor * 0.5)
            else:
                # é«˜é˜¶åœ¨æ•°æ®å……è¶³æ—¶æƒé‡æ›´é«˜
                adjusted_weights[order] = base_weight * data_factor

        # å½’ä¸€åŒ–æƒé‡
        total_weight = sum(adjusted_weights.values())
        if total_weight > 0:
            adjusted_weights = {k: v/total_weight for k, v in adjusted_weights.items()}

        return adjusted_weights

    def _fuse_predictions(self, all_predictions, all_confidences, weights, count):
        """èåˆå¤šä¸ªé¢„æµ‹ç»“æœ"""
        # æ”¶é›†æ‰€æœ‰å€™é€‰å·ç åŠå…¶åŠ æƒç½®ä¿¡åº¦
        number_scores = {}

        for order, numbers in all_predictions.items():
            weight = weights.get(order, 0)
            confidences = all_confidences.get(order, [])

            for i, number in enumerate(numbers):
                confidence = confidences[i] if i < len(confidences) else 0.1
                weighted_score = confidence * weight

                if number not in number_scores:
                    number_scores[number] = 0
                number_scores[number] += weighted_score

        # æŒ‰åŠ æƒå¾—åˆ†æ’åº
        sorted_numbers = sorted(number_scores.items(), key=lambda x: x[1], reverse=True)

        # é€‰æ‹©å‰countä¸ªå·ç 
        final_numbers = [num for num, score in sorted_numbers[:count]]
        final_confidences = [score for num, score in sorted_numbers[:count]]

        # å½’ä¸€åŒ–ç½®ä¿¡åº¦
        if final_confidences:
            max_conf = max(final_confidences)
            if max_conf > 0:
                final_confidences = [conf / max_conf for conf in final_confidences]

        return final_numbers, final_confidences


class LSTMPredictor:
    """LSTMç¥ç»ç½‘ç»œé¢„æµ‹å™¨"""

    def __init__(self, analyzer):
        self.analyzer = analyzer
        self.model = None
        self.scaler = StandardScaler()
    
    def predict(self, data: pd.DataFrame, count: int = 30, **kwargs) -> Tuple[List[int], List[float]]:
        """LSTMé¢„æµ‹"""
        if not TF_AVAILABLE:
            print("TensorFlowæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨LSTMé¢„æµ‹")
            # è¿”å›åŸºäºé¢‘ç‡çš„é¢„æµ‹ä½œä¸ºfallback
            frequency_predictor = FrequencyPredictor(self.analyzer)
            return frequency_predictor.predict(data, count)
        
        print("æ‰§è¡ŒLSTMç¥ç»ç½‘ç»œé¢„æµ‹...")
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        X, y = self._prepare_training_data(data)
        
        if X.size == 0:
            print("è®­ç»ƒæ•°æ®ä¸è¶³ï¼Œä½¿ç”¨é¢‘ç‡åˆ†æé¢„æµ‹")
            frequency_predictor = FrequencyPredictor(self.analyzer)
            return frequency_predictor.predict(data, count)
        
        # è®­ç»ƒæ¨¡å‹
        if self.model is None:
            self.model = self._build_model(X.shape)
            self._train_model(X, y)
        
        # æ‰§è¡Œé¢„æµ‹
        predictions = self._predict_numbers(X, count)
        
        return predictions
    
    def _prepare_training_data(self, data: pd.DataFrame, sequence_length: int = 10):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        if len(data) < sequence_length + 1:
            return np.array([]), np.array([])
        
        features = []
        targets = []
        
        for i in range(len(data) - sequence_length):
            # è¾“å…¥åºåˆ—
            sequence_data = data.iloc[i:i+sequence_length]
            sequence_features = []
            
            for _, row in sequence_data.iterrows():
                numbers = [row[f'num{j}'] for j in range(1, 21)]
                feature_vector = self._extract_features(numbers)
                sequence_features.append(feature_vector)
            
            features.append(sequence_features)
            
            # ç›®æ ‡ï¼šä¸‹ä¸€æœŸçš„å·ç 
            next_row = data.iloc[i + sequence_length]
            next_numbers = [next_row[f'num{j}'] for j in range(1, 21)]
            targets.append(self._encode_target(next_numbers))
        
        X = np.array(features)
        y = np.array(targets)
        
        return X, y
    
    def _extract_features(self, numbers: List[int]) -> List[float]:
        """æå–ç‰¹å¾å‘é‡"""
        features = []
        
        # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
        features.extend([
            sum(numbers) / 20,  # å¹³å‡å€¼
            (max(numbers) - min(numbers)) / 80,  # å½’ä¸€åŒ–è·¨åº¦
            sum(1 for n in numbers if n % 2 == 1) / 20,  # å¥‡æ•°æ¯”ä¾‹
            sum(1 for n in numbers if n >= 41) / 20,  # å¤§å·æ¯”ä¾‹
        ])
        
        # åŒºåŸŸåˆ†å¸ƒç‰¹å¾
        zone_counts = [0] * 8
        for num in numbers:
            zone_idx = (num - 1) // 10
            zone_counts[zone_idx] += 1
        
        features.extend([count / 20 for count in zone_counts])
        
        # å·ç åˆ†å¸ƒç‰¹å¾ (ç®€åŒ–ä¸º10ä¸ªåŒºé—´)
        interval_counts = [0] * 10
        for num in numbers:
            interval_idx = min((num - 1) // 8, 9)
            interval_counts[interval_idx] += 1
        
        features.extend([count / 20 for count in interval_counts])
        
        return features
    
    def _encode_target(self, numbers: List[int]) -> List[float]:
        """ç¼–ç ç›®æ ‡"""
        target = [0.0] * 80
        for num in numbers:
            target[num - 1] = 1.0
        return target
    
    def _build_model(self, input_shape):
        """æ„å»ºLSTMæ¨¡å‹"""
        model = tf.keras.Sequential([
            tf.keras.layers.LSTM(64, return_sequences=True, input_shape=input_shape[1:]),
            tf.keras.layers.Dropout(0.2),
            tf.keras.layers.LSTM(32, return_sequences=False),
            tf.keras.layers.Dropout(0.2),
            tf.keras.layers.Dense(64, activation='relu'),
            tf.keras.layers.Dropout(0.3),
            tf.keras.layers.Dense(80, activation='sigmoid')  # 80ä¸ªå·ç çš„æ¦‚ç‡
        ])
        
        model.compile(
            optimizer='adam',
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    def _train_model(self, X, y):
        """è®­ç»ƒæ¨¡å‹"""
        print("å¼€å§‹è®­ç»ƒLSTMæ¨¡å‹...")
        
        # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯æ•°æ®
        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # æ—©åœå›è°ƒ
        early_stopping = tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True
        )
        
        # è®­ç»ƒæ¨¡å‹
        history = self.model.fit(
            X_train, y_train,
            epochs=100,
            batch_size=32,
            validation_data=(X_val, y_val),
            callbacks=[early_stopping],
            verbose=0
        )
        
        print(f"LSTMæ¨¡å‹è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆéªŒè¯æŸå¤±: {min(history.history['val_loss']):.4f}")
    
    def _predict_numbers(self, X, count: int) -> Tuple[List[int], List[float]]:
        """é¢„æµ‹å·ç """
        # ä½¿ç”¨æœ€åä¸€ä¸ªåºåˆ—è¿›è¡Œé¢„æµ‹
        if len(X) > 0:
            last_sequence = X[-1:]
        else:
            # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œåˆ›å»ºé›¶åºåˆ—
            last_sequence = np.zeros((1, 10, 22))
        
        # é¢„æµ‹æ¦‚ç‡
        probabilities = self.model.predict(last_sequence, verbose=0)[0]
        
        # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„å·ç 
        number_probs = [(i + 1, prob) for i, prob in enumerate(probabilities)]
        number_probs.sort(key=lambda x: x[1], reverse=True)
        
        predicted_numbers = [num for num, _ in number_probs[:count]]
        confidence_scores = [prob for _, prob in number_probs[:count]]
        
        return predicted_numbers, confidence_scores


class TransformerPredictor:
    """Transformeræ¨¡å‹é¢„æµ‹å™¨ - åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„åºåˆ—é¢„æµ‹"""

    def __init__(self, analyzer):
        self.analyzer = analyzer
        self.model = None
        self.scaler = StandardScaler()
        self.vocab_size = 81  # 1-80å·ç  + padding token
        self.d_model = 64
        self.num_heads = 8
        self.num_layers = 3
        self.max_seq_length = 20

    def predict(self, data: pd.DataFrame, count: int = 30, **kwargs) -> Tuple[List[int], List[float]]:
        """Transformeré¢„æµ‹"""
        print(f"ğŸ”„ æ‰§è¡ŒTransformeræ¨¡å‹é¢„æµ‹...")
        print(f"åˆ†ææ•°æ®: {len(data)}æœŸ")

        try:
            import torch
            import torch.nn as nn
            import torch.nn.functional as F
            from torch.utils.data import DataLoader, TensorDataset

            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®
            if len(data) < 10:
                print("âš ï¸ æ•°æ®ä¸è¶³ï¼Œä½¿ç”¨é¢‘ç‡åˆ†æä½œä¸ºåå¤‡")
                frequency_predictor = FrequencyPredictor(self.analyzer)
                return frequency_predictor.predict(data, count)

            # å‡†å¤‡è®­ç»ƒæ•°æ®
            sequences, targets = self._prepare_sequences(data)

            if len(sequences) == 0:
                print("âš ï¸ æ— æ³•æ„å»ºåºåˆ—ï¼Œä½¿ç”¨é¢‘ç‡åˆ†æä½œä¸ºåå¤‡")
                frequency_predictor = FrequencyPredictor(self.analyzer)
                return frequency_predictor.predict(data, count)

            # æ„å»ºå’Œè®­ç»ƒæ¨¡å‹
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            print(f"ä½¿ç”¨è®¾å¤‡: {device}")

            model = self._build_transformer_model().to(device)

            # è®­ç»ƒæ¨¡å‹
            self._train_model(model, sequences, targets, device)

            # é¢„æµ‹
            predicted_numbers, confidence_scores = self._predict_with_model(
                model, sequences, device, count
            )

            print(f"âœ… Transformeré¢„æµ‹å®Œæˆ")
            print(f"é¢„æµ‹å·ç : {predicted_numbers[:10]}...")
            print(f"å¹³å‡ç½®ä¿¡åº¦: {np.mean(confidence_scores):.3f}")

            return predicted_numbers, confidence_scores

        except ImportError:
            print("âš ï¸ PyTorchæœªå®‰è£…ï¼Œä½¿ç”¨é¢‘ç‡åˆ†æä½œä¸ºåå¤‡")
            frequency_predictor = FrequencyPredictor(self.analyzer)
            return frequency_predictor.predict(data, count)
        except Exception as e:
            print(f"âš ï¸ Transformeré¢„æµ‹å¤±è´¥: {e}")
            frequency_predictor = FrequencyPredictor(self.analyzer)
            return frequency_predictor.predict(data, count)

    def _prepare_sequences(self, data: pd.DataFrame):
        """å‡†å¤‡åºåˆ—æ•°æ®"""
        sequences = []
        targets = []

        # å°†æ¯æœŸå·ç è½¬æ¢ä¸ºåºåˆ—
        all_numbers = []
        for _, row in data.iterrows():
            numbers = [int(row[f'num{i}']) for i in range(1, 21)]
            all_numbers.append(numbers)

        # åˆ›å»ºæ»‘åŠ¨çª—å£åºåˆ—
        seq_length = 10  # ä½¿ç”¨å‰10æœŸé¢„æµ‹ä¸‹ä¸€æœŸ

        for i in range(len(all_numbers) - seq_length):
            # è¾“å…¥åºåˆ—ï¼šå‰seq_lengthæœŸçš„å·ç 
            input_seq = []
            for j in range(seq_length):
                input_seq.extend(all_numbers[i + j])

            # ç›®æ ‡ï¼šä¸‹ä¸€æœŸçš„å·ç 
            target = all_numbers[i + seq_length]

            sequences.append(input_seq)
            targets.append(target)

        return sequences, targets

    def _build_transformer_model(self):
        """æ„å»ºTransformeræ¨¡å‹"""
        import torch
        import torch.nn as nn

        class TransformerModel(nn.Module):
            def __init__(self, vocab_size, d_model, num_heads, num_layers, max_seq_length):
                super(TransformerModel, self).__init__()
                self.d_model = d_model
                self.embedding = nn.Embedding(vocab_size, d_model)
                self.pos_encoding = self._create_positional_encoding(max_seq_length, d_model)

                encoder_layer = nn.TransformerEncoderLayer(
                    d_model=d_model,
                    nhead=num_heads,
                    dim_feedforward=256,
                    dropout=0.1,
                    batch_first=True
                )
                self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

                self.output_projection = nn.Linear(d_model, 80)  # è¾“å‡º80ä¸ªå·ç çš„æ¦‚ç‡
                self.dropout = nn.Dropout(0.1)

            def _create_positional_encoding(self, max_len, d_model):
                import torch
                import math

                pe = torch.zeros(max_len, d_model)
                position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
                div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                                   (-math.log(10000.0) / d_model))

                pe[:, 0::2] = torch.sin(position * div_term)
                pe[:, 1::2] = torch.cos(position * div_term)
                return pe.unsqueeze(0)

            def forward(self, x):
                import math
                # x shape: (batch_size, seq_len)
                seq_len = x.size(1)

                # åµŒå…¥å’Œä½ç½®ç¼–ç 
                x = self.embedding(x) * math.sqrt(self.d_model)
                x = x + self.pos_encoding[:, :seq_len, :].to(x.device)
                x = self.dropout(x)

                # Transformerç¼–ç 
                x = self.transformer(x)

                # å…¨å±€å¹³å‡æ± åŒ–
                x = torch.mean(x, dim=1)

                # è¾“å‡ºæŠ•å½±
                x = self.output_projection(x)
                return torch.sigmoid(x)

        return TransformerModel(
            self.vocab_size, self.d_model, self.num_heads,
            self.num_layers, self.max_seq_length * 20
        )

    def _train_model(self, model, sequences, targets, device):
        """è®­ç»ƒTransformeræ¨¡å‹"""
        import torch
        import torch.nn as nn
        from torch.utils.data import DataLoader, TensorDataset

        print("å¼€å§‹è®­ç»ƒTransformeræ¨¡å‹...")

        # å‡†å¤‡æ•°æ®
        X = torch.tensor(sequences, dtype=torch.long).to(device)

        # å°†ç›®æ ‡è½¬æ¢ä¸ºå¤šæ ‡ç­¾æ ¼å¼
        y = torch.zeros(len(targets), 80).to(device)
        for i, target_numbers in enumerate(targets):
            for num in target_numbers:
                if 1 <= num <= 80:
                    y[i, num - 1] = 1.0

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataset = TensorDataset(X, y)
        dataloader = DataLoader(dataset, batch_size=16, shuffle=True)

        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        criterion = nn.BCELoss()

        # è®­ç»ƒå¾ªç¯
        model.train()
        num_epochs = 50

        for epoch in range(num_epochs):
            total_loss = 0
            for batch_x, batch_y in dataloader:
                optimizer.zero_grad()

                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)

                loss.backward()
                optimizer.step()

                total_loss += loss.item()

            if (epoch + 1) % 10 == 0:
                avg_loss = total_loss / len(dataloader)
                print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}")

        print("Transformeræ¨¡å‹è®­ç»ƒå®Œæˆ")

    def _predict_with_model(self, model, sequences, device, count):
        """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        import torch

        model.eval()

        # ä½¿ç”¨æœ€åä¸€ä¸ªåºåˆ—è¿›è¡Œé¢„æµ‹
        if len(sequences) > 0:
            last_seq = torch.tensor([sequences[-1]], dtype=torch.long).to(device)
        else:
            # åˆ›å»ºéšæœºåºåˆ—ä½œä¸ºåå¤‡
            last_seq = torch.randint(1, 81, (1, 200)).to(device)

        with torch.no_grad():
            probabilities = model(last_seq)[0].cpu().numpy()

        # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„å·ç 
        number_probs = [(i + 1, prob) for i, prob in enumerate(probabilities)]
        number_probs.sort(key=lambda x: x[1], reverse=True)

        predicted_numbers = [num for num, _ in number_probs[:count]]
        confidence_scores = [float(prob) for _, prob in number_probs[:count]]

        return predicted_numbers, confidence_scores


class GraphNeuralNetworkPredictor:
    """å›¾ç¥ç»ç½‘ç»œé¢„æµ‹å™¨ - åŸºäºå·ç å…³ç³»å›¾çš„é¢„æµ‹"""

    def __init__(self, analyzer):
        self.analyzer = analyzer
        self.model = None

    def predict(self, data: pd.DataFrame, count: int = 30, **kwargs) -> Tuple[List[int], List[float]]:
        """å›¾ç¥ç»ç½‘ç»œé¢„æµ‹"""
        print(f"ğŸ”„ æ‰§è¡Œå›¾ç¥ç»ç½‘ç»œé¢„æµ‹...")
        print(f"åˆ†ææ•°æ®: {len(data)}æœŸ")

        try:
            import torch
            import torch.nn as nn
            import torch.nn.functional as F

            # æ£€æŸ¥æ•°æ®é‡
            if len(data) < 20:
                print("âš ï¸ æ•°æ®ä¸è¶³ï¼Œä½¿ç”¨é¢‘ç‡åˆ†æä½œä¸ºåå¤‡")
                frequency_predictor = FrequencyPredictor(self.analyzer)
                return frequency_predictor.predict(data, count)

            # æ„å»ºå·ç å…³ç³»å›¾
            adjacency_matrix, node_features = self._build_number_graph(data)

            # æ„å»ºå’Œè®­ç»ƒGNNæ¨¡å‹
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            print(f"ä½¿ç”¨è®¾å¤‡: {device}")

            model = self._build_gnn_model().to(device)

            # è®­ç»ƒæ¨¡å‹
            self._train_gnn_model(model, adjacency_matrix, node_features, data, device)

            # é¢„æµ‹
            predicted_numbers, confidence_scores = self._predict_with_gnn(
                model, adjacency_matrix, node_features, device, count
            )

            print(f"âœ… å›¾ç¥ç»ç½‘ç»œé¢„æµ‹å®Œæˆ")
            print(f"é¢„æµ‹å·ç : {predicted_numbers[:10]}...")
            print(f"å¹³å‡ç½®ä¿¡åº¦: {np.mean(confidence_scores):.3f}")

            return predicted_numbers, confidence_scores

        except ImportError:
            print("âš ï¸ PyTorchæœªå®‰è£…ï¼Œä½¿ç”¨é¢‘ç‡åˆ†æä½œä¸ºåå¤‡")
            frequency_predictor = FrequencyPredictor(self.analyzer)
            return frequency_predictor.predict(data, count)
        except Exception as e:
            print(f"âš ï¸ å›¾ç¥ç»ç½‘ç»œé¢„æµ‹å¤±è´¥: {e}")
            frequency_predictor = FrequencyPredictor(self.analyzer)
            return frequency_predictor.predict(data, count)

    def _build_number_graph(self, data: pd.DataFrame):
        """æ„å»ºå·ç å…³ç³»å›¾"""
        print("æ„å»ºå·ç å…³ç³»å›¾...")

        # åˆå§‹åŒ–é‚»æ¥çŸ©é˜µ (80x80)
        adjacency_matrix = np.zeros((80, 80))

        # ç»Ÿè®¡å·ç å…±ç°é¢‘ç‡
        for _, row in data.iterrows():
            numbers = [int(row[f'num{i}']) for i in range(1, 21)]

            # è®¡ç®—å·ç é—´çš„å…±ç°å…³ç³»
            for i in range(len(numbers)):
                for j in range(i + 1, len(numbers)):
                    num1, num2 = numbers[i] - 1, numbers[j] - 1  # è½¬æ¢ä¸º0-79ç´¢å¼•
                    adjacency_matrix[num1][num2] += 1
                    adjacency_matrix[num2][num1] += 1  # æ— å‘å›¾

        # å½’ä¸€åŒ–é‚»æ¥çŸ©é˜µ
        max_weight = np.max(adjacency_matrix)
        if max_weight > 0:
            adjacency_matrix = adjacency_matrix / max_weight

        # æ„å»ºèŠ‚ç‚¹ç‰¹å¾ (æ¯ä¸ªå·ç çš„ç»Ÿè®¡ç‰¹å¾)
        node_features = self._build_node_features(data)

        print(f"å›¾æ„å»ºå®Œæˆ: 80ä¸ªèŠ‚ç‚¹, {np.sum(adjacency_matrix > 0)//2}æ¡è¾¹")

        return adjacency_matrix, node_features

    def _build_node_features(self, data: pd.DataFrame):
        """æ„å»ºèŠ‚ç‚¹ç‰¹å¾"""
        node_features = np.zeros((80, 5))  # 5ç»´ç‰¹å¾

        # ç»Ÿè®¡æ¯ä¸ªå·ç çš„ç‰¹å¾
        for num in range(1, 81):
            # ç‰¹å¾1: å‡ºç°é¢‘ç‡
            frequency = 0
            # ç‰¹å¾2: æœ€è¿‘å‡ºç°ä½ç½®çš„å¹³å‡å€¼
            recent_positions = []
            # ç‰¹å¾3: ä¸å…¶ä»–å·ç çš„å¹³å‡å…±ç°åº¦
            cooccurrence = 0
            # ç‰¹å¾4: å¥‡å¶æ€§ (0=å¶æ•°, 1=å¥‡æ•°)
            parity = num % 2
            # ç‰¹å¾5: å¤§å° (å½’ä¸€åŒ–åˆ°0-1)
            size = (num - 1) / 79

            for idx, row in data.iterrows():
                numbers = [int(row[f'num{i}']) for i in range(1, 21)]
                if num in numbers:
                    frequency += 1
                    recent_positions.append(numbers.index(num))
                    # è®¡ç®—ä¸å…¶ä»–å·ç çš„å…±ç°
                    cooccurrence += len([n for n in numbers if n != num])

            # å½’ä¸€åŒ–ç‰¹å¾
            frequency = frequency / len(data) if len(data) > 0 else 0
            avg_position = np.mean(recent_positions) / 19 if recent_positions else 0.5
            cooccurrence = cooccurrence / (len(data) * 19) if len(data) > 0 else 0

            node_features[num - 1] = [frequency, avg_position, cooccurrence, parity, size]

        return node_features

    def _build_gnn_model(self):
        """æ„å»ºå›¾ç¥ç»ç½‘ç»œæ¨¡å‹"""
        import torch
        import torch.nn as nn
        import torch.nn.functional as F

        class GraphConvLayer(nn.Module):
            def __init__(self, in_features, out_features):
                super(GraphConvLayer, self).__init__()
                self.linear = nn.Linear(in_features, out_features)

            def forward(self, x, adj):
                # x: (num_nodes, in_features)
                # adj: (num_nodes, num_nodes)
                support = self.linear(x)
                output = torch.mm(adj, support)
                return F.relu(output)

        class GNNModel(nn.Module):
            def __init__(self, input_dim, hidden_dim, output_dim):
                super(GNNModel, self).__init__()
                self.gc1 = GraphConvLayer(input_dim, hidden_dim)
                self.gc2 = GraphConvLayer(hidden_dim, hidden_dim)
                self.gc3 = GraphConvLayer(hidden_dim, output_dim)
                self.dropout = nn.Dropout(0.2)

            def forward(self, x, adj):
                x = self.gc1(x, adj)
                x = self.dropout(x)
                x = self.gc2(x, adj)
                x = self.dropout(x)
                x = self.gc3(x, adj)
                return torch.sigmoid(x)

        return GNNModel(input_dim=5, hidden_dim=32, output_dim=1)

    def _train_gnn_model(self, model, adjacency_matrix, node_features, data, device):
        """è®­ç»ƒGNNæ¨¡å‹"""
        import torch
        import torch.nn as nn

        print("å¼€å§‹è®­ç»ƒå›¾ç¥ç»ç½‘ç»œæ¨¡å‹...")

        # è½¬æ¢ä¸ºå¼ é‡
        adj_tensor = torch.FloatTensor(adjacency_matrix).to(device)
        features_tensor = torch.FloatTensor(node_features).to(device)

        # æ„å»ºè®­ç»ƒç›®æ ‡ (æ¯æœŸå‡ºç°çš„å·ç ä¸ºæ­£æ ·æœ¬)
        targets = torch.zeros(80, len(data)).to(device)
        for idx, (_, row) in enumerate(data.iterrows()):
            numbers = [int(row[f'num{i}']) for i in range(1, 21)]
            for num in numbers:
                targets[num - 1, idx] = 1.0

        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
        criterion = nn.BCELoss()

        # è®­ç»ƒå¾ªç¯
        model.train()
        num_epochs = 100

        for epoch in range(num_epochs):
            optimizer.zero_grad()

            # å‰å‘ä¼ æ’­
            outputs = model(features_tensor, adj_tensor).squeeze()  # (80,)

            # è®¡ç®—å¹³å‡æŸå¤± (å¯¹æ‰€æœ‰æœŸçš„å¹³å‡)
            total_loss = 0
            for period_idx in range(targets.shape[1]):
                period_targets = targets[:, period_idx]
                loss = criterion(outputs, period_targets)
                total_loss += loss

            avg_loss = total_loss / targets.shape[1]
            avg_loss.backward()
            optimizer.step()

            if (epoch + 1) % 20 == 0:
                print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss.item():.4f}")

        print("å›¾ç¥ç»ç½‘ç»œæ¨¡å‹è®­ç»ƒå®Œæˆ")

    def _predict_with_gnn(self, model, adjacency_matrix, node_features, device, count):
        """ä½¿ç”¨GNNæ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        import torch

        model.eval()

        # è½¬æ¢ä¸ºå¼ é‡
        adj_tensor = torch.FloatTensor(adjacency_matrix).to(device)
        features_tensor = torch.FloatTensor(node_features).to(device)

        with torch.no_grad():
            probabilities = model(features_tensor, adj_tensor).squeeze().cpu().numpy()

        # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„å·ç 
        number_probs = [(i + 1, prob) for i, prob in enumerate(probabilities)]
        number_probs.sort(key=lambda x: x[1], reverse=True)

        predicted_numbers = [num for num, _ in number_probs[:count]]
        confidence_scores = [float(prob) for _, prob in number_probs[:count]]

        return predicted_numbers, confidence_scores


class EnsemblePredictor:
    """é›†æˆå­¦ä¹ é¢„æµ‹å™¨"""
    
    def __init__(self, analyzer):
        self.analyzer = analyzer
        self.base_predictors = {
            'frequency': FrequencyPredictor(analyzer),
            'hot_cold': HotColdPredictor(analyzer),
            'missing': MissingPredictor(analyzer),
            'markov': MarkovPredictor(analyzer)
        }
    
    def predict(self, data: pd.DataFrame, count: int = 30, **kwargs) -> Tuple[List[int], List[float]]:
        """é›†æˆé¢„æµ‹"""
        print("æ‰§è¡Œé›†æˆå­¦ä¹ é¢„æµ‹...")
        
        # æ”¶é›†å„ä¸ªé¢„æµ‹å™¨çš„ç»“æœ
        all_predictions = {}
        weights = {'frequency': 0.3, 'hot_cold': 0.25, 'missing': 0.2, 'markov': 0.25}
        
        for name, predictor in self.base_predictors.items():
            try:
                numbers, scores = predictor.predict(data, count=count * 2)  # è·å–æ›´å¤šå€™é€‰
                all_predictions[name] = list(zip(numbers, scores))
            except Exception as e:
                print(f"é¢„æµ‹å™¨ {name} æ‰§è¡Œå¤±è´¥: {e}")
                all_predictions[name] = []
        
        # èåˆé¢„æµ‹ç»“æœ
        final_scores = {}
        
        for name, predictions in all_predictions.items():
            weight = weights.get(name, 0.1)
            for num, score in predictions:
                if num not in final_scores:
                    final_scores[num] = 0
                final_scores[num] += weight * score
        
        # æ’åºå¹¶é€‰æ‹©å‰countä¸ª
        sorted_predictions = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)
        
        predicted_numbers = [num for num, _ in sorted_predictions[:count]]
        confidence_scores = [score for _, score in sorted_predictions[:count]]
        
        return predicted_numbers, confidence_scores


class PredictionEngine:
    """é¢„æµ‹å¼•æ“"""
    
    def __init__(self, analyzer):
        self.analyzer = analyzer
        self.predictors = {
            'frequency': FrequencyPredictor(analyzer),
            'hot_cold': HotColdPredictor(analyzer),
            'missing': MissingPredictor(analyzer),
            'markov': MarkovPredictor(analyzer),
            'markov_2nd': Markov2ndPredictor(analyzer),
            'markov_3rd': Markov3rdPredictor(analyzer),
            'adaptive_markov': AdaptiveMarkovPredictor(analyzer),
            'transformer': TransformerPredictor(analyzer),
            'gnn': GraphNeuralNetworkPredictor(analyzer),
            'lstm': LSTMPredictor(analyzer),
            'ensemble': EnsemblePredictor(analyzer)
        }
    
    def predict(self,
                data: pd.DataFrame,
                target_issue: str,
                count: int,
                method: str,
                **kwargs) -> PredictionResult:
        """æ‰§è¡Œé¢„æµ‹"""
        
        if method not in self.predictors:
            raise ValueError(f"ä¸æ”¯æŒçš„é¢„æµ‹æ–¹æ³•: {method}")
        
        predictor = self.predictors[method]
        
        # æ‰§è¡Œé¢„æµ‹
        start_time = time.time()
        predicted_numbers, confidence_scores = predictor.predict(
            data=data,
            count=count,
            **kwargs
        )
        execution_time = time.time() - start_time
        
        return PredictionResult(
            target_issue=target_issue,
            analysis_periods=len(data),
            method=method,
            predicted_numbers=predicted_numbers,
            confidence_scores=confidence_scores,
            generation_time=datetime.now(),
            execution_time=execution_time,
            parameters=kwargs
        )
    
    def get_available_methods(self) -> List[str]:
        """è·å–å¯ç”¨çš„é¢„æµ‹æ–¹æ³•"""
        return list(self.predictors.keys())


class ComparisonEngine:
    """ç»“æœå¯¹æ¯”å¼•æ“"""
    
    def __init__(self, analyzer):
        self.analyzer = analyzer
    
    def compare(self,
                target_issue: str,
                predicted_numbers: List[int],
                actual_numbers: List[int]) -> ComparisonResult:
        """å¯¹æ¯”é¢„æµ‹ç»“æœ"""
        
        # è®¡ç®—å‘½ä¸­æƒ…å†µ
        hit_numbers = [num for num in predicted_numbers if num in actual_numbers]
        miss_numbers = [num for num in predicted_numbers if num not in actual_numbers]
        
        hit_count = len(hit_numbers)
        total_predicted = len(predicted_numbers)
        hit_rate = hit_count / total_predicted if total_predicted > 0 else 0
        
        # åˆ†æå‘½ä¸­åˆ†å¸ƒ
        hit_distribution = self._analyze_hit_distribution(hit_numbers)
        
        return ComparisonResult(
            target_issue=target_issue,
            predicted_numbers=predicted_numbers,
            actual_numbers=actual_numbers,
            hit_numbers=hit_numbers,
            miss_numbers=miss_numbers,
            hit_count=hit_count,
            total_predicted=total_predicted,
            hit_rate=hit_rate,
            hit_distribution=hit_distribution,
            comparison_time=datetime.now()
        )
    
    def _analyze_hit_distribution(self, hit_numbers: List[int]) -> Dict[str, int]:
        """åˆ†æå‘½ä¸­åˆ†å¸ƒ"""
        distribution = {
            'small_numbers': sum(1 for n in hit_numbers if n <= 40),
            'big_numbers': sum(1 for n in hit_numbers if n >= 41),
            'odd_numbers': sum(1 for n in hit_numbers if n % 2 == 1),
            'even_numbers': sum(1 for n in hit_numbers if n % 2 == 0)
        }
        
        # åŒºåŸŸåˆ†å¸ƒ
        for i in range(8):
            start = i * 10 + 1
            end = (i + 1) * 10
            zone_hits = sum(1 for n in hit_numbers if start <= n <= end)
            distribution[f'zone_{i+1}'] = zone_hits
        
        return distribution


class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.prediction_history = []
        self.performance_stats = {}
    
    def record_prediction(self, method: str, execution_time: float, hit_rate: float = None):
        """è®°å½•é¢„æµ‹æ€§èƒ½"""
        record = {
            'method': method,
            'execution_time': execution_time,
            'hit_rate': hit_rate,
            'timestamp': datetime.now()
        }
        self.prediction_history.append(record)
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        if method not in self.performance_stats:
            self.performance_stats[method] = {
                'total_predictions': 0,
                'total_time': 0,
                'total_hit_rate': 0,
                'count_with_hit_rate': 0
            }
        
        stats = self.performance_stats[method]
        stats['total_predictions'] += 1
        stats['total_time'] += execution_time
        
        if hit_rate is not None:
            stats['total_hit_rate'] += hit_rate
            stats['count_with_hit_rate'] += 1
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        summary = {}
        
        for method, stats in self.performance_stats.items():
            avg_time = stats['total_time'] / stats['total_predictions']
            avg_hit_rate = (stats['total_hit_rate'] / stats['count_with_hit_rate'] 
                           if stats['count_with_hit_rate'] > 0 else 0)
            
            summary[method] = {
                'avg_execution_time': avg_time,
                'avg_hit_rate': avg_hit_rate,
                'total_predictions': stats['total_predictions']
            }
        
        return summary


class Happy8Analyzer:
    """å¿«ä¹8åˆ†æå™¨æ ¸å¿ƒç±»"""
    
    def __init__(self, data_dir: str = "data"):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.data_dir = Path(data_dir)
        self.data_manager = DataManager(data_dir)
        self.prediction_engine = PredictionEngine(self)
        self.comparison_engine = ComparisonEngine(self)
        self.performance_monitor = PerformanceMonitor()
        
        # æ•°æ®ç¼“å­˜
        self.historical_data = None
        
        print("å¿«ä¹8æ™ºèƒ½é¢„æµ‹ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def load_data(self, periods: Optional[int] = None) -> pd.DataFrame:
        """åŠ è½½å†å²æ•°æ®"""
        if self.historical_data is None:
            self.historical_data = self.data_manager.load_historical_data()

        if periods and periods > 0:
            return self.historical_data.head(periods)  # æ”¹ä¸ºheadï¼Œå› ä¸ºæ•°æ®å·²ç»æŒ‰æœ€æ–°æœŸå·æ’åº
        return self.historical_data

    def crawl_latest_data(self, limit: int = 100) -> pd.DataFrame:
        """çˆ¬å–æœ€æ–°æ•°æ®"""
        try:
            self.data_manager.crawl_initial_data(limit)
            # é‡æ–°åŠ è½½æ•°æ®
            self.historical_data = None
            return self.load_data()
        except Exception as e:
            print(f"çˆ¬å–æœ€æ–°æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()

    def crawl_all_historical_data(self) -> int:
        """çˆ¬å–æ‰€æœ‰å†å²æ•°æ®"""
        try:
            total_crawled = self.data_manager.crawl_all_historical_data()
            # é‡æ–°åŠ è½½æ•°æ®
            self.historical_data = None
            return total_crawled
        except Exception as e:
            print(f"çˆ¬å–æ‰€æœ‰å†å²æ•°æ®å¤±è´¥: {e}")
            return 0
    
    def predict(self,
                target_issue: str,
                periods: int = 300,
                count: int = 30,
                method: str = 'frequency',
                **kwargs) -> PredictionResult:
        """æ™ºèƒ½é¢„æµ‹ - è‡ªåŠ¨åˆ¤æ–­å†å²éªŒè¯æ¨¡å¼æˆ–æœªæ¥é¢„æµ‹æ¨¡å¼"""

        # åŠ è½½æ•°æ®
        data = self.load_data(periods)

        if len(data) == 0:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„å†å²æ•°æ®")

        # æ‰§è¡Œé¢„æµ‹
        result = self.prediction_engine.predict(
            data=data,
            target_issue=target_issue,
            count=count,
            method=method,
            **kwargs
        )

        # è®°å½•æ€§èƒ½
        self.performance_monitor.record_prediction(method, result.execution_time)

        return result

    def predict_with_smart_mode(self,
                               target_issue: str,
                               periods: int = 300,
                               count: int = 30,
                               method: str = 'frequency',
                               **kwargs) -> Dict[str, Any]:
        """æ™ºèƒ½é¢„æµ‹æ¨¡å¼ - è‡ªåŠ¨åˆ¤æ–­å¹¶æ‰§è¡Œç›¸åº”çš„é¢„æµ‹æ¨¡å¼

        Returns:
            DictåŒ…å«:
            - prediction_result: PredictionResultå¯¹è±¡
            - comparison_result: ComparisonResultå¯¹è±¡ï¼ˆä»…å†å²éªŒè¯æ¨¡å¼ï¼‰
            - mode: 'historical_validation' æˆ– 'future_prediction'
            - mode_description: æ¨¡å¼æè¿°
        """

        print(f"ğŸ¯ å¼€å§‹æ™ºèƒ½é¢„æµ‹åˆ†æ...")
        print(f"ç›®æ ‡æœŸå·: {target_issue}")
        print(f"é¢„æµ‹æ–¹æ³•: {method}")
        print(f"ç”Ÿæˆæ•°é‡: {count}ä¸ªå·ç ")
        print("-" * 50)

        # æ£€æŸ¥ç›®æ ‡æœŸå·æ˜¯å¦å­˜åœ¨äºå†å²æ•°æ®ä¸­
        is_historical = self._check_issue_exists(target_issue)

        if is_historical:
            # æ¨¡å¼1: å†å²éªŒè¯æ¨¡å¼
            print("ğŸ“Š æ£€æµ‹åˆ°å†å²æœŸå·ï¼Œå¯åŠ¨ã€å†å²éªŒè¯æ¨¡å¼ã€‘")
            print("æ‰§è¡Œæµç¨‹: é¢„æµ‹åˆ†æ â†’ è·å–å®é™…ç»“æœ â†’ å¯¹æ¯”åˆ†æ")
            mode = 'historical_validation'
            mode_description = 'å†å²éªŒè¯æ¨¡å¼ï¼šå¯¹å·²çŸ¥æœŸå·è¿›è¡Œé¢„æµ‹å¹¶éªŒè¯å‡†ç¡®æ€§'

            # æ‰§è¡Œé¢„æµ‹
            prediction_result = self.predict(
                target_issue=target_issue,
                periods=periods,
                count=count,
                method=method,
                **kwargs
            )

            print(f"âœ… é¢„æµ‹å®Œæˆï¼Œç”Ÿæˆ {len(prediction_result.predicted_numbers)} ä¸ªå·ç ")
            print(f"é¢„æµ‹å·ç : {prediction_result.predicted_numbers}")

            # æ‰§è¡Œå¯¹æ¯”åˆ†æ
            try:
                comparison_result = self.compare_results(
                    target_issue=target_issue,
                    predicted_numbers=prediction_result.predicted_numbers
                )

                print(f"âœ… å¯¹æ¯”åˆ†æå®Œæˆ")
                print(f"å‘½ä¸­ç‡: {comparison_result.hit_rate:.1%}")
                print(f"å‘½ä¸­å·ç : {comparison_result.hit_numbers}")
                print(f"æœªå‘½ä¸­å·ç : {comparison_result.miss_numbers}")

                return {
                    'prediction_result': prediction_result,
                    'comparison_result': comparison_result,
                    'mode': mode,
                    'mode_description': mode_description,
                    'success': True
                }

            except Exception as e:
                print(f"âš ï¸ å¯¹æ¯”åˆ†æå¤±è´¥: {e}")
                return {
                    'prediction_result': prediction_result,
                    'comparison_result': None,
                    'mode': mode,
                    'mode_description': mode_description,
                    'success': False,
                    'error': str(e)
                }

        else:
            # æ¨¡å¼2: æœªæ¥é¢„æµ‹æ¨¡å¼
            print("ğŸ”® æ£€æµ‹åˆ°æœªæ¥æœŸå·ï¼Œå¯åŠ¨ã€æœªæ¥é¢„æµ‹æ¨¡å¼ã€‘")
            print("æ‰§è¡Œæµç¨‹: é¢„æµ‹åˆ†æ â†’ è¿”å›é¢„æµ‹ç»“æœ")
            mode = 'future_prediction'
            mode_description = 'æœªæ¥é¢„æµ‹æ¨¡å¼ï¼šå¯¹æœªçŸ¥æœŸå·è¿›è¡Œé¢„æµ‹åˆ†æ'

            # æ‰§è¡Œé¢„æµ‹
            prediction_result = self.predict(
                target_issue=target_issue,
                periods=periods,
                count=count,
                method=method,
                **kwargs
            )

            print(f"âœ… é¢„æµ‹å®Œæˆï¼Œç”Ÿæˆ {len(prediction_result.predicted_numbers)} ä¸ªå·ç ")
            print(f"é¢„æµ‹å·ç : {prediction_result.predicted_numbers}")
            print("ğŸ’¡ æç¤º: è¿™æ˜¯æœªæ¥æœŸå·é¢„æµ‹ï¼Œæ— æ³•è¿›è¡Œå‡†ç¡®æ€§éªŒè¯")

            return {
                'prediction_result': prediction_result,
                'comparison_result': None,
                'mode': mode,
                'mode_description': mode_description,
                'success': True
            }

    def _check_issue_exists(self, target_issue: str) -> bool:
        """æ£€æŸ¥ç›®æ ‡æœŸå·æ˜¯å¦å­˜åœ¨äºå†å²æ•°æ®ä¸­"""
        try:
            actual_result = self.data_manager.get_issue_result(target_issue)
            return actual_result is not None
        except Exception:
            return False
    
    def compare_results(self, 
                       target_issue: str,
                       predicted_numbers: List[int]) -> ComparisonResult:
        """å¯¹æ¯”é¢„æµ‹ç»“æœ"""
        
        # è·å–å¼€å¥–ç»“æœ
        actual_result = self.data_manager.get_issue_result(target_issue)
        if not actual_result:
            raise ValueError(f"æœªæ‰¾åˆ°æœŸå· {target_issue} çš„å¼€å¥–ç»“æœ")
        
        # æ‰§è¡Œå¯¹æ¯”
        comparison = self.comparison_engine.compare(
            target_issue=target_issue,
            predicted_numbers=predicted_numbers,
            actual_numbers=actual_result.numbers
        )
        
        # æ›´æ–°æ€§èƒ½è®°å½•
        method = getattr(self, '_last_prediction_method', 'unknown')
        self.performance_monitor.record_prediction(
            method, 0, comparison.hit_rate
        )
        
        return comparison
    
    def analyze_statistics(self, data: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†ææ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        if data.empty:
            return {}

        stats = {}

        # åŸºæœ¬ç»Ÿè®¡
        stats['total_periods'] = len(data)
        stats['date_range'] = {
            'start': data['issue'].iloc[0] if len(data) > 0 else None,
            'end': data['issue'].iloc[-1] if len(data) > 0 else None
        }

        # å·ç é¢‘ç‡ç»Ÿè®¡
        all_numbers = []
        for _, row in data.iterrows():
            numbers = [row[f'num{i}'] for i in range(1, 21)]
            all_numbers.extend(numbers)

        from collections import Counter
        number_freq = Counter(all_numbers)
        stats['number_frequency'] = dict(number_freq.most_common(10))

        # åŒºåŸŸåˆ†å¸ƒç»Ÿè®¡
        stats['zone_distribution'] = ZoneAnalyzer.analyze_zone_distribution(data)

        # å’Œå€¼åˆ†å¸ƒç»Ÿè®¡
        stats['sum_distribution'] = SumAnalyzer.analyze_sum_distribution(data)

        # å†·çƒ­å·ç»Ÿè®¡
        hot_numbers = [num for num, freq in number_freq.most_common(10)]
        cold_numbers = [num for num, freq in number_freq.most_common()[-10:]]
        stats['hot_numbers'] = hot_numbers
        stats['cold_numbers'] = cold_numbers

        return stats

    def analyze_and_predict(self,
                           target_issue: str,
                           periods: int = 300,
                           count: int = 30,
                           method: str = 'frequency',
                           **kwargs) -> Tuple[PredictionResult, ComparisonResult]:
        """åˆ†æé¢„æµ‹å¹¶å¯¹æ¯”ç»“æœ"""
        
        # è®°å½•é¢„æµ‹æ–¹æ³•
        self._last_prediction_method = method
        
        # æ‰§è¡Œé¢„æµ‹
        prediction_result = self.predict(
            target_issue=target_issue,
            periods=periods,
            count=count,
            method=method,
            **kwargs
        )
        
        # å¯¹æ¯”ç»“æœ
        comparison_result = self.compare_results(
            target_issue=target_issue,
            predicted_numbers=prediction_result.predicted_numbers
        )
        
        return prediction_result, comparison_result
    
    def get_available_methods(self) -> List[str]:
        """è·å–å¯ç”¨çš„é¢„æµ‹æ–¹æ³•"""
        return self.prediction_engine.get_available_methods()
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        return self.performance_monitor.get_performance_summary()


class Happy8CLI:
    """å¿«ä¹8å‘½ä»¤è¡Œç•Œé¢"""
    
    def __init__(self):
        self.analyzer = Happy8Analyzer()
        self.parser = self._create_parser()
    
    def _create_parser(self):
        """åˆ›å»ºå‘½ä»¤è¡Œè§£æå™¨"""
        parser = argparse.ArgumentParser(
            description="å¿«ä¹8æ™ºèƒ½é¢„æµ‹ç³»ç»Ÿ",
            formatter_class=argparse.RawDescriptionHelpFormatter,
            epilog="""
ç¤ºä¾‹ç”¨æ³•:
  %(prog)s crawl --count 1000
  %(prog)s predict --target 20250813001 --periods 300 --count 30 --method frequency
  %(prog)s compare --target 20250813001 --periods 300 --count 30 --method ensemble
            """
        )
        
        subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
        
        # æ•°æ®ç®¡ç†å‘½ä»¤
        crawl_parser = subparsers.add_parser('crawl', help='çˆ¬å–å†å²æ•°æ®')
        crawl_parser.add_argument('--count', type=int, default=1000, help='çˆ¬å–æœŸæ•°')
        
        update_parser = subparsers.add_parser('update', help='æ›´æ–°æœ€æ–°æ•°æ®')
        
        validate_parser = subparsers.add_parser('validate', help='éªŒè¯æ•°æ®å®Œæ•´æ€§')
        
        # é¢„æµ‹å‘½ä»¤
        predict_parser = subparsers.add_parser('predict', help='æ‰§è¡Œé¢„æµ‹')
        predict_parser.add_argument('--target', required=True, help='ç›®æ ‡æœŸå·')
        predict_parser.add_argument('--periods', type=int, default=300, help='åˆ†ææœŸæ•°')
        predict_parser.add_argument('--count', type=int, default=30, help='ç”Ÿæˆå·ç æ•°')
        predict_parser.add_argument('--method', default='frequency', 
                                   choices=['frequency', 'hot_cold', 'markov', 'lstm', 'ensemble'],
                                   help='é¢„æµ‹æ–¹æ³•')
        predict_parser.add_argument('--explain', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†è¿‡ç¨‹')
        
        # å¯¹æ¯”å‘½ä»¤
        compare_parser = subparsers.add_parser('compare', help='é¢„æµ‹å¹¶å¯¹æ¯”ç»“æœ')
        compare_parser.add_argument('--target', required=True, help='ç›®æ ‡æœŸå·')
        compare_parser.add_argument('--periods', type=int, default=300, help='åˆ†ææœŸæ•°')
        compare_parser.add_argument('--count', type=int, default=30, help='ç”Ÿæˆå·ç æ•°')
        compare_parser.add_argument('--method', default='frequency',
                                   choices=['frequency', 'hot_cold', 'markov', 'lstm', 'ensemble'],
                                   help='é¢„æµ‹æ–¹æ³•')
        compare_parser.add_argument('--output', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
        
        return parser
    
    def run(self, args=None):
        """è¿è¡ŒCLI"""
        args = self.parser.parse_args(args)
        
        if args.command == 'crawl':
            self._handle_crawl(args)
        elif args.command == 'update':
            self._handle_update(args)
        elif args.command == 'validate':
            self._handle_validate(args)
        elif args.command == 'predict':
            self._handle_predict(args)
        elif args.command == 'compare':
            self._handle_compare(args)
        else:
            self.parser.print_help()
    
    def _handle_crawl(self, args):
        """å¤„ç†çˆ¬å–å‘½ä»¤"""
        print(f"å¼€å§‹çˆ¬å– {args.count} æœŸå†å²æ•°æ®...")
        self.analyzer.data_manager.crawl_initial_data(args.count)
        print("æ•°æ®çˆ¬å–å®Œæˆ!")
    
    def _handle_update(self, args):
        """å¤„ç†æ›´æ–°å‘½ä»¤"""
        print("æ›´æ–°æœ€æ–°æ•°æ®...")
        # è¿™é‡Œå¯ä»¥å®ç°æ•°æ®æ›´æ–°é€»è¾‘
        print("æ•°æ®æ›´æ–°å®Œæˆ!")
    
    def _handle_validate(self, args):
        """å¤„ç†éªŒè¯å‘½ä»¤"""
        print("éªŒè¯æ•°æ®å®Œæ•´æ€§...")
        data = self.analyzer.load_data()
        validation_result = self.analyzer.data_manager.validator.validate_happy8_data(data)
        
        print(f"éªŒè¯ç»“æœ:")
        print(f"- æ€»è®°å½•æ•°: {validation_result['total_records']}")
        print(f"- é‡å¤æœŸå·: {validation_result['duplicate_issues']}")
        print(f"- æ— æ•ˆå·ç èŒƒå›´: {validation_result['invalid_ranges']}")
        print(f"- æ— æ•ˆå·ç æ•°é‡: {validation_result['invalid_number_counts']}")
        
        if validation_result['errors']:
            print(f"- é”™è¯¯: {validation_result['errors']}")
        else:
            print("- æ•°æ®éªŒè¯é€šè¿‡!")
    
    def _handle_predict(self, args):
        """å¤„ç†é¢„æµ‹å‘½ä»¤"""
        print("å¿«ä¹8æ™ºèƒ½é¢„æµ‹ç³»ç»Ÿ")
        print("=" * 50)
        print()
        
        print("é¢„æµ‹å‚æ•°:")
        print(f"- ç›®æ ‡æœŸå·: {args.target}")
        print(f"- åˆ†ææœŸæ•°: {args.periods}æœŸ")
        print(f"- ç”Ÿæˆæ•°é‡: {args.count}ä¸ªå·ç ")
        print(f"- é¢„æµ‹æ–¹æ³•: {args.method}")
        print()
        
        try:
            # æ‰§è¡Œé¢„æµ‹
            print("æ­£åœ¨æ‰§è¡Œé¢„æµ‹... ", end="", flush=True)
            result = self.analyzer.predict(
                target_issue=args.target,
                periods=args.periods,
                count=args.count,
                method=args.method
            )
            print("âœ“")
            
            # æ˜¾ç¤ºç»“æœ
            self._display_prediction_result(result)
            
        except Exception as e:
            print(f"âœ—\né”™è¯¯: {str(e)}")
    
    def _handle_compare(self, args):
        """å¤„ç†å¯¹æ¯”å‘½ä»¤"""
        print("å¿«ä¹8æ™ºèƒ½é¢„æµ‹ç³»ç»Ÿ")
        print("=" * 50)
        print()
        
        print("é¢„æµ‹å‚æ•°:")
        print(f"- ç›®æ ‡æœŸå·: {args.target}")
        print(f"- åˆ†ææœŸæ•°: {args.periods}æœŸ")
        print(f"- ç”Ÿæˆæ•°é‡: {args.count}ä¸ªå·ç ")
        print(f"- é¢„æµ‹æ–¹æ³•: {args.method}")
        print()
        
        try:
            # æ‰§è¡Œé¢„æµ‹å’Œå¯¹æ¯”
            print("æ­£åœ¨æ‰§è¡Œé¢„æµ‹å’Œå¯¹æ¯”... ", end="", flush=True)
            prediction_result, comparison_result = self.analyzer.analyze_and_predict(
                target_issue=args.target,
                periods=args.periods,
                count=args.count,
                method=args.method
            )
            print("âœ“")
            
            # æ˜¾ç¤ºç»“æœ
            self._display_comparison_results(prediction_result, comparison_result)
            
            # ä¿å­˜ç»“æœ
            if args.output:
                self._save_results(prediction_result, comparison_result, args.output)
                print(f"\nç»“æœå·²ä¿å­˜åˆ°: {args.output}")
            
        except Exception as e:
            print(f"âœ—\né”™è¯¯: {str(e)}")
    
    def _display_prediction_result(self, result: PredictionResult):
        """æ˜¾ç¤ºé¢„æµ‹ç»“æœ"""
        print("\né¢„æµ‹ç»“æœ:")
        print("=" * 50)
        
        # é¢„æµ‹å·ç 
        predicted_numbers = result.predicted_numbers
        print(f"é¢„æµ‹å·ç  ({len(predicted_numbers)}ä¸ª):")
        
        # æŒ‰è¡Œæ˜¾ç¤ºï¼Œæ¯è¡Œ10ä¸ª
        for i in range(0, len(predicted_numbers), 10):
            line_numbers = predicted_numbers[i:i+10]
            formatted_numbers = [f"{num:02d}" for num in line_numbers]
            print(" ".join(formatted_numbers))
        
        print(f"\né¢„æµ‹å®Œæˆ! ç”¨æ—¶: {result.execution_time:.2f}ç§’")
    
    def _display_comparison_results(self, prediction_result: PredictionResult, comparison_result: ComparisonResult):
        """æ˜¾ç¤ºå¯¹æ¯”ç»“æœ"""
        print("\né¢„æµ‹ç»“æœ:")
        print("=" * 50)
        
        # é¢„æµ‹å·ç 
        predicted_numbers = prediction_result.predicted_numbers
        print(f"é¢„æµ‹å·ç  ({len(predicted_numbers)}ä¸ª):")
        
        for i in range(0, len(predicted_numbers), 10):
            line_numbers = predicted_numbers[i:i+10]
            formatted_numbers = []
            
            for num in line_numbers:
                if num in comparison_result.hit_numbers:
                    formatted_numbers.append(f"\033[91m[{num:02d}]\033[0m")  # çº¢è‰²æ ‡è®°
                else:
                    formatted_numbers.append(f"{num:02d}")
            
            print(" ".join(formatted_numbers))
        
        print()
        
        # å¼€å¥–å·ç 
        actual_numbers = comparison_result.actual_numbers
        print(f"å¼€å¥–å·ç  ({len(actual_numbers)}ä¸ª):")
        
        for i in range(0, len(actual_numbers), 10):
            line_numbers = actual_numbers[i:i+10]
            formatted_numbers = [f"\033[92m[{num:02d}]\033[0m" for num in line_numbers]  # ç»¿è‰²
            print(" ".join(formatted_numbers))
        
        print()
        
        # å‘½ä¸­åˆ†æ
        print("å‘½ä¸­åˆ†æ:")
        print("=" * 50)
        hit_numbers_str = " ".join([f"\033[91m{num:02d}\033[0m" for num in sorted(comparison_result.hit_numbers)])
        print(f"å‘½ä¸­å·ç : {hit_numbers_str}")
        print(f"å‘½ä¸­æ•°é‡: {comparison_result.hit_count}/{len(predicted_numbers)}")
        print(f"å‘½ä¸­ç‡: {comparison_result.hit_rate:.2%}")
        
        # è¯¦ç»†åˆ†æ
        self._display_detailed_analysis(comparison_result)
        
        print(f"\né¢„æµ‹å®Œæˆ! ç”¨æ—¶: {prediction_result.execution_time:.2f}ç§’")
    
    def _display_detailed_analysis(self, comparison_result: ComparisonResult):
        """æ˜¾ç¤ºè¯¦ç»†åˆ†æ"""
        hit_numbers = comparison_result.hit_numbers
        distribution = comparison_result.hit_distribution
        
        print("\nè¯¦ç»†åˆ†æ:")
        print(f"- å°å·å‘½ä¸­: {distribution.get('small_numbers', 0)}ä¸ª (1-40å·æ®µ)")
        print(f"- å¤§å·å‘½ä¸­: {distribution.get('big_numbers', 0)}ä¸ª (41-80å·æ®µ)")
        print(f"- å¥‡æ•°å‘½ä¸­: {distribution.get('odd_numbers', 0)}ä¸ª")
        print(f"- å¶æ•°å‘½ä¸­: {distribution.get('even_numbers', 0)}ä¸ª")
        
        # åŒºåŸŸåˆ†å¸ƒ
        zone_hits = [distribution.get(f'zone_{i}', 0) for i in range(1, 9)]
        print(f"- å„åŒºåŸŸå‘½ä¸­åˆ†å¸ƒ: {zone_hits}")
    
    def _save_results(self, prediction_result: PredictionResult, comparison_result: ComparisonResult, output_path: str):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        results = {
            'prediction': asdict(prediction_result),
            'comparison': asdict(comparison_result)
        }
        
        # å¤„ç†datetimeå¯¹è±¡
        results['prediction']['generation_time'] = results['prediction']['generation_time'].isoformat()
        results['comparison']['comparison_time'] = results['comparison']['comparison_time'].isoformat()
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)


def main():
    """ä¸»å‡½æ•°"""
    cli = Happy8CLI()
    cli.run()


if __name__ == "__main__":
    main()
