#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«ä¹8æ™ºèƒ½é¢„æµ‹ç³»ç»Ÿ - æ ¸å¿ƒåˆ†æå™¨
Happy8 Prediction System - Core Analyzer

åŸºäºå…ˆè¿›çš„æœºå™¨å­¦ä¹ å’Œç»Ÿè®¡åˆ†ææŠ€æœ¯ï¼Œä¸“ä¸ºå¿«ä¹8å½©ç¥¨è®¾è®¡ï¼š
- å·ç èŒƒå›´: 1-80å·
- å¼€å¥–å·ç : æ¯æœŸå¼€å‡º20ä¸ªå·ç 
- å¼€å¥–é¢‘ç‡: æ¯å¤©ä¸€æœŸ
- 17ç§é¢„æµ‹ç®—æ³•: ç»Ÿè®¡å­¦+æœºå™¨å­¦ä¹ +æ·±åº¦å­¦ä¹ +è´å¶æ–¯æ¨ç†

ä½œè€…: linshibo
å¼€å‘è€…: linshibo
ç‰ˆæœ¬: v1.4.0
åˆ›å»ºæ—¶é—´: 2025-08-17
æœ€åæ›´æ–°: 2025-08-19
"""

import os
import sys
import time
import json
import pickle
import argparse
import warnings
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Any, Union
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import multiprocessing as mp

import numpy as np
import pandas as pd
import requests
from bs4 import BeautifulSoup
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
import scipy.stats as stats
from scipy.spatial.distance import pdist, squareform

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings('ignore')
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# å°è¯•å¯¼å…¥æ·±åº¦å­¦ä¹ åº“
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers, models, optimizers, callbacks
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding, MultiHeadAttention
    TF_AVAILABLE = True
    
    # GPUé…ç½®
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print(f"æ£€æµ‹åˆ° {len(gpus)} ä¸ªGPUè®¾å¤‡ï¼Œå·²å¯ç”¨GPUåŠ é€Ÿ")
        except RuntimeError as e:
            print(f"GPUé…ç½®å¤±è´¥: {e}")
    else:
        print("æœªæ£€æµ‹åˆ°GPUè®¾å¤‡ï¼Œä½¿ç”¨CPUè®¡ç®—")
        
except ImportError:
    TF_AVAILABLE = False
    print("TensorFlowæœªå®‰è£…ï¼Œæ·±åº¦å­¦ä¹ åŠŸèƒ½å°†ä¸å¯ç”¨")

# å°è¯•å¯¼å…¥é«˜çº§åº“
try:
    import optuna
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False

try:
    import ray
    RAY_AVAILABLE = True
except ImportError:
    RAY_AVAILABLE = False


@dataclass
class Happy8Result:
    """å¿«ä¹8å¼€å¥–ç»“æœæ•°æ®æ¨¡å‹"""
    issue: str                    # æœŸå· (å¦‚: "2025238")
    date: str                     # å¼€å¥–æ—¥æœŸ (å¦‚: "2025-08-13")
    time: str                     # å¼€å¥–æ—¶é—´ (å¦‚: "09:05:00")
    numbers: List[int]            # å¼€å¥–å·ç  (20ä¸ªæ•°å­—)
    
    def __post_init__(self):
        """æ•°æ®éªŒè¯"""
        if len(self.numbers) != 20:
            raise ValueError(f"å¼€å¥–å·ç å¿…é¡»æ˜¯20ä¸ªï¼Œå®é™…: {len(self.numbers)}")
        if not all(1 <= num <= 80 for num in self.numbers):
            raise ValueError("å¼€å¥–å·ç å¿…é¡»åœ¨1-80èŒƒå›´å†…")
        if len(set(self.numbers)) != 20:
            raise ValueError("å¼€å¥–å·ç ä¸èƒ½é‡å¤")
    
    @property
    def number_sum(self) -> int:
        """å·ç æ€»å’Œ"""
        return sum(self.numbers)
    
    @property
    def number_avg(self) -> float:
        """å·ç å¹³å‡å€¼"""
        return self.number_sum / 20
    
    @property
    def number_range(self) -> int:
        """å·ç è·¨åº¦"""
        return max(self.numbers) - min(self.numbers)
    
    @property
    def odd_count(self) -> int:
        """å¥‡æ•°ä¸ªæ•°"""
        return sum(1 for n in self.numbers if n % 2 == 1)
    
    @property
    def big_count(self) -> int:
        """å¤§å·ä¸ªæ•° (41-80)"""
        return sum(1 for n in self.numbers if n >= 41)
    
    @property
    def zone_distribution(self) -> List[int]:
        """åŒºåŸŸåˆ†å¸ƒ (1-80åˆ†ä¸º8ä¸ªåŒºåŸŸ)"""
        zones = [0] * 8
        for num in self.numbers:
            zone_idx = (num - 1) // 10
            zones[zone_idx] += 1
        return zones
    
    @property
    def consecutive_count(self) -> int:
        """è¿å·ä¸ªæ•°"""
        sorted_nums = sorted(self.numbers)
        consecutive = 0
        for i in range(1, len(sorted_nums)):
            if sorted_nums[i] == sorted_nums[i-1] + 1:
                consecutive += 1
        return consecutive


@dataclass
class PredictionResult:
    """é¢„æµ‹ç»“æœæ•°æ®æ¨¡å‹"""
    target_issue: str             # ç›®æ ‡æœŸå·
    analysis_periods: int         # åˆ†ææœŸæ•°
    method: str                   # é¢„æµ‹æ–¹æ³•
    predicted_numbers: List[int]  # é¢„æµ‹å·ç 
    confidence_scores: List[float] # ç½®ä¿¡åº¦åˆ†æ•°
    generation_time: datetime     # ç”Ÿæˆæ—¶é—´
    execution_time: float         # æ‰§è¡Œè€—æ—¶
    parameters: Dict[str, Any]    # ç®—æ³•å‚æ•°
    
    @property
    def top_numbers(self) -> List[int]:
        """æŒ‰ç½®ä¿¡åº¦æ’åºçš„å‰20ä¸ªå·ç """
        if len(self.confidence_scores) != len(self.predicted_numbers):
            return self.predicted_numbers[:20]
        
        paired = list(zip(self.predicted_numbers, self.confidence_scores))
        sorted_pairs = sorted(paired, key=lambda x: x[1], reverse=True)
        return [num for num, _ in sorted_pairs[:20]]


@dataclass
class ComparisonResult:
    """å¯¹æ¯”ç»“æœæ•°æ®æ¨¡å‹"""
    target_issue: str             # ç›®æ ‡æœŸå·
    predicted_numbers: List[int]  # é¢„æµ‹å·ç 
    actual_numbers: List[int]     # å®é™…å¼€å¥–å·ç 
    hit_numbers: List[int]        # å‘½ä¸­å·ç 
    miss_numbers: List[int]       # æœªå‘½ä¸­å·ç 
    hit_count: int               # å‘½ä¸­æ•°é‡
    total_predicted: int         # é¢„æµ‹æ€»æ•°
    hit_rate: float             # å‘½ä¸­ç‡
    hit_distribution: Dict[str, int]  # å‘½ä¸­åˆ†å¸ƒåˆ†æ
    comparison_time: datetime    # å¯¹æ¯”æ—¶é—´
    
    def generate_report(self) -> str:
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        return f"""
å¯¹æ¯”ç»“æœæŠ¥å‘Š
============
ç›®æ ‡æœŸå·: {self.target_issue}
é¢„æµ‹æ•°é‡: {self.total_predicted}
å‘½ä¸­æ•°é‡: {self.hit_count}
å‘½ä¸­ç‡: {self.hit_rate:.2%}

å‘½ä¸­å·ç : {sorted(self.hit_numbers)}
æœªå‘½ä¸­å·ç : {sorted(self.miss_numbers)}

è¯¦ç»†åˆ†æ:
- å°å·å‘½ä¸­: {sum(1 for n in self.hit_numbers if n <= 40)}ä¸ª (1-40å·æ®µ)
- å¤§å·å‘½ä¸­: {sum(1 for n in self.hit_numbers if n >= 41)}ä¸ª (41-80å·æ®µ)
- å¥‡æ•°å‘½ä¸­: {sum(1 for n in self.hit_numbers if n % 2 == 1)}ä¸ª
- å¶æ•°å‘½ä¸­: {sum(1 for n in self.hit_numbers if n % 2 == 0)}ä¸ª
        """


@dataclass
class PairFrequencyItem:
    """å•ä¸ªæ•°å­—å¯¹é¢‘ç‡é¡¹"""
    pair: Tuple[int, int]         # æ•°å­—å¯¹ (å¦‚: (5, 15))
    count: int                    # å‡ºç°æ¬¡æ•°
    percentage: float             # å‡ºç°ç™¾åˆ†æ¯”
    
    def __post_init__(self):
        """æ•°æ®éªŒè¯"""
        if not isinstance(self.pair, tuple) or len(self.pair) != 2:
            raise ValueError("æ•°å­—å¯¹å¿…é¡»æ˜¯åŒ…å«ä¸¤ä¸ªæ•´æ•°çš„å…ƒç»„")
        if not all(1 <= num <= 80 for num in self.pair):
            raise ValueError("æ•°å­—å¯¹ä¸­çš„æ•°å­—å¿…é¡»åœ¨1-80èŒƒå›´å†…")
        if self.pair[0] >= self.pair[1]:
            raise ValueError("æ•°å­—å¯¹ä¸­ç¬¬ä¸€ä¸ªæ•°å­—å¿…é¡»å°äºç¬¬äºŒä¸ªæ•°å­—")
        if self.count < 0:
            raise ValueError("å‡ºç°æ¬¡æ•°ä¸èƒ½ä¸ºè´Ÿæ•°")
        if not 0 <= self.percentage <= 100:
            raise ValueError("ç™¾åˆ†æ¯”å¿…é¡»åœ¨0-100èŒƒå›´å†…")
    
    def __str__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤º"""
        return f"({self.pair[0]:02d}, {self.pair[1]:02d}) - å‡ºç° {self.count} æ¬¡ - æ¦‚ç‡ {self.percentage:.1f}%"
    
    def __repr__(self) -> str:
        """è°ƒè¯•è¡¨ç¤º"""
        return f"PairFrequencyItem(pair={self.pair}, count={self.count}, percentage={self.percentage:.1f})"


@dataclass
class PairFrequencyResult:
    """æ•°å­—å¯¹é¢‘ç‡åˆ†æç»“æœ"""
    target_issue: str                    # ç›®æ ‡æœŸå·
    requested_periods: int               # è¯·æ±‚çš„ç»Ÿè®¡æœŸæ•°
    actual_periods: int                  # å®é™…ç»Ÿè®¡æœŸæ•°
    start_issue: str                     # èµ·å§‹æœŸå·
    end_issue: str                       # ç»“æŸæœŸå·
    total_pairs: int                     # åˆ†æçš„æ•°å­—å¯¹æ€»æ•°
    frequency_items: List[PairFrequencyItem]  # é¢‘ç‡é¡¹åˆ—è¡¨
    analysis_time: datetime              # åˆ†ææ—¶é—´
    execution_time: float                # æ‰§è¡Œè€—æ—¶(ç§’)
    
    def __post_init__(self):
        """æ•°æ®éªŒè¯"""
        if self.requested_periods <= 0:
            raise ValueError("è¯·æ±‚æœŸæ•°å¿…é¡»å¤§äº0")
        if self.actual_periods < 0:
            raise ValueError("å®é™…æœŸæ•°ä¸èƒ½ä¸ºè´Ÿæ•°")
        if self.actual_periods > self.requested_periods:
            raise ValueError("å®é™…æœŸæ•°ä¸èƒ½å¤§äºè¯·æ±‚æœŸæ•°")
        if self.total_pairs < 0:
            raise ValueError("æ•°å­—å¯¹æ€»æ•°ä¸èƒ½ä¸ºè´Ÿæ•°")
        if self.execution_time < 0:
            raise ValueError("æ‰§è¡Œæ—¶é—´ä¸èƒ½ä¸ºè´Ÿæ•°")
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'target_issue': self.target_issue,
            'requested_periods': self.requested_periods,
            'actual_periods': self.actual_periods,
            'start_issue': self.start_issue,
            'end_issue': self.end_issue,
            'total_pairs': self.total_pairs,
            'analysis_time': self.analysis_time.isoformat(),
            'execution_time': self.execution_time,
            'frequency_items': [
                {
                    'pair': item.pair,
                    'count': item.count,
                    'percentage': item.percentage
                }
                for item in self.frequency_items
            ]
        }
    
    def to_dataframe(self) -> pd.DataFrame:
        """è½¬æ¢ä¸ºDataFrameæ ¼å¼ï¼Œä¾¿äºå¯¼å‡º"""
        data = []
        for item in self.frequency_items:
            data.append({
                'æ•°å­—1': item.pair[0],
                'æ•°å­—2': item.pair[1],
                'æ•°å­—å¯¹': f"({item.pair[0]:02d}, {item.pair[1]:02d})",
                'å‡ºç°æ¬¡æ•°': item.count,
                'å‡ºç°é¢‘ç‡(%)': round(item.percentage, 2)
            })
        
        df = pd.DataFrame(data)
        
        # æ·»åŠ å…ƒæ•°æ®ä½œä¸ºDataFrameå±æ€§
        df.attrs = {
            'target_issue': self.target_issue,
            'requested_periods': self.requested_periods,
            'actual_periods': self.actual_periods,
            'start_issue': self.start_issue,
            'end_issue': self.end_issue,
            'total_pairs': self.total_pairs,
            'analysis_time': self.analysis_time.isoformat(),
            'execution_time': self.execution_time
        }
        
        return df
    
    def get_summary(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡æ‘˜è¦"""
        if not self.frequency_items:
            return {
                'total_unique_pairs': 0,
                'max_frequency': 0,
                'min_frequency': 0,
                'avg_frequency': 0,
                'top_pairs': []
            }
        
        frequencies = [item.count for item in self.frequency_items]
        
        return {
            'total_unique_pairs': len(self.frequency_items),
            'max_frequency': max(frequencies),
            'min_frequency': min(frequencies),
            'avg_frequency': sum(frequencies) / len(frequencies),
            'top_pairs': [
                {
                    'pair': item.pair,
                    'count': item.count,
                    'percentage': item.percentage
                }
                for item in self.frequency_items[:10]  # å‰10ä¸ªæœ€é«˜é¢‘ç‡çš„æ•°å­—å¯¹
            ]
        }
    
    def get_top_pairs(self, n: int = 10) -> List[PairFrequencyItem]:
        """è·å–å‰Nä¸ªæœ€é«˜é¢‘ç‡çš„æ•°å­—å¯¹"""
        return self.frequency_items[:min(n, len(self.frequency_items))]
    
    def find_pair(self, num1: int, num2: int) -> Optional['PairFrequencyItem']:
        """æŸ¥æ‰¾ç‰¹å®šæ•°å­—å¯¹çš„é¢‘ç‡ä¿¡æ¯"""
        # ç¡®ä¿æ•°å­—å¯¹çš„é¡ºåºæ­£ç¡®ï¼ˆå°æ•°åœ¨å‰ï¼‰
        pair = (min(num1, num2), max(num1, num2))
        
        for item in self.frequency_items:
            if item.pair == pair:
                return item
        return None
    
    def generate_report(self) -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        summary = self.get_summary()
        
        report = f"""
æ•°å­—å¯¹é¢‘ç‡åˆ†ææŠ¥å‘Š
==================
ç›®æ ‡æœŸå·: {self.target_issue}
ç»Ÿè®¡èŒƒå›´: {self.start_issue} - {self.end_issue} (å…±{self.actual_periods}æœŸ)
è¯·æ±‚æœŸæ•°: {self.requested_periods}æœŸ
å®é™…æœŸæ•°: {self.actual_periods}æœŸ
åˆ†ææ—¶é—´: {self.analysis_time.strftime('%Y-%m-%d %H:%M:%S')}
æ‰§è¡Œè€—æ—¶: {self.execution_time:.3f}ç§’

ç»Ÿè®¡æ‘˜è¦:
- ä¸åŒæ•°å­—å¯¹æ€»æ•°: {summary['total_unique_pairs']}
- æœ€é«˜å‡ºç°é¢‘ç‡: {summary['max_frequency']}æ¬¡
- æœ€ä½å‡ºç°é¢‘ç‡: {summary['min_frequency']}æ¬¡
- å¹³å‡å‡ºç°é¢‘ç‡: {summary['avg_frequency']:.2f}æ¬¡

å‰10ä¸ªé«˜é¢‘æ•°å­—å¯¹:
"""
        
        for i, item in enumerate(self.get_top_pairs(10), 1):
            report += f"{i:2d}. {item}\n"
        
        return report
    
    def to_excel(self, filename: Optional[str] = None) -> bytes:
        """
        å¯¼å‡ºä¸ºExcelæ ¼å¼
        
        Args:
            filename: æ–‡ä»¶åï¼Œå¦‚æœä¸ºNoneåˆ™è¿”å›å­—èŠ‚æ•°æ®
            
        Returns:
            Excelæ–‡ä»¶çš„å­—èŠ‚æ•°æ®
        """
        import io
        
        # åˆ›å»ºExcel writer
        output = io.BytesIO()
        
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            # ä¸»è¦æ•°æ®è¡¨
            df_main = self.to_dataframe()
            df_main.to_excel(writer, sheet_name='æ•°å­—å¯¹é¢‘ç‡', index=False)
            
            # ç»Ÿè®¡æ‘˜è¦è¡¨
            summary = self.get_summary()
            df_summary = pd.DataFrame([
                {'é¡¹ç›®': 'ç›®æ ‡æœŸå·', 'å€¼': self.target_issue},
                {'é¡¹ç›®': 'ç»Ÿè®¡èŒƒå›´', 'å€¼': f"{self.start_issue} - {self.end_issue}"},
                {'é¡¹ç›®': 'å®é™…æœŸæ•°', 'å€¼': self.actual_periods},
                {'é¡¹ç›®': 'æ•°å­—å¯¹æ€»æ•°', 'å€¼': self.total_pairs},
                {'é¡¹ç›®': 'æœ€é«˜é¢‘ç‡', 'å€¼': f"{summary['max_frequency']}æ¬¡"},
                {'é¡¹ç›®': 'æœ€ä½é¢‘ç‡', 'å€¼': f"{summary['min_frequency']}æ¬¡"},
                {'é¡¹ç›®': 'å¹³å‡é¢‘ç‡', 'å€¼': f"{summary['avg_frequency']:.2f}æ¬¡"},
                {'é¡¹ç›®': 'æ‰§è¡Œæ—¶é—´', 'å€¼': f"{self.execution_time:.3f}ç§’"},
            ])
            df_summary.to_excel(writer, sheet_name='ç»Ÿè®¡æ‘˜è¦', index=False)
            
            # å‰20åæ•°å­—å¯¹
            df_top20 = pd.DataFrame([
                {
                    'æ’å': i + 1,
                    'æ•°å­—å¯¹': f"({item.pair[0]:02d}, {item.pair[1]:02d})",
                    'å‡ºç°æ¬¡æ•°': item.count,
                    'å‡ºç°é¢‘ç‡(%)': round(item.percentage, 2)
                }
                for i, item in enumerate(self.get_top_pairs(20))
            ])
            df_top20.to_excel(writer, sheet_name='å‰20å', index=False)
        
        excel_data = output.getvalue()
        
        # å¦‚æœæŒ‡å®šäº†æ–‡ä»¶åï¼Œä¿å­˜åˆ°æ–‡ä»¶
        if filename:
            with open(filename, 'wb') as f:
                f.write(excel_data)
        
        return excel_data
    
    def to_html(self, include_charts: bool = False) -> str:
        """
        å¯¼å‡ºä¸ºHTMLæ ¼å¼
        
        Args:
            include_charts: æ˜¯å¦åŒ…å«å›¾è¡¨
            
        Returns:
            HTMLå­—ç¬¦ä¸²
        """
        summary = self.get_summary()
        
        html = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ•°å­—å¯¹é¢‘ç‡åˆ†ææŠ¥å‘Š</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 5px; }}
        .summary {{ margin: 20px 0; }}
        .summary-item {{ display: inline-block; margin: 10px; padding: 10px; background-color: #e8f4f8; border-radius: 3px; }}
        table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
        .top-pair {{ background-color: #fff3cd; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ”¢ æ•°å­—å¯¹é¢‘ç‡åˆ†ææŠ¥å‘Š</h1>
        <p><strong>ç›®æ ‡æœŸå·:</strong> {self.target_issue}</p>
        <p><strong>ç»Ÿè®¡èŒƒå›´:</strong> {self.start_issue} - {self.end_issue} (å…±{self.actual_periods}æœŸ)</p>
        <p><strong>åˆ†ææ—¶é—´:</strong> {self.analysis_time.strftime('%Y-%m-%d %H:%M:%S')}</p>
        <p><strong>æ‰§è¡Œè€—æ—¶:</strong> {self.execution_time:.3f}ç§’</p>
    </div>
    
    <div class="summary">
        <h2>ğŸ“ˆ ç»Ÿè®¡æ‘˜è¦</h2>
        <div class="summary-item">
            <strong>ä¸åŒæ•°å­—å¯¹æ€»æ•°:</strong> {summary['total_unique_pairs']}
        </div>
        <div class="summary-item">
            <strong>æœ€é«˜å‡ºç°é¢‘ç‡:</strong> {summary['max_frequency']}æ¬¡
        </div>
        <div class="summary-item">
            <strong>æœ€ä½å‡ºç°é¢‘ç‡:</strong> {summary['min_frequency']}æ¬¡
        </div>
        <div class="summary-item">
            <strong>å¹³å‡å‡ºç°é¢‘ç‡:</strong> {summary['avg_frequency']:.2f}æ¬¡
        </div>
    </div>
    
    <h2>ğŸ“‹ è¯¦ç»†ç»“æœ</h2>
    <table>
        <thead>
            <tr>
                <th>æ’å</th>
                <th>æ•°å­—å¯¹</th>
                <th>æ•°å­—1</th>
                <th>æ•°å­—2</th>
                <th>å‡ºç°æ¬¡æ•°</th>
                <th>å‡ºç°é¢‘ç‡(%)</th>
            </tr>
        </thead>
        <tbody>
"""
        
        # æ·»åŠ æ•°æ®è¡Œ
        for i, item in enumerate(self.frequency_items[:50]):  # åªæ˜¾ç¤ºå‰50ä¸ª
            row_class = "top-pair" if i < 10 else ""
            html += f"""
            <tr class="{row_class}">
                <td>{i + 1}</td>
                <td>({item.pair[0]:02d}, {item.pair[1]:02d})</td>
                <td>{item.pair[0]}</td>
                <td>{item.pair[1]}</td>
                <td>{item.count}</td>
                <td>{item.percentage:.1f}%</td>
            </tr>
"""
        
        html += """
        </tbody>
    </table>
    
    <div style="margin-top: 40px; padding: 20px; background-color: #f8f9fa; border-radius: 5px;">
        <p><small>æŠ¥å‘Šç”Ÿæˆæ—¶é—´: """ + datetime.now().strftime('%Y-%m-%d %H:%M:%S') + """</small></p>
        <p><small>å¿«ä¹8æ™ºèƒ½é¢„æµ‹ç³»ç»Ÿ - æ•°å­—å¯¹é¢‘ç‡åˆ†ææ¨¡å—</small></p>
    </div>
</body>
</html>
"""
        
        return html
    
    def to_xml(self) -> str:
        """
        å¯¼å‡ºä¸ºXMLæ ¼å¼
        
        Returns:
            XMLå­—ç¬¦ä¸²
        """
        from xml.etree.ElementTree import Element, SubElement, tostring
        from xml.dom import minidom
        
        # åˆ›å»ºæ ¹å…ƒç´ 
        root = Element('PairFrequencyAnalysis')
        
        # åŸºæœ¬ä¿¡æ¯
        info = SubElement(root, 'AnalysisInfo')
        SubElement(info, 'TargetIssue').text = self.target_issue
        SubElement(info, 'RequestedPeriods').text = str(self.requested_periods)
        SubElement(info, 'ActualPeriods').text = str(self.actual_periods)
        SubElement(info, 'StartIssue').text = self.start_issue
        SubElement(info, 'EndIssue').text = self.end_issue
        SubElement(info, 'TotalPairs').text = str(self.total_pairs)
        SubElement(info, 'AnalysisTime').text = self.analysis_time.isoformat()
        SubElement(info, 'ExecutionTime').text = str(self.execution_time)
        
        # ç»Ÿè®¡æ‘˜è¦
        summary = self.get_summary()
        summary_elem = SubElement(root, 'Summary')
        SubElement(summary_elem, 'TotalUniquePairs').text = str(summary['total_unique_pairs'])
        SubElement(summary_elem, 'MaxFrequency').text = str(summary['max_frequency'])
        SubElement(summary_elem, 'MinFrequency').text = str(summary['min_frequency'])
        SubElement(summary_elem, 'AvgFrequency').text = str(summary['avg_frequency'])
        
        # é¢‘ç‡é¡¹
        items_elem = SubElement(root, 'FrequencyItems')
        for item in self.frequency_items:
            item_elem = SubElement(items_elem, 'Item')
            SubElement(item_elem, 'Number1').text = str(item.pair[0])
            SubElement(item_elem, 'Number2').text = str(item.pair[1])
            SubElement(item_elem, 'Count').text = str(item.count)
            SubElement(item_elem, 'Percentage').text = str(item.percentage)
        
        # æ ¼å¼åŒ–XML
        rough_string = tostring(root, 'utf-8')
        reparsed = minidom.parseString(rough_string)
        return reparsed.toprettyxml(indent="  ")
    
    def export_to_file(self, filepath: str, format_type: str = 'auto') -> bool:
        """
        å¯¼å‡ºåˆ°æ–‡ä»¶
        
        Args:
            filepath: æ–‡ä»¶è·¯å¾„
            format_type: æ ¼å¼ç±»å‹ ('auto', 'csv', 'excel', 'json', 'html', 'xml', 'txt')
            
        Returns:
            æ˜¯å¦æˆåŠŸå¯¼å‡º
        """
        try:
            # è‡ªåŠ¨æ£€æµ‹æ ¼å¼
            if format_type == 'auto':
                ext = filepath.lower().split('.')[-1]
                format_map = {
                    'csv': 'csv',
                    'xlsx': 'excel',
                    'xls': 'excel',
                    'json': 'json',
                    'html': 'html',
                    'htm': 'html',
                    'xml': 'xml',
                    'txt': 'txt'
                }
                format_type = format_map.get(ext, 'csv')
            
            # æ ¹æ®æ ¼å¼å¯¼å‡º
            if format_type == 'csv':
                df = self.to_dataframe()
                df.to_csv(filepath, index=False, encoding='utf-8-sig')
            
            elif format_type == 'excel':
                excel_data = self.to_excel()
                with open(filepath, 'wb') as f:
                    f.write(excel_data)
            
            elif format_type == 'json':
                import json
                data = self.to_dict()
                with open(filepath, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
            
            elif format_type == 'html':
                html_content = self.to_html()
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(html_content)
            
            elif format_type == 'xml':
                xml_content = self.to_xml()
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(xml_content)
            
            elif format_type == 'txt':
                report = self.generate_report()
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(report)
            
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {format_type}")
            
            return True
            
        except Exception as e:
            print(f"å¯¼å‡ºå¤±è´¥: {str(e)}")
            return False


# æ•°å­—å¯¹åˆ†æå·¥å…·å‡½æ•°
def extract_number_pairs(numbers: List[int]) -> List[Tuple[int, int]]:
    """
    ä»20ä¸ªå¼€å¥–å·ç ä¸­æå–æ‰€æœ‰ä¸¤ä½æ•°ç»„åˆ
    
    Args:
        numbers: å¼€å¥–å·ç åˆ—è¡¨ï¼Œåº”åŒ…å«20ä¸ª1-80èŒƒå›´å†…çš„æ•°å­—
        
    Returns:
        æ‰€æœ‰å¯èƒ½çš„æ•°å­—å¯¹ç»„åˆåˆ—è¡¨ï¼Œæ¯ä¸ªæ•°å­—å¯¹æŒ‰(å°æ•°, å¤§æ•°)æ ¼å¼æ’åº
        
    Raises:
        ValueError: å½“è¾“å…¥æ•°æ®æ— æ•ˆæ—¶
        
    Example:
        >>> extract_number_pairs([1, 2, 3, 4, 5])
        [(1, 2), (1, 3), (1, 4), (1, 5), (2, 3), (2, 4), (2, 5), (3, 4), (3, 5), (4, 5)]
    """
    # è¾“å…¥éªŒè¯
    if not isinstance(numbers, (list, tuple)):
        raise ValueError("è¾“å…¥å¿…é¡»æ˜¯åˆ—è¡¨æˆ–å…ƒç»„")
    
    if len(numbers) != 20:
        raise ValueError(f"å¼€å¥–å·ç å¿…é¡»æ˜¯20ä¸ªï¼Œå®é™…: {len(numbers)}")
    
    if not all(isinstance(num, int) for num in numbers):
        raise ValueError("æ‰€æœ‰å·ç å¿…é¡»æ˜¯æ•´æ•°")
    
    if not all(1 <= num <= 80 for num in numbers):
        raise ValueError("æ‰€æœ‰å·ç å¿…é¡»åœ¨1-80èŒƒå›´å†…")
    
    if len(set(numbers)) != 20:
        raise ValueError("å¼€å¥–å·ç ä¸èƒ½é‡å¤")
    
    # æå–æ‰€æœ‰ä¸¤ä½æ•°ç»„åˆ
    pairs = []
    for i in range(len(numbers)):
        for j in range(i + 1, len(numbers)):
            # ç¡®ä¿è¾ƒå°çš„æ•°å­—åœ¨å‰
            num1, num2 = numbers[i], numbers[j]
            pair = (min(num1, num2), max(num1, num2))
            pairs.append(pair)
    
    # æŒ‰æ•°å­—å¯¹æ’åºï¼ˆå…ˆæŒ‰ç¬¬ä¸€ä¸ªæ•°å­—ï¼Œå†æŒ‰ç¬¬äºŒä¸ªæ•°å­—ï¼‰
    pairs.sort()
    
    return pairs


def validate_issue_format(issue: str) -> bool:
    """
    éªŒè¯æœŸå·æ ¼å¼æ˜¯å¦æ­£ç¡®
    
    Args:
        issue: æœŸå·å­—ç¬¦ä¸²ï¼Œæ ¼å¼åº”ä¸ºYYYYNNNï¼ˆå¦‚2025238ï¼‰
        
    Returns:
        bool: æ ¼å¼æ˜¯å¦æ­£ç¡®
        
    Example:
        >>> validate_issue_format("2025238")
        True
        >>> validate_issue_format("25091")
        False
    """
    if not isinstance(issue, str):
        return False
    
    if len(issue) != 7:
        return False
    
    if not issue.isdigit():
        return False
    
    year = int(issue[:4])
    period = int(issue[4:])
    
    # å¹´ä»½åº”è¯¥åœ¨åˆç†èŒƒå›´å†…
    if not (2020 <= year <= 2030):
        return False
    
    # æœŸæ•°åº”è¯¥åœ¨åˆç†èŒƒå›´å†…ï¼ˆæ¯å¤©æœ€å¤š300æœŸå·¦å³ï¼‰
    if not (1 <= period <= 999):
        return False
    
    return True


def calculate_issue_range(target_issue: str, period_count: int, available_data: Optional[pd.DataFrame] = None) -> Tuple[str, str, int]:
    """
    ä»ç›®æ ‡æœŸå·å‘å‰è®¡ç®—æŒ‡å®šæœŸæ•°çš„èŒƒå›´ï¼ŒåŸºäºå®é™…å¯ç”¨æ•°æ®
    
    Args:
        target_issue: ç›®æ ‡æœŸå·ï¼ˆå¦‚"2025238"ï¼‰
        period_count: è¦ç»Ÿè®¡çš„æœŸæ•°
        available_data: å¯ç”¨çš„å†å²æ•°æ®DataFrameï¼Œå¦‚æœæä¾›åˆ™åŸºäºå®é™…æ•°æ®è®¡ç®—
        
    Returns:
        Tuple[start_issue, end_issue, actual_count]: èµ·å§‹æœŸå·ã€ç»“æŸæœŸå·ã€å®é™…æœŸæ•°
        
    Raises:
        ValueError: å½“è¾“å…¥å‚æ•°æ— æ•ˆæ—¶
        
    Example:
        >>> calculate_issue_range("2025238", 20)
        ("2025219", "2025238", 20)
    """
    # è¾“å…¥éªŒè¯
    if not validate_issue_format(target_issue):
        raise ValueError(f"æ— æ•ˆçš„æœŸå·æ ¼å¼: {target_issue}")
    
    if not isinstance(period_count, int) or period_count <= 0:
        raise ValueError(f"æœŸæ•°å¿…é¡»æ˜¯æ­£æ•´æ•°: {period_count}")
    
    if period_count > 100:
        raise ValueError(f"æœŸæ•°ä¸èƒ½è¶…è¿‡100: {period_count}")
    
    # å¦‚æœæä¾›äº†å®é™…æ•°æ®ï¼ŒåŸºäºæ•°æ®è®¡ç®—
    if available_data is not None and not available_data.empty:
        return _calculate_range_from_data(target_issue, period_count, available_data)
    
    # å¦åˆ™ä½¿ç”¨ç®€å•çš„æ•°å­¦è®¡ç®—
    return _calculate_range_simple(target_issue, period_count)


def _calculate_range_from_data(target_issue: str, period_count: int, data: pd.DataFrame) -> Tuple[str, str, int]:
    """
    åŸºäºå®é™…æ•°æ®è®¡ç®—æœŸå·èŒƒå›´
    """
    # ç¡®ä¿æ•°æ®æŒ‰æœŸå·æ’åº
    data_sorted = data.sort_values('issue')
    issues = data_sorted['issue'].tolist()
    
    # æ£€æŸ¥ç›®æ ‡æœŸå·æ˜¯å¦å­˜åœ¨
    if target_issue not in issues:
        raise ValueError(f"ç›®æ ‡æœŸå· {target_issue} ä¸å­˜åœ¨äºå†å²æ•°æ®ä¸­")
    
    # æ‰¾åˆ°ç›®æ ‡æœŸå·çš„ä½ç½®
    target_index = issues.index(target_issue)
    
    # è®¡ç®—èµ·å§‹ä½ç½®
    start_index = max(0, target_index - period_count + 1)
    
    # è·å–å®é™…çš„æœŸå·èŒƒå›´
    start_issue = issues[start_index]
    end_issue = target_issue
    actual_count = target_index - start_index + 1
    
    return start_issue, end_issue, actual_count


def _calculate_range_simple(target_issue: str, period_count: int) -> Tuple[str, str, int]:
    """
    ç®€å•çš„æ•°å­¦è®¡ç®—æœŸå·èŒƒå›´ï¼ˆä¸ä¾èµ–å®é™…æ•°æ®ï¼‰
    """
    # è§£æç›®æ ‡æœŸå·
    year = int(target_issue[:4])
    target_period = int(target_issue[4:])
    
    # è®¡ç®—èµ·å§‹æœŸå·
    start_period = target_period - (period_count - 1)
    
    # å¤„ç†è·¨å¹´æƒ…å†µï¼ˆç®€åŒ–å¤„ç†ï¼Œå‡è®¾æ¯å¹´æœŸå·è¿ç»­ï¼‰
    if start_period <= 0:
        # å¦‚æœèµ·å§‹æœŸå·å°äºç­‰äº0ï¼Œåˆ™ä»ç¬¬1æœŸå¼€å§‹
        start_period = 1
        actual_count = target_period
    else:
        actual_count = period_count
    
    # æ ¼å¼åŒ–æœŸå·
    start_issue = f"{year}{start_period:03d}"
    end_issue = target_issue
    
    return start_issue, end_issue, actual_count


def get_available_issues_in_range(start_issue: str, end_issue: str, data: pd.DataFrame) -> List[str]:
    """
    è·å–æŒ‡å®šèŒƒå›´å†…å®é™…å¯ç”¨çš„æœŸå·åˆ—è¡¨
    
    Args:
        start_issue: èµ·å§‹æœŸå·
        end_issue: ç»“æŸæœŸå·
        data: å†å²æ•°æ®DataFrame
        
    Returns:
        åœ¨æŒ‡å®šèŒƒå›´å†…çš„æœŸå·åˆ—è¡¨ï¼ŒæŒ‰æ—¶é—´é¡ºåºæ’åº
    """
    if data.empty:
        return []
    
    # ç­›é€‰èŒƒå›´å†…çš„æ•°æ®
    mask = (data['issue'] >= start_issue) & (data['issue'] <= end_issue)
    filtered_data = data[mask]
    
    # æŒ‰æœŸå·æ’åºå¹¶è¿”å›æœŸå·åˆ—è¡¨
    return sorted(filtered_data['issue'].tolist())


def count_pair_frequencies(data: pd.DataFrame, start_issue: str, end_issue: str) -> Dict[Tuple[int, int], int]:
    """
    ç»Ÿè®¡æŒ‡å®šæœŸå·èŒƒå›´å†…æ•°å­—å¯¹çš„å‡ºç°é¢‘ç‡
    
    Args:
        data: å†å²å¼€å¥–æ•°æ®DataFrame
        start_issue: èµ·å§‹æœŸå·
        end_issue: ç»“æŸæœŸå·
        
    Returns:
        æ•°å­—å¯¹å‡ºç°é¢‘ç‡å­—å…¸ï¼Œé”®ä¸º(num1, num2)ï¼Œå€¼ä¸ºå‡ºç°æ¬¡æ•°
        
    Raises:
        ValueError: å½“è¾“å…¥æ•°æ®æ— æ•ˆæ—¶
        
    Example:
        >>> data = pd.DataFrame({...})
        >>> frequencies = count_pair_frequencies(data, "2025210", "2025220")
        >>> frequencies[(5, 15)]
        12
    """
    # è¾“å…¥éªŒè¯
    if data.empty:
        return {}
    
    required_cols = ['issue'] + [f'num{i}' for i in range(1, 21)]
    missing_cols = [col for col in required_cols if col not in data.columns]
    if missing_cols:
        raise ValueError(f"æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
    
    # ç­›é€‰æŒ‡å®šèŒƒå›´çš„æ•°æ®
    mask = (data['issue'] >= start_issue) & (data['issue'] <= end_issue)
    filtered_data = data[mask]
    
    if filtered_data.empty:
        return {}
    
    # ç»Ÿè®¡æ•°å­—å¯¹é¢‘ç‡
    pair_counts = {}
    
    for _, row in filtered_data.iterrows():
        # æå–å½“æœŸçš„20ä¸ªå¼€å¥–å·ç 
        numbers = [int(row[f'num{i}']) for i in range(1, 21)]
        
        # éªŒè¯å·ç æœ‰æ•ˆæ€§
        if len(set(numbers)) != 20:
            continue  # è·³è¿‡æœ‰é‡å¤å·ç çš„æ— æ•ˆæ•°æ®
        
        if not all(1 <= num <= 80 for num in numbers):
            continue  # è·³è¿‡å·ç èŒƒå›´æ— æ•ˆçš„æ•°æ®
        
        # æå–æ‰€æœ‰æ•°å­—å¯¹
        try:
            pairs = extract_number_pairs(numbers)
            
            # ç»Ÿè®¡æ¯ä¸ªæ•°å­—å¯¹çš„å‡ºç°æ¬¡æ•°
            for pair in pairs:
                pair_counts[pair] = pair_counts.get(pair, 0) + 1
                
        except ValueError:
            # è·³è¿‡æ— æ•ˆçš„å·ç ç»„åˆ
            continue
    
    return pair_counts


def sort_pair_frequencies(pair_counts: Dict[Tuple[int, int], int], total_periods: int) -> List[PairFrequencyItem]:
    """
    å¯¹æ•°å­—å¯¹é¢‘ç‡è¿›è¡Œæ’åºå¹¶è½¬æ¢ä¸ºPairFrequencyItemåˆ—è¡¨
    
    Args:
        pair_counts: æ•°å­—å¯¹å‡ºç°æ¬¡æ•°å­—å…¸
        total_periods: æ€»æœŸæ•°ï¼Œç”¨äºè®¡ç®—ç™¾åˆ†æ¯”
        
    Returns:
        æŒ‰å‡ºç°é¢‘ç‡ä»é«˜åˆ°ä½æ’åºçš„PairFrequencyItemåˆ—è¡¨
        
    Example:
        >>> pair_counts = {(5, 15): 12, (4, 18): 10}
        >>> items = sort_pair_frequencies(pair_counts, 20)
        >>> items[0].pair
        (5, 15)
    """
    if not pair_counts or total_periods <= 0:
        return []
    
    # è½¬æ¢ä¸ºPairFrequencyItemåˆ—è¡¨
    frequency_items = []
    for pair, count in pair_counts.items():
        percentage = (count / total_periods) * 100
        item = PairFrequencyItem(
            pair=pair,
            count=count,
            percentage=percentage
        )
        frequency_items.append(item)
    
    # æŒ‰å‡ºç°æ¬¡æ•°é™åºæ’åºï¼Œæ¬¡æ•°ç›¸åŒæ—¶æŒ‰æ•°å­—å¯¹å‡åºæ’åº
    frequency_items.sort(key=lambda x: (-x.count, x.pair))
    
    return frequency_items


def analyze_pair_frequency_core(data: pd.DataFrame, target_issue: str, period_count: int) -> PairFrequencyResult:
    """
    æ•°å­—å¯¹é¢‘ç‡åˆ†æçš„æ ¸å¿ƒç®—æ³•
    
    Args:
        data: å†å²å¼€å¥–æ•°æ®DataFrame
        target_issue: ç›®æ ‡æœŸå·
        period_count: ç»Ÿè®¡æœŸæ•°
        
    Returns:
        å®Œæ•´çš„åˆ†æç»“æœ
        
    Raises:
        ValueError: å½“è¾“å…¥å‚æ•°æ— æ•ˆæ—¶
    """
    start_time = datetime.now()
    
    try:
        # è®¡ç®—æœŸå·èŒƒå›´
        start_issue, end_issue, actual_periods = calculate_issue_range(
            target_issue, period_count, data
        )
        
        # ç»Ÿè®¡æ•°å­—å¯¹é¢‘ç‡
        pair_counts = count_pair_frequencies(data, start_issue, end_issue)
        
        # æ’åºå’Œæ ¼å¼åŒ–ç»“æœ
        frequency_items = sort_pair_frequencies(pair_counts, actual_periods)
        
        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        execution_time = (datetime.now() - start_time).total_seconds()
        
        # åˆ›å»ºç»“æœå¯¹è±¡
        result = PairFrequencyResult(
            target_issue=target_issue,
            requested_periods=period_count,
            actual_periods=actual_periods,
            start_issue=start_issue,
            end_issue=end_issue,
            total_pairs=len(frequency_items),
            frequency_items=frequency_items,
            analysis_time=start_time,
            execution_time=execution_time
        )
        
        return result
        
    except Exception as e:
        execution_time = (datetime.now() - start_time).total_seconds()
        raise ValueError(f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}ï¼Œæ‰§è¡Œæ—¶é—´: {execution_time:.3f}ç§’")


class DataValidator:
    """æ•°æ®éªŒè¯å™¨"""
    
    @staticmethod
    def validate_happy8_data(data: pd.DataFrame) -> Dict[str, Any]:
        """éªŒè¯å¿«ä¹8æ•°æ®"""
        results = {
            'total_records': len(data),
            'missing_values': {},
            'invalid_ranges': 0,
            'duplicate_issues': 0,
            'invalid_number_counts': 0,
            'errors': []
        }
        
        # æ£€æŸ¥å¿…è¦åˆ—ï¼ˆç§»é™¤timeåˆ—ï¼‰
        required_cols = ['issue', 'date'] + [f'num{i}' for i in range(1, 21)]
        missing_cols = [col for col in required_cols if col not in data.columns]
        if missing_cols:
            results['errors'].append(f"ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
            return results
        
        # æ£€æŸ¥ç¼ºå¤±å€¼
        for col in required_cols:
            missing_count = data[col].isnull().sum()
            if missing_count > 0:
                results['missing_values'][col] = missing_count
        
        # æ£€æŸ¥å·ç èŒƒå›´
        number_cols = [f'num{i}' for i in range(1, 21)]
        for col in number_cols:
            invalid_range = ((data[col] < 1) | (data[col] > 80)).sum()
            results['invalid_ranges'] += invalid_range
        
        # æ£€æŸ¥é‡å¤æœŸå·
        results['duplicate_issues'] = data['issue'].duplicated().sum()
        
        # æ£€æŸ¥æ¯æœŸå·ç æ•°é‡
        for idx, row in data.iterrows():
            numbers = [row[f'num{i}'] for i in range(1, 21)]
            if len(set(numbers)) != 20:
                results['invalid_number_counts'] += 1
        
        return results


class ResultCache:
    """
    ç»“æœç¼“å­˜ç®¡ç†å™¨ - æ”¯æŒLRUç­–ç•¥å’Œç¼“å­˜ç»Ÿè®¡
    """
    
    def __init__(self, max_size: int = 100):
        """
        åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        
        Args:
            max_size: æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
        """
        self.max_size = max_size
        self.cache = {}  # ç¼“å­˜æ•°æ®
        self.access_order = []  # è®¿é—®é¡ºåºï¼Œç”¨äºLRU
        self.hit_count = 0  # ç¼“å­˜å‘½ä¸­æ¬¡æ•°
        self.miss_count = 0  # ç¼“å­˜æœªå‘½ä¸­æ¬¡æ•°
        self.creation_time = datetime.now()
    
    def get(self, key: str) -> Optional[PairFrequencyResult]:
        """
        è·å–ç¼“å­˜ç»“æœ
        
        Args:
            key: ç¼“å­˜é”®
            
        Returns:
            ç¼“å­˜çš„ç»“æœï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        if key in self.cache:
            # æ›´æ–°è®¿é—®é¡ºåº
            self.access_order.remove(key)
            self.access_order.append(key)
            self.hit_count += 1
            return self.cache[key]
        else:
            self.miss_count += 1
            return None
    
    def set(self, key: str, result: PairFrequencyResult):
        """
        è®¾ç½®ç¼“å­˜ç»“æœ
        
        Args:
            key: ç¼“å­˜é”®
            result: åˆ†æç»“æœ
        """
        # å¦‚æœé”®å·²å­˜åœ¨ï¼Œæ›´æ–°å¹¶è°ƒæ•´é¡ºåº
        if key in self.cache:
            self.access_order.remove(key)
        # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œåˆ é™¤æœ€ä¹…æœªä½¿ç”¨çš„æ¡ç›®
        elif len(self.cache) >= self.max_size:
            oldest_key = self.access_order.pop(0)
            del self.cache[oldest_key]
        
        # æ·»åŠ æ–°æ¡ç›®
        self.cache[key] = result
        self.access_order.append(key)
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        self.access_order.clear()
        self.hit_count = 0
        self.miss_count = 0
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_requests = self.hit_count + self.miss_count
        hit_rate = (self.hit_count / total_requests * 100) if total_requests > 0 else 0
        
        return {
            'cache_size': len(self.cache),
            'max_size': self.max_size,
            'hit_count': self.hit_count,
            'miss_count': self.miss_count,
            'hit_rate': round(hit_rate, 2),
            'total_requests': total_requests,
            'cached_keys': list(self.cache.keys()),
            'creation_time': self.creation_time.isoformat(),
            'uptime_seconds': (datetime.now() - self.creation_time).total_seconds()
        }
    
    def remove(self, key: str) -> bool:
        """
        åˆ é™¤æŒ‡å®šçš„ç¼“å­˜æ¡ç›®
        
        Args:
            key: ç¼“å­˜é”®
            
        Returns:
            æ˜¯å¦æˆåŠŸåˆ é™¤
        """
        if key in self.cache:
            del self.cache[key]
            self.access_order.remove(key)
            return True
        return False
    
    def resize(self, new_max_size: int):
        """
        è°ƒæ•´ç¼“å­˜å¤§å°
        
        Args:
            new_max_size: æ–°çš„æœ€å¤§ç¼“å­˜å¤§å°
        """
        self.max_size = new_max_size
        
        # å¦‚æœæ–°å¤§å°å°äºå½“å‰ç¼“å­˜æ•°é‡ï¼Œåˆ é™¤æœ€ä¹…æœªä½¿ç”¨çš„æ¡ç›®
        while len(self.cache) > self.max_size:
            oldest_key = self.access_order.pop(0)
            del self.cache[oldest_key]


class PairAnalysisPerformanceMonitor:
    """
    æ€§èƒ½ç›‘æ§å™¨ - ä¸“é—¨ç”¨äºæ•°å­—å¯¹é¢‘ç‡åˆ†æ
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æ€§èƒ½ç›‘æ§å™¨"""
        self.metrics = {
            'total_analyses': 0,
            'total_execution_time': 0.0,
            'cache_hits': 0,
            'cache_misses': 0,
            'avg_execution_time': 0.0,
            'max_execution_time': 0.0,
            'min_execution_time': float('inf'),
            'memory_usage_mb': 0.0,
            'start_time': datetime.now()
        }
        self.analysis_history = []  # ä¿å­˜æœ€è¿‘100æ¬¡åˆ†æçš„è¯¦ç»†ä¿¡æ¯
        self.max_history = 100
    
    def record_analysis(self, execution_time: float, cache_hit: bool, data_size: int, result_size: int):
        """
        è®°å½•ä¸€æ¬¡åˆ†æçš„æ€§èƒ½æ•°æ®
        
        Args:
            execution_time: æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰
            cache_hit: æ˜¯å¦å‘½ä¸­ç¼“å­˜
            data_size: å¤„ç†çš„æ•°æ®å¤§å°
            result_size: ç»“æœæ•°æ®å¤§å°
        """
        # æ›´æ–°åŸºæœ¬æŒ‡æ ‡
        self.metrics['total_analyses'] += 1
        self.metrics['total_execution_time'] += execution_time
        
        if cache_hit:
            self.metrics['cache_hits'] += 1
        else:
            self.metrics['cache_misses'] += 1
        
        # æ›´æ–°æ‰§è¡Œæ—¶é—´ç»Ÿè®¡
        self.metrics['avg_execution_time'] = (
            self.metrics['total_execution_time'] / self.metrics['total_analyses']
        )
        self.metrics['max_execution_time'] = max(
            self.metrics['max_execution_time'], execution_time
        )
        self.metrics['min_execution_time'] = min(
            self.metrics['min_execution_time'], execution_time
        )
        
        # è®°å½•è¯¦ç»†å†å²
        analysis_record = {
            'timestamp': datetime.now(),
            'execution_time': execution_time,
            'cache_hit': cache_hit,
            'data_size': data_size,
            'result_size': result_size
        }
        
        self.analysis_history.append(analysis_record)
        
        # ä¿æŒå†å²è®°å½•åœ¨é™åˆ¶èŒƒå›´å†…
        if len(self.analysis_history) > self.max_history:
            self.analysis_history.pop(0)
        
        # æ›´æ–°å†…å­˜ä½¿ç”¨æƒ…å†µ
        self._update_memory_usage()
    
    def _update_memory_usage(self):
        """æ›´æ–°å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        try:
            import psutil
            process = psutil.Process()
            self.metrics['memory_usage_mb'] = process.memory_info().rss / 1024 / 1024
        except ImportError:
            # å¦‚æœæ²¡æœ‰psutilï¼Œä½¿ç”¨ç®€å•çš„ä¼°ç®—
            import sys
            self.metrics['memory_usage_mb'] = sys.getsizeof(self.analysis_history) / 1024 / 1024
    
    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        cache_hit_rate = 0.0
        if self.metrics['total_analyses'] > 0:
            cache_hit_rate = (self.metrics['cache_hits'] / self.metrics['total_analyses']) * 100
        
        uptime = (datetime.now() - self.metrics['start_time']).total_seconds()
        
        return {
            'total_analyses': self.metrics['total_analyses'],
            'total_execution_time': round(self.metrics['total_execution_time'], 3),
            'avg_execution_time': round(self.metrics['avg_execution_time'], 3),
            'max_execution_time': round(self.metrics['max_execution_time'], 3),
            'min_execution_time': round(self.metrics['min_execution_time'], 3) if self.metrics['min_execution_time'] != float('inf') else 0.0,
            'cache_hit_rate': round(cache_hit_rate, 2),
            'cache_hits': self.metrics['cache_hits'],
            'cache_misses': self.metrics['cache_misses'],
            'memory_usage_mb': round(self.metrics['memory_usage_mb'], 2),
            'uptime_seconds': round(uptime, 1),
            'analyses_per_minute': round((self.metrics['total_analyses'] / uptime * 60), 2) if uptime > 0 else 0.0
        }
    
    def get_recent_performance_trend(self, last_n: int = 20) -> List[Dict[str, Any]]:
        """è·å–æœ€è¿‘Næ¬¡åˆ†æçš„æ€§èƒ½è¶‹åŠ¿"""
        recent_history = self.analysis_history[-last_n:] if len(self.analysis_history) >= last_n else self.analysis_history
        
        return [
            {
                'timestamp': record['timestamp'].isoformat(),
                'execution_time': record['execution_time'],
                'cache_hit': record['cache_hit'],
                'data_size': record['data_size'],
                'result_size': record['result_size']
            }
            for record in recent_history
        ]
    
    def reset_metrics(self):
        """é‡ç½®æ€§èƒ½æŒ‡æ ‡"""
        self.__init__()


class PairFrequencyAnalyzer:
    """
    æ•°å­—å¯¹é¢‘ç‡åˆ†æå™¨
    
    æä¾›å®Œæ•´çš„æ•°å­—å¯¹é¢‘ç‡åˆ†æåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
    - æ•°å­—å¯¹æå–å’Œç»Ÿè®¡
    - æœŸå·èŒƒå›´è®¡ç®—
    - é¢‘ç‡åˆ†æå’Œæ’åº
    - ç»“æœç¼“å­˜å’Œæ€§èƒ½ä¼˜åŒ–
    - æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–
    """
    
    def __init__(self, data_manager=None, cache_size: int = 100, enable_parallel: bool = True):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            data_manager: æ•°æ®ç®¡ç†å™¨å®ä¾‹ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºæ–°å®ä¾‹
            cache_size: ç¼“å­˜å¤§å°
            enable_parallel: æ˜¯å¦å¯ç”¨å¹¶è¡Œå¤„ç†
        """
        self.data_manager = data_manager
        self.cache = ResultCache(cache_size)  # ä½¿ç”¨é«˜çº§ç¼“å­˜ç®¡ç†å™¨
        self.performance_monitor = PairAnalysisPerformanceMonitor()  # æ€§èƒ½ç›‘æ§å™¨
        self.enable_parallel = enable_parallel
        self.max_workers = min(4, os.cpu_count() or 1)  # æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
        self.logger = self._setup_logger()
    
    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        import logging
        logger = logging.getLogger(f"{__name__}.PairFrequencyAnalyzer")
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def analyze_pair_frequency(
        self, 
        target_issue: str, 
        period_count: int,
        use_cache: bool = True
    ) -> PairFrequencyResult:
        """
        åˆ†ææ•°å­—å¯¹é¢‘ç‡çš„ä¸»è¦æ–¹æ³•
        
        Args:
            target_issue: ç›®æ ‡æœŸå·ï¼ˆå¦‚"2025238"ï¼‰
            period_count: ç»Ÿè®¡æœŸæ•°
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            å®Œæ•´çš„åˆ†æç»“æœ
            
        Raises:
            ValueError: å½“è¾“å…¥å‚æ•°æ— æ•ˆæ—¶
            
        Example:
            >>> analyzer = PairFrequencyAnalyzer()
            >>> result = analyzer.analyze_pair_frequency("2025238", 20)
            >>> print(f"åˆ†æäº†{result.actual_periods}æœŸæ•°æ®")
        """
        # è¾“å…¥éªŒè¯
        self._validate_inputs(target_issue, period_count)
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._get_cache_key(target_issue, period_count)
        cached_result = None
        if use_cache:
            cached_result = self.cache.get(cache_key)
            if cached_result is not None:
                self.logger.info(f"ä½¿ç”¨ç¼“å­˜ç»“æœ: {cache_key}")
                return cached_result
        
        # è®°å½•åˆ†æå¼€å§‹
        self.logger.info(f"å¼€å§‹åˆ†ææ•°å­—å¯¹é¢‘ç‡: æœŸå·={target_issue}, æœŸæ•°={period_count}")
        
        try:
            # è·å–å†å²æ•°æ®
            data = self._get_historical_data()
            data_size = len(data) if not data.empty else 0
            
            # æ‰§è¡Œæ ¸å¿ƒåˆ†æï¼ˆå¯èƒ½ä½¿ç”¨å¹¶è¡Œå¤„ç†ï¼‰
            if self.enable_parallel and data_size > 50:
                result = self._analyze_with_parallel_processing(data, target_issue, period_count)
            else:
                result = analyze_pair_frequency_core(data, target_issue, period_count)
            
            # ç¼“å­˜ç»“æœ
            if use_cache:
                self.cache.set(cache_key, result)
            
            # è®°å½•æ€§èƒ½æ•°æ®
            cache_hit = cached_result is not None
            result_size = len(result.frequency_items)
            self.performance_monitor.record_analysis(
                execution_time=result.execution_time,
                cache_hit=cache_hit,
                data_size=data_size,
                result_size=result_size
            )
            
            # è®°å½•åˆ†æå®Œæˆ
            self.logger.info(
                f"åˆ†æå®Œæˆ: å®é™…æœŸæ•°={result.actual_periods}, "
                f"æ•°å­—å¯¹æ€»æ•°={result.total_pairs}, "
                f"æ‰§è¡Œæ—¶é—´={result.execution_time:.3f}ç§’"
            )
            
            return result
            
        except Exception as e:
            self.logger.error(f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            raise
    
    def _validate_inputs(self, target_issue: str, period_count: int):
        """éªŒè¯è¾“å…¥å‚æ•°"""
        if not validate_issue_format(target_issue):
            raise ValueError(f"æ— æ•ˆçš„æœŸå·æ ¼å¼: {target_issue}")
        
        if not isinstance(period_count, int) or period_count <= 0:
            raise ValueError(f"æœŸæ•°å¿…é¡»æ˜¯æ­£æ•´æ•°: {period_count}")
        
        if period_count > 100:
            raise ValueError(f"æœŸæ•°ä¸èƒ½è¶…è¿‡100: {period_count}")
    
    def _get_historical_data(self) -> pd.DataFrame:
        """è·å–å†å²æ•°æ®"""
        if self.data_manager is not None:
            # ä½¿ç”¨æ•°æ®ç®¡ç†å™¨è·å–æ•°æ®
            return self.data_manager.load_historical_data()
        else:
            # ç›´æ¥ä»æ–‡ä»¶è¯»å–æ•°æ®
            try:
                data_path = "data/happy8_results.csv"
                if os.path.exists(data_path):
                    return pd.read_csv(data_path)
                else:
                    raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            except Exception as e:
                raise ValueError(f"æ— æ³•è¯»å–å†å²æ•°æ®: {str(e)}")
    
    def _get_cache_key(self, target_issue: str, period_count: int) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return f"{target_issue}_{period_count}"
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        self.logger.info("ç¼“å­˜å·²æ¸…ç©º")
    
    def get_cache_info(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        return self.cache.get_stats()
    
    def remove_cache_item(self, target_issue: str, period_count: int) -> bool:
        """
        åˆ é™¤æŒ‡å®šçš„ç¼“å­˜é¡¹
        
        Args:
            target_issue: ç›®æ ‡æœŸå·
            period_count: æœŸæ•°
            
        Returns:
            æ˜¯å¦æˆåŠŸåˆ é™¤
        """
        cache_key = self._get_cache_key(target_issue, period_count)
        success = self.cache.remove(cache_key)
        if success:
            self.logger.info(f"åˆ é™¤ç¼“å­˜é¡¹: {cache_key}")
        return success
    
    def resize_cache(self, new_size: int):
        """
        è°ƒæ•´ç¼“å­˜å¤§å°
        
        Args:
            new_size: æ–°çš„ç¼“å­˜å¤§å°
        """
        old_size = self.cache.max_size
        self.cache.resize(new_size)
        self.logger.info(f"ç¼“å­˜å¤§å°å·²è°ƒæ•´: {old_size} -> {new_size}")
    
    def get_cache_hit_rate(self) -> float:
        """è·å–ç¼“å­˜å‘½ä¸­ç‡"""
        stats = self.cache.get_stats()
        return stats['hit_rate']
    
    def _analyze_with_parallel_processing(self, data: pd.DataFrame, target_issue: str, period_count: int) -> PairFrequencyResult:
        """
        ä½¿ç”¨å¹¶è¡Œå¤„ç†è¿›è¡Œåˆ†æï¼ˆé€‚ç”¨äºå¤§æ•°æ®é›†ï¼‰
        
        Args:
            data: å†å²æ•°æ®
            target_issue: ç›®æ ‡æœŸå·
            period_count: ç»Ÿè®¡æœŸæ•°
            
        Returns:
            åˆ†æç»“æœ
        """
        from concurrent.futures import ThreadPoolExecutor
        import numpy as np
        
        start_time = datetime.now()
        
        try:
            # è®¡ç®—æœŸå·èŒƒå›´
            start_issue, end_issue, actual_periods = calculate_issue_range(
                target_issue, period_count, data
            )
            
            # ç­›é€‰æ•°æ®
            mask = (data['issue'] >= start_issue) & (data['issue'] <= end_issue)
            filtered_data = data[mask]
            
            if filtered_data.empty:
                # è¿”å›ç©ºç»“æœ
                return PairFrequencyResult(
                    target_issue=target_issue,
                    requested_periods=period_count,
                    actual_periods=0,
                    start_issue=start_issue,
                    end_issue=end_issue,
                    total_pairs=0,
                    frequency_items=[],
                    analysis_time=start_time,
                    execution_time=0.0
                )
            
            # å°†æ•°æ®åˆ†å—è¿›è¡Œå¹¶è¡Œå¤„ç†
            chunk_size = max(1, len(filtered_data) // self.max_workers)
            data_chunks = [
                filtered_data.iloc[i:i + chunk_size] 
                for i in range(0, len(filtered_data), chunk_size)
            ]
            
            # å¹¶è¡Œå¤„ç†æ¯ä¸ªæ•°æ®å—
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = [
                    executor.submit(self._process_data_chunk, chunk, start_issue, end_issue)
                    for chunk in data_chunks
                ]
                
                # æ”¶é›†ç»“æœ
                chunk_results = [future.result() for future in futures]
            
            # åˆå¹¶ç»“æœ
            combined_pair_counts = {}
            for chunk_result in chunk_results:
                for pair, count in chunk_result.items():
                    combined_pair_counts[pair] = combined_pair_counts.get(pair, 0) + count
            
            # æ’åºå’Œæ ¼å¼åŒ–ç»“æœ
            frequency_items = sort_pair_frequencies(combined_pair_counts, actual_periods)
            
            # è®¡ç®—æ‰§è¡Œæ—¶é—´
            execution_time = (datetime.now() - start_time).total_seconds()
            
            # åˆ›å»ºç»“æœå¯¹è±¡
            result = PairFrequencyResult(
                target_issue=target_issue,
                requested_periods=period_count,
                actual_periods=actual_periods,
                start_issue=start_issue,
                end_issue=end_issue,
                total_pairs=len(frequency_items),
                frequency_items=frequency_items,
                analysis_time=start_time,
                execution_time=execution_time
            )
            
            return result
            
        except Exception as e:
            execution_time = (datetime.now() - start_time).total_seconds()
            raise ValueError(f"å¹¶è¡Œåˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}ï¼Œæ‰§è¡Œæ—¶é—´: {execution_time:.3f}ç§’")
    
    def _process_data_chunk(self, chunk: pd.DataFrame, start_issue: str, end_issue: str) -> Dict[Tuple[int, int], int]:
        """
        å¤„ç†å•ä¸ªæ•°æ®å—
        
        Args:
            chunk: æ•°æ®å—
            start_issue: èµ·å§‹æœŸå·
            end_issue: ç»“æŸæœŸå·
            
        Returns:
            æ•°å­—å¯¹é¢‘ç‡å­—å…¸
        """
        return count_pair_frequencies(chunk, start_issue, end_issue)
    
    def optimize_performance(self) -> Dict[str, Any]:
        """
        æ€§èƒ½ä¼˜åŒ–å»ºè®®
        
        Returns:
            ä¼˜åŒ–å»ºè®®å’Œå½“å‰æ€§èƒ½çŠ¶æ€
        """
        performance_report = self.performance_monitor.get_performance_report()
        cache_stats = self.cache.get_stats()
        
        suggestions = []
        
        # ç¼“å­˜å‘½ä¸­ç‡å»ºè®®
        if cache_stats['hit_rate'] < 50:
            suggestions.append("ç¼“å­˜å‘½ä¸­ç‡è¾ƒä½ï¼Œè€ƒè™‘å¢åŠ ç¼“å­˜å¤§å°æˆ–ä¼˜åŒ–æŸ¥è¯¢æ¨¡å¼")
        
        # æ‰§è¡Œæ—¶é—´å»ºè®®
        if performance_report['avg_execution_time'] > 5.0:
            suggestions.append("å¹³å‡æ‰§è¡Œæ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®å¯ç”¨å¹¶è¡Œå¤„ç†æˆ–ä¼˜åŒ–æ•°æ®ç»“æ„")
        
        # å†…å­˜ä½¿ç”¨å»ºè®®
        if performance_report['memory_usage_mb'] > 500:
            suggestions.append("å†…å­˜ä½¿ç”¨è¾ƒé«˜ï¼Œè€ƒè™‘å‡å°‘ç¼“å­˜å¤§å°æˆ–æ¸…ç†å†å²æ•°æ®")
        
        # å¹¶è¡Œå¤„ç†å»ºè®®
        if not self.enable_parallel and performance_report['avg_execution_time'] > 2.0:
            suggestions.append("å»ºè®®å¯ç”¨å¹¶è¡Œå¤„ç†ä»¥æé«˜å¤§æ•°æ®é›†çš„å¤„ç†é€Ÿåº¦")
        
        return {
            'performance_report': performance_report,
            'cache_stats': cache_stats,
            'suggestions': suggestions,
            'parallel_enabled': self.enable_parallel,
            'max_workers': self.max_workers
        }
    
    def set_parallel_processing(self, enabled: bool, max_workers: Optional[int] = None):
        """
        è®¾ç½®å¹¶è¡Œå¤„ç†å‚æ•°
        
        Args:
            enabled: æ˜¯å¦å¯ç”¨å¹¶è¡Œå¤„ç†
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
        """
        self.enable_parallel = enabled
        if max_workers is not None:
            self.max_workers = min(max_workers, os.cpu_count() or 1)
        
        self.logger.info(f"å¹¶è¡Œå¤„ç†è®¾ç½®: enabled={enabled}, max_workers={self.max_workers}")
    
    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–è¯¦ç»†çš„æ€§èƒ½æŠ¥å‘Š"""
        return self.performance_monitor.get_performance_report()
    
    def get_performance_trend(self, last_n: int = 20) -> List[Dict[str, Any]]:
        """è·å–æ€§èƒ½è¶‹åŠ¿æ•°æ®"""
        return self.performance_monitor.get_recent_performance_trend(last_n)
    
    def reset_performance_metrics(self):
        """é‡ç½®æ€§èƒ½æŒ‡æ ‡"""
        self.performance_monitor.reset_metrics()
        self.logger.info("æ€§èƒ½æŒ‡æ ‡å·²é‡ç½®")
    
    def benchmark_performance(self, test_cases: List[Tuple[str, int]]) -> Dict[str, Any]:
        """
        æ€§èƒ½åŸºå‡†æµ‹è¯•
        
        Args:
            test_cases: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨ [(target_issue, period_count), ...]
            
        Returns:
            åŸºå‡†æµ‹è¯•ç»“æœ
        """
        self.logger.info(f"å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œå…±{len(test_cases)}ä¸ªæµ‹è¯•ç”¨ä¾‹")
        
        benchmark_start = datetime.now()
        results = []
        
        for i, (target_issue, period_count) in enumerate(test_cases):
            try:
                case_start = datetime.now()
                result = self.analyze_pair_frequency(target_issue, period_count, use_cache=False)
                case_time = (datetime.now() - case_start).total_seconds()
                
                results.append({
                    'case_index': i + 1,
                    'target_issue': target_issue,
                    'period_count': period_count,
                    'execution_time': case_time,
                    'actual_periods': result.actual_periods,
                    'result_size': len(result.frequency_items),
                    'success': True
                })
                
                self.logger.info(f"æµ‹è¯•ç”¨ä¾‹ {i+1}/{len(test_cases)} å®Œæˆ: {case_time:.3f}ç§’")
                
            except Exception as e:
                results.append({
                    'case_index': i + 1,
                    'target_issue': target_issue,
                    'period_count': period_count,
                    'execution_time': 0.0,
                    'error': str(e),
                    'success': False
                })
                
                self.logger.error(f"æµ‹è¯•ç”¨ä¾‹ {i+1}/{len(test_cases)} å¤±è´¥: {str(e)}")
        
        total_time = (datetime.now() - benchmark_start).total_seconds()
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        successful_results = [r for r in results if r['success']]
        if successful_results:
            execution_times = [r['execution_time'] for r in successful_results]
            avg_time = sum(execution_times) / len(execution_times)
            max_time = max(execution_times)
            min_time = min(execution_times)
        else:
            avg_time = max_time = min_time = 0.0
        
        benchmark_report = {
            'total_cases': len(test_cases),
            'successful_cases': len(successful_results),
            'failed_cases': len(test_cases) - len(successful_results),
            'total_benchmark_time': total_time,
            'avg_execution_time': avg_time,
            'max_execution_time': max_time,
            'min_execution_time': min_time,
            'cases_per_second': len(test_cases) / total_time if total_time > 0 else 0,
            'detailed_results': results
        }
        
        self.logger.info(f"åŸºå‡†æµ‹è¯•å®Œæˆ: {len(successful_results)}/{len(test_cases)} æˆåŠŸ")
        
        return benchmark_report
    
    def batch_analyze(
        self, 
        requests: List[Tuple[str, int]], 
        use_cache: bool = True
    ) -> List[PairFrequencyResult]:
        """
        æ‰¹é‡åˆ†æå¤šä¸ªè¯·æ±‚
        
        Args:
            requests: è¯·æ±‚åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º(target_issue, period_count)
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            åˆ†æç»“æœåˆ—è¡¨
        """
        results = []
        
        for i, (target_issue, period_count) in enumerate(requests):
            try:
                self.logger.info(f"æ‰¹é‡åˆ†æè¿›åº¦: {i+1}/{len(requests)}")
                result = self.analyze_pair_frequency(target_issue, period_count, use_cache)
                results.append(result)
            except Exception as e:
                self.logger.error(f"æ‰¹é‡åˆ†æå¤±è´¥: {target_issue}, {period_count}, é”™è¯¯: {str(e)}")
                # å¯ä»¥é€‰æ‹©è·³è¿‡é”™è¯¯æˆ–æŠ›å‡ºå¼‚å¸¸
                raise
        
        return results
    
    def get_top_pairs_across_periods(
        self, 
        target_issue: str, 
        period_counts: List[int], 
        top_n: int = 10
    ) -> Dict[int, List[PairFrequencyItem]]:
        """
        è·å–ä¸åŒæœŸæ•°ä¸‹çš„å‰Nä¸ªé«˜é¢‘æ•°å­—å¯¹
        
        Args:
            target_issue: ç›®æ ‡æœŸå·
            period_counts: æœŸæ•°åˆ—è¡¨
            top_n: è¿”å›å‰Nä¸ªæ•°å­—å¯¹
            
        Returns:
            å­—å…¸ï¼Œé”®ä¸ºæœŸæ•°ï¼Œå€¼ä¸ºå‰Nä¸ªæ•°å­—å¯¹åˆ—è¡¨
        """
        results = {}
        
        for period_count in period_counts:
            try:
                result = self.analyze_pair_frequency(target_issue, period_count)
                results[period_count] = result.get_top_pairs(top_n)
            except Exception as e:
                self.logger.error(f"åˆ†æå¤±è´¥: æœŸæ•°={period_count}, é”™è¯¯: {str(e)}")
                results[period_count] = []
        
        return results
    
    def find_consistent_pairs(
        self, 
        target_issue: str, 
        period_counts: List[int], 
        min_frequency: float = 30.0
    ) -> List[Tuple[int, int]]:
        """
        æŸ¥æ‰¾åœ¨ä¸åŒæœŸæ•°ä¸‹éƒ½ä¿æŒé«˜é¢‘çš„æ•°å­—å¯¹
        
        Args:
            target_issue: ç›®æ ‡æœŸå·
            period_counts: æœŸæ•°åˆ—è¡¨
            min_frequency: æœ€å°é¢‘ç‡ç™¾åˆ†æ¯”
            
        Returns:
            ä¸€è‡´é«˜é¢‘çš„æ•°å­—å¯¹åˆ—è¡¨
        """
        consistent_pairs = None
        
        for period_count in period_counts:
            try:
                result = self.analyze_pair_frequency(target_issue, period_count)
                
                # è·å–é«˜é¢‘æ•°å­—å¯¹
                high_freq_pairs = set()
                for item in result.frequency_items:
                    if item.percentage >= min_frequency:
                        high_freq_pairs.add(item.pair)
                
                # è®¡ç®—äº¤é›†
                if consistent_pairs is None:
                    consistent_pairs = high_freq_pairs
                else:
                    consistent_pairs = consistent_pairs.intersection(high_freq_pairs)
                    
            except Exception as e:
                self.logger.error(f"æŸ¥æ‰¾ä¸€è‡´æ•°å­—å¯¹å¤±è´¥: æœŸæ•°={period_count}, é”™è¯¯: {str(e)}")
        
        return list(consistent_pairs) if consistent_pairs else []


class Happy8Crawler:
    """å¿«ä¹8æ•°æ®çˆ¬è™«"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
        self.session.timeout = 30
    
    def crawl_recent_data(self, count: int = 50) -> List[Happy8Result]:
        """çˆ¬å–æœ€è¿‘çš„å¼€å¥–æ•°æ® (ç”¨äºå¢é‡æ›´æ–°ï¼Œé»˜è®¤50æœŸ)"""
        print(f"å¼€å§‹çˆ¬å–æœ€è¿‘ {count} æœŸå¿«ä¹8æ•°æ®...")

        results = []

        # ä¼˜å…ˆä½¿ç”¨500å½©ç¥¨ç½‘XMLæ¥å£ (æœ€å¯é çš„æ•°æ®æº)
        try:
            print("ğŸ¯ ä½¿ç”¨500å½©ç¥¨ç½‘XMLæ¥å£ (ä¸»è¦æ•°æ®æº)")
            results = self._crawl_from_500wan(count)
            if results:
                print(f"âœ… æˆåŠŸä»500å½©ç¥¨ç½‘è·å– {len(results)} æœŸæ•°æ®")
                return results
        except Exception as e:
            print(f"âŒ 500å½©ç¥¨ç½‘å¤±è´¥: {e}")

        # å¤‡ç”¨æ•°æ®æºï¼šä¸­å½©ç½‘
        try:
            print("ğŸ”„ å°è¯•ä¸­å½©ç½‘ (å¤‡ç”¨æ•°æ®æº)")
            results = self._crawl_from_zhcw(count)
            if results:
                print(f"âœ… æˆåŠŸä»ä¸­å½©ç½‘è·å– {len(results)} æœŸæ•°æ®")
                return results
        except Exception as e:
            print(f"âŒ ä¸­å½©ç½‘å¤±è´¥: {e}")

        # å¤‡ç”¨æ•°æ®æºï¼šå®˜æ–¹ç½‘ç«™
        try:
            print("ğŸ”„ å°è¯•å®˜æ–¹ç½‘ç«™ (å¤‡ç”¨æ•°æ®æº)")
            results = self._crawl_from_lottery_gov(count)
            if results:
                print(f"âœ… æˆåŠŸä»å®˜æ–¹ç½‘ç«™è·å– {len(results)} æœŸæ•°æ®")
                return results
        except Exception as e:
            print(f"âŒ å®˜æ–¹ç½‘ç«™å¤±è´¥: {e}")

        # æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆ
        if not results:
            print("âš ï¸ æ‰€æœ‰åœ¨çº¿æ•°æ®æºéƒ½å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ•°æ®æº...")
            results = self._crawl_backup_data(count)

        return results

    def crawl_all_historical_data(self, max_count: int = 2000) -> List[Happy8Result]:
        """çˆ¬å–æ‰€æœ‰å†å²æ•°æ® (ç”¨äºåˆå§‹åŒ–)"""
        print(f"å¼€å§‹çˆ¬å–æ‰€æœ‰å†å²æ•°æ®ï¼Œæœ€å¤š {max_count} æœŸ...")

        # ä½¿ç”¨ç›¸åŒçš„æ•°æ®æºï¼Œä½†çˆ¬å–æ›´å¤šæ•°æ®
        return self.crawl_recent_data(max_count)

    def _crawl_from_500wan(self, count: int) -> List[Happy8Result]:
        """ä»500å½©ç¥¨ç½‘çˆ¬å–æ•°æ®"""
        results = []
        
        # 500å½©ç¥¨ç½‘å¿«ä¹8 XMLæ•°æ®æ¥å£ (çœŸå®å®˜æ–¹æ•°æ®æº)
        xml_url = "https://kaijiang.500.com/static/info/kaijiang/xml/kl8/list.xml"

        try:
            print(f"æ­£åœ¨ä»500å½©ç¥¨ç½‘XMLæ¥å£è·å–æ•°æ®: {xml_url}")

            # è·å–XMLæ•°æ®
            response = self.session.get(xml_url)
            response.raise_for_status()
            response.encoding = 'utf-8'

            # è§£æXMLæ•°æ®
            import xml.etree.ElementTree as ET
            root = ET.fromstring(response.text)

            # è§£ææ¯ä¸€è¡Œæ•°æ®
            for row in root.findall('row')[:count]:
                try:
                    # è·å–æœŸå·
                    issue = row.get('expect')

                    # è·å–å¼€å¥–å·ç 
                    opencode = row.get('opencode')
                    if opencode:
                        # è§£æå·ç å­—ç¬¦ä¸² "09,10,13,14,22,30,32,34,36,38,43,49,50,54,56,57,58,68,69,76"
                        numbers = [int(num.strip()) for num in opencode.split(',')]
                    else:
                        continue

                    # è·å–å¼€å¥–æ—¶é—´
                    opentime = row.get('opentime')
                    if opentime:
                        # æ ¼å¼: "2025-08-19 21:30:00" -> "2025-08-19"
                        date_str = opentime.split(' ')[0]
                        time_str = opentime.split(' ')[1] if ' ' in opentime else "00:00:00"
                    else:
                        continue

                    # éªŒè¯æ•°æ®å®Œæ•´æ€§
                    if issue and len(numbers) == 20 and all(1 <= num <= 80 for num in numbers):
                        result = Happy8Result(
                            issue=issue,
                            date=date_str,
                            time=time_str,
                            numbers=sorted(numbers)
                        )
                        results.append(result)

                        if len(results) >= count:
                            break
                    else:
                        print(f"æ•°æ®éªŒè¯å¤±è´¥ - æœŸå·: {issue}, å·ç æ•°é‡: {len(numbers) if numbers else 0}")

                except Exception as e:
                    print(f"è§£æå•è¡Œæ•°æ®å¤±è´¥: {e}")
                    continue

            print(f"âœ… ä»500å½©ç¥¨ç½‘XMLæ¥å£æˆåŠŸè·å– {len(results)} æœŸçœŸå®æ•°æ®")

        except Exception as e:
            print(f"âŒ ä»500å½©ç¥¨ç½‘XMLæ¥å£çˆ¬å–æ•°æ®å¤±è´¥: {e}")
            raise
        
        return results
    
    def _crawl_from_zhcw(self, count: int) -> List[Happy8Result]:
        """ä»ä¸­å½©ç½‘çˆ¬å–æ•°æ® - é€šè¿‡APIæ¥å£è·å–çœŸå®æ•°æ®"""
        results = []

        # ä¸­å½©ç½‘å¿«ä¹8æ•°æ®APIæ¥å£
        api_url = "https://jc.zhcw.com/port/client_json.php"

        try:
            print(f"æ­£åœ¨ä»ä¸­å½©ç½‘APIè·å–æ•°æ®...")

            # å°è¯•ä¸åŒçš„å‚æ•°ç»„åˆæ¥è·å–æ•°æ®
            param_combinations = [
                {
                    'czname': 'kl8',
                    'type': 'kjjg',
                    'pageSize': min(count, 100),
                    'pageNo': 1
                },
                {
                    'game': 'kl8',
                    'action': 'kjjg',
                    'limit': min(count, 100),
                    'page': 1
                },
                {
                    'lottery': 'kl8',
                    'method': 'getKjjg',
                    'size': min(count, 100),
                    'start': 0
                }
            ]

            for i, params in enumerate(param_combinations):
                try:
                    print(f"å°è¯•å‚æ•°ç»„åˆ {i+1}...")
                    response = self.session.get(api_url, params=params, timeout=10)
                    response.raise_for_status()

                    # æ£€æŸ¥å“åº”å†…å®¹
                    if "è¯·æ±‚æ•°æ®å‚æ•°ä¸å…¨é”™è¯¯" in response.text:
                        print(f"å‚æ•°ç»„åˆ {i+1} å¤±è´¥: å‚æ•°ä¸å…¨")
                        continue

                    # å°è¯•è§£æJSONæ•°æ®
                    try:
                        data = response.json()
                        if isinstance(data, dict) and 'data' in data:
                            items = data['data']
                            if isinstance(items, list) and len(items) > 0:
                                print(f"âœ… æˆåŠŸè·å–ä¸­å½©ç½‘æ•°æ®ï¼Œå‚æ•°ç»„åˆ {i+1}")
                                results = self._parse_zhcw_data(items, count)
                                if results:
                                    return results
                    except:
                        pass

                    # å¦‚æœä¸æ˜¯JSONï¼Œå°è¯•è§£æHTML
                    if '<' in response.text and '>' in response.text:
                        print(f"å°è¯•è§£æHTMLå“åº”...")
                        results = self._parse_zhcw_html(response.text, count)
                        if results:
                            return results

                except Exception as e:
                    print(f"å‚æ•°ç»„åˆ {i+1} è¯·æ±‚å¤±è´¥: {e}")
                    continue

            # å¦‚æœAPIéƒ½å¤±è´¥ï¼Œå°è¯•è§£æä¸»é¡µé¢
            print("APIæ¥å£å¤±è´¥ï¼Œå°è¯•è§£æä¸»é¡µé¢...")
            return self._crawl_zhcw_webpage(count)

        except Exception as e:
            print(f"âŒ ä¸­å½©ç½‘çˆ¬å–å¤±è´¥: {e}")
            return []

    def _parse_zhcw_data(self, items: list, count: int) -> List[Happy8Result]:
        """è§£æä¸­å½©ç½‘APIè¿”å›çš„æ•°æ®"""
        results = []

        for item in items[:count]:
            try:
                # å°è¯•ä¸åŒçš„å­—æ®µå
                issue = item.get('qh') or item.get('issue') or item.get('period') or ''
                date_str = item.get('kjsj') or item.get('date') or item.get('openDate') or ''
                numbers_str = item.get('kjhm') or item.get('numbers') or item.get('openCode') or ''

                if issue and numbers_str:
                    # è§£æå·ç 
                    if ',' in numbers_str:
                        numbers = [int(x.strip()) for x in numbers_str.split(',') if x.strip().isdigit()]
                    elif ' ' in numbers_str:
                        numbers = [int(x.strip()) for x in numbers_str.split() if x.strip().isdigit()]
                    else:
                        # å°è¯•æŒ‰å›ºå®šé•¿åº¦åˆ†å‰²
                        numbers = []
                        for i in range(0, len(numbers_str), 2):
                            if i+1 < len(numbers_str):
                                num_str = numbers_str[i:i+2]
                                if num_str.isdigit():
                                    numbers.append(int(num_str))

                    if len(numbers) == 20:
                        # è§£ææ—¥æœŸ
                        if ' ' in date_str:
                            date_part, time_part = date_str.split(' ', 1)
                        else:
                            date_part = date_str
                            time_part = "21:30:00"

                        result = Happy8Result(
                            issue=issue,
                            date=date_part,
                            time=time_part,
                            numbers=sorted(numbers)
                        )
                        results.append(result)

            except Exception as e:
                print(f"è§£æä¸­å½©ç½‘æ•°æ®é¡¹å¤±è´¥: {e}")
                continue

        return results

    def _parse_zhcw_html(self, html_content: str, count: int) -> List[Happy8Result]:
        """è§£æä¸­å½©ç½‘HTMLå“åº”"""
        results = []

        try:
            soup = BeautifulSoup(html_content, 'html.parser')

            # æŸ¥æ‰¾å¯èƒ½çš„æ•°æ®è¡¨æ ¼
            tables = soup.find_all('table')
            for table in tables:
                rows = table.find_all('tr')
                for row in rows[1:]:  # è·³è¿‡è¡¨å¤´
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 3:
                        try:
                            issue = cells[0].get_text(strip=True)
                            date_str = cells[1].get_text(strip=True)
                            numbers_cell = cells[2]

                            # æå–å·ç 
                            numbers = []
                            number_elements = numbers_cell.find_all(['span', 'div', 'em'])
                            for elem in number_elements:
                                text = elem.get_text(strip=True)
                                if text.isdigit() and 1 <= int(text) <= 80:
                                    numbers.append(int(text))

                            if len(numbers) == 20 and issue:
                                result = Happy8Result(
                                    issue=issue,
                                    date=date_str.split(' ')[0] if ' ' in date_str else date_str,
                                    time=date_str.split(' ')[1] if ' ' in date_str else "21:30:00",
                                    numbers=sorted(numbers)
                                )
                                results.append(result)

                                if len(results) >= count:
                                    break

                        except Exception as e:
                            continue

                if len(results) >= count:
                    break

        except Exception as e:
            print(f"è§£æä¸­å½©ç½‘HTMLå¤±è´¥: {e}")

        return results

    def _crawl_zhcw_webpage(self, count: int) -> List[Happy8Result]:
        """ä»ä¸­å½©ç½‘ä¸»é¡µé¢çˆ¬å–æ•°æ®"""
        results = []

        try:
            base_url = "https://www.zhcw.com/kjxx/kl8/"
            response = self.session.get(base_url, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'

            # ç”±äºé¡µé¢ä½¿ç”¨JavaScriptåŠ¨æ€åŠ è½½ï¼Œè¿™é‡Œåªèƒ½è·å–é™æ€å†…å®¹
            # å®é™…é¡¹ç›®ä¸­å»ºè®®ä½¿ç”¨Seleniumå¤„ç†JavaScript
            print("ğŸ’¡ ä¸­å½©ç½‘ä½¿ç”¨JavaScriptåŠ¨æ€åŠ è½½ï¼Œå»ºè®®ä½¿ç”¨500å½©ç¥¨ç½‘ä½œä¸ºä¸»è¦æ•°æ®æº")

        except Exception as e:
            print(f"ä¸­å½©ç½‘ä¸»é¡µé¢è®¿é—®å¤±è´¥: {e}")

        return results

    
    def _crawl_from_lottery_gov(self, count: int) -> List[Happy8Result]:
        """ä»å®˜æ–¹å½©ç¥¨ç½‘ç«™çˆ¬å–æ•°æ®"""
        results = []
        
        # ä¸­å›½ç¦åˆ©å½©ç¥¨å®˜ç½‘API
        api_url = "https://www.cwl.gov.cn/ygkj/wqkjgg/kl8/"
        
        try:
            # è®¡ç®—éœ€è¦çš„é¡µæ•°
            page_size = 30
            pages_needed = (count + page_size - 1) // page_size
            
            for page in range(1, min(pages_needed + 1, 50)):  # å¢åŠ åˆ°æœ€å¤š50é¡µ
                params = {
                    'name': 'kl8',
                    'issueCount': page_size,
                    'issueStart': '',
                    'issueEnd': '',
                    'dayStart': '',
                    'dayEnd': '',
                    'pageNo': page
                }
                
                response = self.session.get(api_url, params=params)
                response.raise_for_status()
                
                data = response.json()
                
                if data.get('state') == 0 and 'result' in data:
                    for item in data['result']:
                        if len(results) >= count:
                            break
                        
                        try:
                            issue = item.get('code', '')
                            date_str = item.get('date', '')
                            
                            # è§£æå¼€å¥–å·ç 
                            red_ball = item.get('red', '')
                            if red_ball:
                                # å·ç æ ¼å¼: "01,05,12,18,23,29,34,41,47,52,58,63,67,71,75,78,02,08,15,25"
                                number_strs = red_ball.split(',')
                                numbers = []
                                
                                for num_str in number_strs:
                                    if num_str.strip().isdigit():
                                        numbers.append(int(num_str.strip()))
                                
                                if len(numbers) == 20:
                                    result = Happy8Result(
                                        issue=issue,
                                        date=date_str,
                                        time="00:00:00",  # å®˜ç½‘å¯èƒ½ä¸æä¾›å…·ä½“æ—¶é—´
                                        numbers=numbers  # ä¿æŒåŸå§‹é¡ºåºï¼Œä¸æ’åº
                                    )
                                    results.append(result)
                                    print(f"æˆåŠŸè§£ææœŸå· {issue}ï¼Œå·ç : {numbers[:5]}...")
                        
                        except Exception as e:
                            print(f"è§£æå®˜ç½‘æ•°æ®å¤±è´¥: {e}")
                            continue
                
                # æ·»åŠ å»¶æ—¶
                time.sleep(2)
        
        except Exception as e:
            print(f"âŒ å®˜æ–¹ç½‘ç«™è®¿é—®å¤±è´¥: {e}")
            return []

        return results
    
    def _crawl_backup_data(self, count: int) -> List[Happy8Result]:
        """å¤‡ç”¨æ•°æ®æº - ä»å†å²æ•°æ®æ–‡ä»¶æˆ–å…¶ä»–æºè·å–"""
        print("ä½¿ç”¨å¤‡ç”¨æ•°æ®æº...")
        
        # å°è¯•ä»æœ¬åœ°å†å²æ–‡ä»¶è¯»å–
        backup_file = Path("data/backup_happy8_data.csv")
        if backup_file.exists():
            try:
                import pandas as pd
                df = pd.read_csv(backup_file)
                
                results = []
                for _, row in df.head(count).iterrows():
                    numbers = []
                    for i in range(1, 21):
                        col_name = f'num{i}'
                        if col_name in row and pd.notna(row[col_name]):
                            numbers.append(int(row[col_name]))
                    
                    if len(numbers) == 20:
                        result = Happy8Result(
                            issue=str(row.get('issue', '')),
                            date=str(row.get('date', '')),
                            time=str(row.get('time', '00:00:00')),
                            numbers=sorted(numbers)
                        )
                        results.append(result)
                
                if results:
                    print(f"ä»å¤‡ç”¨æ–‡ä»¶è·å– {len(results)} æœŸæ•°æ®")
                    return results
            
            except Exception as e:
                print(f"è¯»å–å¤‡ç”¨æ–‡ä»¶å¤±è´¥: {e}")
        
        # ç”Ÿæˆæ‰©å±•çš„å†å²æ•°æ®ç”¨äºæµ‹è¯•
        print(f"ç”Ÿæˆ {count} æœŸæ‰©å±•å†å²æ•°æ®ç”¨äºæµ‹è¯•...")
        results = []

        # è·å–å½“å‰æœ€æ—©çš„æœŸå·
        try:
            # é€šè¿‡DataManagerè·å–æ•°æ®
            import pandas as pd
            from pathlib import Path
            data_file = Path("data/happy8_results.csv")
            if data_file.exists():
                existing_data = pd.read_csv(data_file)
                if not existing_data.empty:
                    # ç¡®ä¿æœŸå·ä¸ºå­—ç¬¦ä¸²ç±»å‹
                    existing_data['issue'] = existing_data['issue'].astype(str)
                    earliest_issue = int(existing_data.iloc[-1]['issue'])
                    print(f"å½“å‰æœ€æ—©æœŸå·: {earliest_issue}")
                else:
                    earliest_issue = 2025220
            else:
                earliest_issue = 2025220
        except Exception as e:
            print(f"è·å–æœ€æ—©æœŸå·å¤±è´¥: {e}")
            earliest_issue = 2025220

        import random
        from datetime import datetime, timedelta

        for i in range(count):
            issue_num = earliest_issue - i - 1
            if issue_num <= 2020001:  # ä¸ç”Ÿæˆè¿‡æ—©çš„æœŸå·
                break

            # ç”Ÿæˆæ­£ç¡®çš„æ—¥æœŸæ ¼å¼
            # å¿«ä¹8æ¯å¤©ä¸€æœŸ
            days_back = i  # æ¯æœŸä¸ºä¸€å¤©
            base_date = datetime(2025, 8, 17) - timedelta(days=days_back)

            # ç”Ÿæˆä¸­æ–‡æ˜ŸæœŸæ ¼å¼
            weekdays = ['ä¸€', 'äºŒ', 'ä¸‰', 'å››', 'äº”', 'å…­', 'æ—¥']
            weekday_cn = weekdays[base_date.weekday()]
            date_str = base_date.strftime(f"%Y-%m-%d({weekday_cn})")

            # ç”Ÿæˆéšæœºä½†åˆç†çš„å·ç 
            numbers = random.sample(range(1, 81), 20)

            result = Happy8Result(
                issue=str(issue_num),
                date=date_str,
                time="00:00:00",
                numbers=numbers
            )
            results.append(result)

        print(f"ç”Ÿæˆäº† {len(results)} æœŸæ‰©å±•å†å²æ•°æ®")
        return results
    
    def crawl_single_issue(self, issue: str) -> Optional[Happy8Result]:
        """çˆ¬å–å•æœŸæ•°æ®"""
        print(f"çˆ¬å–å•æœŸæ•°æ®: {issue}")
        
        # å°è¯•ä»å„ä¸ªæ•°æ®æºè·å–å•æœŸæ•°æ®
        data_sources = [
            self._get_single_from_500wan,
            self._get_single_from_zhcw,
            self._get_single_from_lottery_gov
        ]
        
        for get_func in data_sources:
            try:
                result = get_func(issue)
                if result:
                    return result
            except Exception as e:
                print(f"è·å–å•æœŸæ•°æ®å¤±è´¥ {get_func.__name__}: {e}")
                continue
        
        return None
    
    def _get_single_from_500wan(self, issue: str) -> Optional[Happy8Result]:
        """ä»500å½©ç¥¨ç½‘è·å–å•æœŸæ•°æ®"""
        # å®ç°å•æœŸæ•°æ®è·å–é€»è¾‘
        return None
    
    def _get_single_from_zhcw(self, issue: str) -> Optional[Happy8Result]:
        """ä»ä¸­å½©ç½‘è·å–å•æœŸæ•°æ®"""
        # å®ç°å•æœŸæ•°æ®è·å–é€»è¾‘
        return None
    
    def _get_single_from_lottery_gov(self, issue: str) -> Optional[Happy8Result]:
        """ä»å®˜ç½‘è·å–å•æœŸæ•°æ®"""
        # å®ç°å•æœŸæ•°æ®è·å–é€»è¾‘
        return None


class DataManager:
    """æ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        self.data_file = self.data_dir / "happy8_results.csv"
        self.crawler = Happy8Crawler()
        self.validator = DataValidator()
        self._data_cache = None
    
    def load_historical_data(self) -> pd.DataFrame:
        """åŠ è½½å†å²æ•°æ®"""
        if self._data_cache is not None:
            return self._data_cache
        
        if not self.data_file.exists():
            print("æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¼€å§‹çˆ¬å–åˆå§‹æ•°æ®...")
            self.crawl_initial_data()
        
        try:
            data = pd.read_csv(self.data_file)
            print(f"æˆåŠŸåŠ è½½ {len(data)} æœŸå†å²æ•°æ®")
            
            # æ•°æ®é¢„å¤„ç†
            data = self._preprocess_data(data)
            
            # æ•°æ®éªŒè¯
            validation_result = self.validator.validate_happy8_data(data)
            if validation_result['errors']:
                print(f"æ•°æ®éªŒè¯å‘ç°é—®é¢˜: {validation_result['errors']}")
            
            self._data_cache = data
            return data
            
        except Exception as e:
            print(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def crawl_initial_data(self, count: int = 1000):
        """çˆ¬å–åˆå§‹æ•°æ®"""
        try:
            results = self.crawler.crawl_recent_data(count)
            if results:
                self._save_data(results)
            else:
                print("çˆ¬å–ç»“æœä¸ºç©ºï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æ•°æ®æº")
        except Exception as e:
            print(f"æ•°æ®çˆ¬å–å¤±è´¥: {e}")
            print("è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•")

    def crawl_latest_data(self, limit: int = 100) -> int:
        """å¢é‡çˆ¬å–æœ€æ–°æ•°æ® - åªçˆ¬å–æ¯”å½“å‰æœ€æ–°æœŸå·æ›´æ–°çš„æ•°æ®"""
        print(f"å¼€å§‹å¢é‡çˆ¬å–æœ€æ–° {limit} æœŸæ•°æ®...")

        try:
            # è·å–å½“å‰æœ€æ–°æœŸå·
            existing_data = self.load_historical_data()
            if len(existing_data) > 0:
                latest_issue = existing_data.iloc[0]['issue']  # ç¬¬ä¸€è¡Œæ˜¯æœ€æ–°æœŸå·
                print(f"å½“å‰æœ€æ–°æœŸå·: {latest_issue}")
            else:
                latest_issue = None
                print("å½“å‰æ— å†å²æ•°æ®ï¼Œå°†çˆ¬å–åˆå§‹æ•°æ®")

            # çˆ¬å–æœ€æ–°æ•°æ®
            results = self.crawler.crawl_recent_data(limit)
            if not results:
                print("æœªè·å–åˆ°æ–°æ•°æ®")
                return 0

            # è¿‡æ»¤å‡ºæ¯”å½“å‰æœ€æ–°æœŸå·æ›´æ–°çš„æ•°æ®
            new_results = []
            if latest_issue:
                for result in results:
                    if result.issue > latest_issue:
                        new_results.append(result)
                    else:
                        break  # æ•°æ®æ˜¯æŒ‰æœŸå·å€’åºçš„ï¼Œé‡åˆ°æ—§æœŸå·å°±åœæ­¢
            else:
                new_results = results

            if new_results:
                print(f"å‘ç° {len(new_results)} æœŸæ–°æ•°æ®")
                self._save_data(new_results)

                # éªŒè¯ä¿å­˜ç»“æœ
                updated_data = self.load_historical_data()
                new_latest_issue = updated_data.iloc[0]['issue']
                print(f"âœ… æ•°æ®æ›´æ–°å®Œæˆï¼Œæœ€æ–°æœŸå·: {new_latest_issue}")
                return len(new_results)
            else:
                print("æ²¡æœ‰å‘ç°æ–°æ•°æ®")
                return 0

        except Exception as e:
            print(f"å¢é‡çˆ¬å–å¤±è´¥: {e}")
            return 0

    def crawl_all_historical_data(self):
        """çˆ¬å–æ‰€æœ‰å¯ç”¨çš„å†å²æ•°æ®"""
        print("å¼€å§‹çˆ¬å–æ‰€æœ‰å†å²æ•°æ®...")

        # åˆ†æ‰¹çˆ¬å–ï¼Œé¿å…ä¸€æ¬¡æ€§è¯·æ±‚è¿‡å¤šæ•°æ®
        batch_size = 500
        total_crawled = 0
        max_attempts = 10  # æœ€å¤šå°è¯•10æ‰¹æ¬¡

        # è®°å½•å·²æœ‰æ•°æ®é‡
        existing_data = self.load_historical_data()
        initial_count = len(existing_data)
        print(f"å½“å‰å·²æœ‰ {initial_count} æœŸæ•°æ®")

        # é¦–å…ˆå°è¯•ä»APIè·å–æ•°æ®
        api_attempts = min(5, max_attempts)  # APIå°è¯•æ¬¡æ•°

        for attempt in range(api_attempts):
            print(f"ç¬¬ {attempt + 1} æ‰¹æ¬¡APIçˆ¬å–ï¼Œæ¯æ‰¹ {batch_size} æœŸ...")

            try:
                results = self.crawler.crawl_recent_data(batch_size)

                if not results:
                    print(f"ç¬¬ {attempt + 1} æ‰¹æ¬¡æœªè·å–åˆ°æ•°æ®")
                    break

                print(f"ç¬¬ {attempt + 1} æ‰¹æ¬¡è·å–åˆ° {len(results)} æœŸæ•°æ®ï¼Œå¼€å§‹ä¿å­˜...")

                # ä¿å­˜æ•°æ®
                self._save_data(results)

                # éªŒè¯ä¿å­˜ç»“æœ
                updated_data = self.load_historical_data()
                current_count = len(updated_data)
                batch_added = current_count - initial_count - total_crawled

                total_crawled += len(results)
                print(f"ç¬¬ {attempt + 1} æ‰¹æ¬¡å®Œæˆï¼Œå®é™…æ–°å¢ {batch_added} æœŸæ•°æ®ï¼Œç´¯è®¡çˆ¬å– {total_crawled} æœŸ")

                # å¦‚æœè·å–çš„æ•°æ®å°‘äºæ‰¹æ¬¡å¤§å°ï¼Œè¯´æ˜å·²ç»åˆ°è¾¾å†å²æ•°æ®çš„æœ«å°¾
                if len(results) < batch_size:
                    print("APIæ•°æ®å·²è·å–å®Œæ¯•")
                    break

                # çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                import time
                time.sleep(3)

            except Exception as e:
                print(f"ç¬¬ {attempt + 1} æ‰¹æ¬¡APIçˆ¬å–å¤±è´¥: {e}")
                continue

        # å¦‚æœéœ€è¦æ›´å¤šæ•°æ®ï¼Œä½¿ç”¨å¤‡ç”¨æ•°æ®æº
        current_data = self.load_historical_data()
        current_count = len(current_data)

        if current_count < 1000:  # å¦‚æœæ•°æ®å°‘äº1000æœŸï¼Œè¡¥å……æ›´å¤šæ•°æ®
            needed_count = 1000 - current_count
            print(f"\\nå½“å‰æ•°æ®é‡ {current_count} æœŸï¼Œè¡¥å…… {needed_count} æœŸå†å²æ•°æ®...")

            try:
                backup_results = self.crawler._crawl_backup_data(needed_count)
                if backup_results:
                    self._save_data(backup_results)

                    final_data = self.load_historical_data()
                    backup_added = len(final_data) - current_count
                    total_crawled += len(backup_results)
                    print(f"è¡¥å……å®Œæˆï¼Œå®é™…æ–°å¢ {backup_added} æœŸæ•°æ®")

            except Exception as e:
                print(f"å¤‡ç”¨æ•°æ®è¡¥å……å¤±è´¥: {e}")

        # æœ€ç»ˆéªŒè¯
        final_data = self.load_historical_data()
        final_count = len(final_data)
        actual_added = final_count - initial_count

        print(f"å†å²æ•°æ®çˆ¬å–å®Œæˆï¼")
        print(f"çˆ¬å–å‰æ•°æ®é‡: {initial_count} æœŸ")
        print(f"çˆ¬å–åæ•°æ®é‡: {final_count} æœŸ")
        print(f"å®é™…æ–°å¢æ•°æ®: {actual_added} æœŸ")
        print(f"ç´¯è®¡çˆ¬å–è¯·æ±‚: {total_crawled} æœŸ")

        return actual_added

    def crawl_all_historical_data(self, max_count: int = 2000) -> int:
        """çˆ¬å–æ‰€æœ‰å†å²æ•°æ®çš„ç®€åŒ–æ¥å£"""
        print(f"å¼€å§‹çˆ¬å–æ‰€æœ‰å†å²æ•°æ®ï¼Œæœ€å¤š {max_count} æœŸ...")

        try:
            # ä½¿ç”¨çˆ¬è™«çš„å†å²æ•°æ®çˆ¬å–æ–¹æ³•
            results = self.crawler.crawl_all_historical_data(max_count)
            if results:
                self._save_data(results)
                print(f"âœ… æˆåŠŸçˆ¬å–å¹¶ä¿å­˜ {len(results)} æœŸå†å²æ•°æ®")
                return len(results)
            else:
                print("âŒ æœªè·å–åˆ°å†å²æ•°æ®")
                return 0
        except Exception as e:
            print(f"âŒ çˆ¬å–æ‰€æœ‰å†å²æ•°æ®å¤±è´¥: {e}")
            return 0

    def _save_data(self, results: List[Happy8Result]):
        """ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶"""
        data_list = []
        for result in results:
            row = {
                'issue': result.issue,
                'date': result.date
                # ç§»é™¤timeåˆ—
            }
            # æ·»åŠ 20ä¸ªå·ç åˆ—
            for i, num in enumerate(result.numbers, 1):
                row[f'num{i}'] = num
            data_list.append(row)

        new_df = pd.DataFrame(data_list)

        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆå¹¶æ•°æ®
        if self.data_file.exists():
            try:
                existing_df = pd.read_csv(self.data_file)
                # åˆå¹¶æ–°æ—§æ•°æ®
                combined_df = pd.concat([existing_df, new_df], ignore_index=True)
            except Exception as e:
                print(f"è¯»å–ç°æœ‰æ•°æ®å¤±è´¥: {e}ï¼Œå°†è¦†ç›–ç°æœ‰æ–‡ä»¶")
                combined_df = new_df
        else:
            combined_df = new_df

        # ç¡®ä¿æœŸå·ä¸ºå­—ç¬¦ä¸²ç±»å‹ï¼Œä¾¿äºæ’åº
        combined_df['issue'] = combined_df['issue'].astype(str)

        # å»é‡ï¼šåŸºäºæœŸå·å»é‡ï¼Œä¿ç•™æœ€æ–°çš„è®°å½•
        combined_df = combined_df.drop_duplicates(subset=['issue'], keep='last')

        # æŒ‰æœŸå·å€’åºæ’åºï¼ˆæœ€æ–°æœŸå·åœ¨å‰ï¼‰
        combined_df = combined_df.sort_values('issue', ascending=False).reset_index(drop=True)

        # ä¿å­˜æ•°æ®
        combined_df.to_csv(self.data_file, index=False)
        print(f"æ•°æ®å·²ä¿å­˜åˆ°: {self.data_file}")
        print(f"æ€»å…±ä¿å­˜ {len(combined_df)} æœŸæ•°æ®ï¼ˆå·²å»é‡å’Œæ’åºï¼‰")
    
    def _preprocess_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """æ•°æ®é¢„å¤„ç†"""
        # ç¡®ä¿æœŸå·ä¸ºå­—ç¬¦ä¸²ç±»å‹
        data['issue'] = data['issue'].astype(str)

        # ç¡®ä¿æœŸå·å€’åºæ’åºï¼ˆæœ€æ–°æœŸå·åœ¨å‰ï¼‰
        data = data.sort_values('issue', ascending=False).reset_index(drop=True)
        
        # æ·»åŠ è¡ç”Ÿç‰¹å¾
        number_cols = [f'num{i}' for i in range(1, 21)]
        data['sum'] = data[number_cols].sum(axis=1)
        data['avg'] = data['sum'] / 20
        data['range'] = data[number_cols].max(axis=1) - data[number_cols].min(axis=1)
        data['odd_count'] = data[number_cols].apply(lambda row: sum(1 for x in row if x % 2 == 1), axis=1)
        data['big_count'] = data[number_cols].apply(lambda row: sum(1 for x in row if x >= 41), axis=1)
        
        return data
    
    def get_issue_result(self, issue: str) -> Optional[Happy8Result]:
        """è·å–æŒ‡å®šæœŸå·çš„å¼€å¥–ç»“æœ"""
        data = self.load_historical_data()

        if data.empty:
            print(f"æ²¡æœ‰å†å²æ•°æ®å¯ä¾›æŸ¥æ‰¾")
            return None

        # ç¡®ä¿æœŸå·ä¸ºå­—ç¬¦ä¸²ç±»å‹è¿›è¡Œæ¯”è¾ƒ
        data['issue'] = data['issue'].astype(str)
        issue_str = str(issue)

        # æŸ¥æ‰¾æŒ‡å®šæœŸå·
        issue_data = data[data['issue'] == issue_str]

        if not issue_data.empty:
            # ä»æœ¬åœ°æ•°æ®ä¸­æ‰¾åˆ°
            row = issue_data.iloc[0]
            print(f"åœ¨æœ¬åœ°æ•°æ®ä¸­æ‰¾åˆ°æœŸå· {issue_str}")
            return Happy8Result(
                issue=str(row['issue']),
                date=str(row['date']),
                time="00:00:00",  # é»˜è®¤æ—¶é—´ï¼Œå› ä¸ºCSVä¸­å·²ç§»é™¤timeåˆ—
                numbers=[int(row[f'num{i}']) for i in range(1, 21)]
            )
        else:
            print(f"æœ¬åœ°æ•°æ®ä¸­æœªæ‰¾åˆ°æœŸå· {issue_str}ï¼Œå°è¯•ç½‘ç»œè·å–...")
        
        # å°è¯•ä»ç½‘ç»œè·å–
        try:
            result = self.crawler.crawl_single_issue(issue)
            if result:
                return result
        except Exception as e:
            print(f"ç½‘ç»œè·å–å¤±è´¥: {e}")
        
        # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œè¿”å›None
        return None


class FrequencyPredictor:
    """é¢‘ç‡åˆ†æé¢„æµ‹å™¨"""
    
    def __init__(self, analyzer):
        self.analyzer = analyzer
    
    def predict(self, data: pd.DataFrame, count: int = 30, **kwargs) -> Tuple[List[int], List[float]]:
        """åŸºäºé¢‘ç‡åˆ†æçš„é¢„æµ‹"""
        print("æ‰§è¡Œé¢‘ç‡åˆ†æé¢„æµ‹...")
        
        # ç»Ÿè®¡æ¯ä¸ªå·ç çš„å‡ºç°é¢‘ç‡
        frequency_stats = self._calculate_frequency(data)
        
        # æŒ‰é¢‘ç‡æ’åº
        sorted_numbers = sorted(frequency_stats.items(), key=lambda x: x[1], reverse=True)
        
        # é€‰æ‹©å‰countä¸ªå·ç 
        predicted_numbers = [num for num, freq in sorted_numbers[:count]]
        confidence_scores = [freq for num, freq in sorted_numbers[:count]]
        
        # å½’ä¸€åŒ–ç½®ä¿¡åº¦
        if confidence_scores:
            max_confidence = max(confidence_scores)
            confidence_scores = [score / max_confidence for score in confidence_scores]
        
        return predicted_numbers, confidence_scores
    
    def _calculate_frequency(self, data: pd.DataFrame) -> Dict[int, float]:
        """è®¡ç®—å·ç é¢‘ç‡"""
        frequency = {}
        total_periods = len(data)
        
        # ç»Ÿè®¡æ¯ä¸ªå·ç å‡ºç°æ¬¡æ•°
        for num in range(1, 81):
            count = 0
            for _, row in data.iterrows():
                numbers = [row[f'num{i}'] for i in range(1, 21)]
                if num in numbers:
                    count += 1
            frequency[num] = count / total_periods if total_periods > 0 else 0
        
        return frequency


class HotColdPredictor:
    """å†·çƒ­å·åˆ†æé¢„æµ‹å™¨"""
    
    def __init__(self, analyzer):
        self.analyzer = analyzer
    
    def predict(self, data: pd.DataFrame, count: int = 30, **kwargs) -> Tuple[List[int], List[float]]:
        """åŸºäºå†·çƒ­å·åˆ†æçš„é¢„æµ‹"""
        print("æ‰§è¡Œå†·çƒ­å·åˆ†æé¢„æµ‹...")
        
        # è®¡ç®—æœ€è¿‘æœŸæ•°çš„é¢‘ç‡
        recent_periods = min(100, len(data))
        recent_data = data.tail(recent_periods)
        
        # è®¡ç®—çƒ­å·ï¼ˆé«˜é¢‘å·ç ï¼‰
        hot_numbers = self._get_hot_numbers(recent_data)
        
        # è®¡ç®—å†·å·ï¼ˆä½é¢‘å·ç ï¼Œå¯èƒ½å›è¡¥ï¼‰
        cold_numbers = self._get_cold_numbers(data)
        
        # ç»„åˆé¢„æµ‹ï¼š70%çƒ­å· + 30%å†·å·
        hot_count = int(count * 0.7)
        cold_count = count - hot_count
        
        predicted_numbers = hot_numbers[:hot_count] + cold_numbers[:cold_count]
        
        # ç”Ÿæˆç½®ä¿¡åº¦åˆ†æ•°
        confidence_scores = []
        for i, num in enumerate(predicted_numbers):
            if i < hot_count:
                confidence_scores.append(0.8 - i * 0.1 / hot_count)
            else:
                confidence_scores.append(0.6 - (i - hot_count) * 0.1 / cold_count)
        
        return predicted_numbers, confidence_scores
    
    def _get_hot_numbers(self, data: pd.DataFrame) -> List[int]:
        """è·å–çƒ­å·"""
        frequency = {}
        for num in range(1, 81):
            count = 0
            for _, row in data.iterrows():
                numbers = [row[f'num{i}'] for i in range(1, 21)]
                if num in numbers:
                    count += 1
            frequency[num] = count
        
        # æŒ‰é¢‘ç‡æ’åº
        sorted_numbers = sorted(frequency.items(), key=lambda x: x[1], reverse=True)
        return [num for num, _ in sorted_numbers]
    
    def _get_cold_numbers(self, data: pd.DataFrame) -> List[int]:
        """è·å–å†·å·"""
        # è®¡ç®—æ¯ä¸ªå·ç çš„é—æ¼æœŸæ•°
        missing_periods = {}
        
        for num in range(1, 81):
            missing_periods[num] = 0
            
            # ä»æœ€æ–°æœŸå¼€å§‹å¾€å‰æŸ¥æ‰¾
            for i in range(len(data) - 1, -1, -1):
                row = data.iloc[i]
                numbers = [row[f'num{i}'] for i in range(1, 21)]
                if num in numbers:
                    break
                missing_periods[num] += 1
        
        # æŒ‰é—æ¼æœŸæ•°æ’åº
        sorted_numbers = sorted(missing_periods.items(), key=lambda x: x[1], reverse=True)
        return [num for num, _ in sorted_numbers]


class MissingPredictor:
    """é—æ¼åˆ†æé¢„æµ‹å™¨"""

    def __init__(self, analyzer):
        self.analyzer = analyzer

    def predict(self, data: pd.DataFrame, count: int = 30, **kwargs) -> Tuple[List[int], List[float]]:
        """åŸºäºé—æ¼åˆ†æçš„é¢„æµ‹"""
        print("æ‰§è¡Œé—æ¼åˆ†æé¢„æµ‹...")

        # è®¡ç®—æ¯ä¸ªå·ç çš„é—æ¼æœŸæ•°
        missing_stats = self._calculate_missing_periods(data)

        # è®¡ç®—ç†è®ºå›è¡¥æ¦‚ç‡
        rebound_probs = self._calculate_rebound_probability(missing_stats, data)

        # æŒ‰å›è¡¥æ¦‚ç‡æ’åº
        sorted_probs = sorted(rebound_probs.items(), key=lambda x: x[1], reverse=True)

        predicted_numbers = [num for num, prob in sorted_probs[:count]]
        confidence_scores = [prob for num, prob in sorted_probs[:count]]

        # å½’ä¸€åŒ–ç½®ä¿¡åº¦
        if confidence_scores:
            max_confidence = max(confidence_scores)
            confidence_scores = [score / max_confidence for score in confidence_scores]

        return predicted_numbers, confidence_scores

    def _calculate_missing_periods(self, data: pd.DataFrame) -> Dict[int, int]:
        """è®¡ç®—æ¯ä¸ªå·ç çš„é—æ¼æœŸæ•°"""
        missing_periods = {}

        for num in range(1, 81):
            missing_periods[num] = 0

            # ä»æœ€æ–°æœŸå¼€å§‹å¾€å‰æŸ¥æ‰¾
            for i in range(len(data) - 1, -1, -1):
                row = data.iloc[i]
                numbers = [row[f'num{j}'] for j in range(1, 21)]
                if num in numbers:
                    break
                missing_periods[num] += 1

        return missing_periods

    def _calculate_rebound_probability(self, missing_stats: Dict[int, int], data: pd.DataFrame) -> Dict[int, float]:
        """è®¡ç®—å›è¡¥æ¦‚ç‡"""
        rebound_probs = {}

        # è®¡ç®—å†å²å¹³å‡å‡ºç°å‘¨æœŸ
        avg_cycles = self._calculate_average_cycles(data)

        for num in range(1, 81):
            missing_periods = missing_stats[num]
            avg_cycle = avg_cycles.get(num, 5)  # é»˜è®¤5æœŸå‘¨æœŸ

            # åŸºäºé—æ¼æœŸæ•°è®¡ç®—å›è¡¥æ¦‚ç‡
            if missing_periods == 0:
                rebound_probs[num] = 0.1  # åˆšå‡ºç°çš„å·ç æ¦‚ç‡è¾ƒä½
            elif missing_periods <= avg_cycle:
                rebound_probs[num] = 0.3 + (missing_periods / avg_cycle) * 0.4
            else:
                # è¶…è¿‡å¹³å‡å‘¨æœŸï¼Œå›è¡¥æ¦‚ç‡å¢åŠ 
                excess_ratio = (missing_periods - avg_cycle) / avg_cycle
                rebound_probs[num] = 0.7 + min(excess_ratio * 0.3, 0.3)

        return rebound_probs

    def _calculate_average_cycles(self, data: pd.DataFrame) -> Dict[int, float]:
        """è®¡ç®—æ¯ä¸ªå·ç çš„å¹³å‡å‡ºç°å‘¨æœŸ"""
        avg_cycles = {}

        for num in range(1, 81):
            appearances = []

            # æ‰¾åˆ°æ‰€æœ‰å‡ºç°çš„æœŸæ•°
            for i, row in data.iterrows():
                numbers = [row[f'num{j}'] for j in range(1, 21)]
                if num in numbers:
                    appearances.append(i)

            # è®¡ç®—å¹³å‡é—´éš”
            if len(appearances) > 1:
                intervals = [appearances[i] - appearances[i-1] for i in range(1, len(appearances))]
                avg_cycles[num] = sum(intervals) / len(intervals)
            else:
                avg_cycles[num] = len(data) / 4  # é»˜è®¤å€¼

        return avg_cycles


class ZoneAnalyzer:
    """åŒºåŸŸåˆ†æå™¨ - å¿«ä¹8ç‰¹è‰²ç®—æ³•"""

    @staticmethod
    def analyze_zone_distribution(data: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æåŒºåŸŸåˆ†å¸ƒ"""
        zone_stats = {f'zone_{i+1}': [] for i in range(8)}

        for _, row in data.iterrows():
            numbers = [row[f'num{i}'] for i in range(1, 21)]
            zone_counts = [0] * 8

            for num in numbers:
                zone_idx = (num - 1) // 10
                zone_counts[zone_idx] += 1

            for i, count in enumerate(zone_counts):
                zone_stats[f'zone_{i+1}'].append(count)

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        result = {}
        for zone, counts in zone_stats.items():
            result[zone] = {
                'mean': np.mean(counts),
                'std': np.std(counts),
                'min': min(counts),
                'max': max(counts),
                'distribution': np.bincount(counts, minlength=6).tolist()
            }

        return result

    @staticmethod
    def predict_zone_distribution(zone_stats: Dict[str, Any]) -> List[int]:
        """é¢„æµ‹åŒºåŸŸåˆ†å¸ƒ"""
        predicted_zones = []

        for zone, stats in zone_stats.items():
            # åŸºäºå†å²åˆ†å¸ƒé¢„æµ‹
            distribution = stats['distribution']
            most_likely = np.argmax(distribution)
            predicted_zones.append(most_likely)

        return predicted_zones


class SumAnalyzer:
    """å’Œå€¼åˆ†æå™¨ - å¿«ä¹8ç‰¹è‰²ç®—æ³•"""

    @staticmethod
    def analyze_sum_distribution(data: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æå’Œå€¼åˆ†å¸ƒ"""
        sums = []

        for _, row in data.iterrows():
            numbers = [row[f'num{i}'] for i in range(1, 21)]
            sums.append(sum(numbers))

        return {
            'mean': np.mean(sums),
            'std': np.std(sums),
            'min': min(sums),
            'max': max(sums),
            'median': np.median(sums),
            'distribution': np.histogram(sums, bins=20)[0].tolist()
        }

    @staticmethod
    def predict_sum_range(sum_stats: Dict[str, Any]) -> Tuple[int, int]:
        """é¢„æµ‹å’Œå€¼èŒƒå›´"""
        mean = sum_stats['mean']
        std = sum_stats['std']

        # é¢„æµ‹èŒƒå›´ï¼šå‡å€¼ Â± 1ä¸ªæ ‡å‡†å·®
        lower_bound = int(mean - std)
        upper_bound = int(mean + std)

        return lower_bound, upper_bound


class MarkovPredictor:
    """1é˜¶é©¬å°”å¯å¤«é“¾é¢„æµ‹å™¨ - åŸºäºçœŸå®å·ç è½¬ç§»"""

    def __init__(self, analyzer):
        self.analyzer = analyzer

    def predict(self, data: pd.DataFrame, count: int = 30, **kwargs) -> Tuple[List[int], List[float]]:
        """1é˜¶é©¬å°”å¯å¤«é“¾é¢„æµ‹"""
        print("æ‰§è¡Œ1é˜¶é©¬å°”å¯å¤«é“¾é¢„æµ‹...")

        # ç»Ÿè®¡æ¯ä¸ªå·ç çš„å‡ºç°é¢‘ç‡ä½œä¸ºåŸºç¡€æ¦‚ç‡
        number_frequencies = np.zeros(80)

        # ç»Ÿè®¡å·ç é¢‘ç‡
        for _, row in data.iterrows():
            numbers = [int(row[f'num{i}']) for i in range(1, 21)]
            for num in numbers:
                number_frequencies[num - 1] += 1

        # å½’ä¸€åŒ–é¢‘ç‡
        total_count = np.sum(number_frequencies)
        if total_count > 0:
            number_frequencies = number_frequencies / total_count
        else:
            number_frequencies = np.ones(80) / 80

        # æ„å»ºåŸºäºä½ç½®çš„è½¬ç§»æ¦‚ç‡
        position_transitions = np.zeros((20, 80))  # 20ä¸ªä½ç½®ï¼Œæ¯ä¸ªä½ç½®å¯¹80ä¸ªå·ç çš„æ¦‚ç‡

        for _, row in data.iterrows():
            numbers = [int(row[f'num{i}']) for i in range(1, 21)]
            for pos, num in enumerate(numbers):
                position_transitions[pos][num - 1] += 1

        # å½’ä¸€åŒ–ä½ç½®è½¬ç§»
        for pos in range(20):
            row_sum = np.sum(position_transitions[pos])
            if row_sum > 0:
                position_transitions[pos] /= row_sum
            else:
                position_transitions[pos] = number_frequencies

        # ç»“åˆé¢‘ç‡å’Œä½ç½®ä¿¡æ¯è®¡ç®—æœ€ç»ˆæ¦‚ç‡
        # ä½¿ç”¨åŠ æƒå¹³å‡ï¼š70%é¢‘ç‡ + 30%ä½ç½®ä¿¡æ¯
        final_probs = 0.7 * number_frequencies
        for pos in range(20):
            final_probs += 0.3 * position_transitions[pos] / 20

        next_probs = final_probs

        # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„å·ç 
        number_probs = [(i + 1, prob) for i, prob in enumerate(next_probs)]
        number_probs.sort(key=lambda x: x[1], reverse=True)

        predicted_numbers = [num for num, _ in number_probs[:count]]
        confidence_scores = [float(prob) for _, prob in number_probs[:count]]

        return predicted_numbers, confidence_scores


class Markov2ndPredictor:
    """2é˜¶é©¬å°”å¯å¤«é“¾é¢„æµ‹å™¨"""

    def __init__(self, analyzer):
        self.analyzer = analyzer

    def predict(self, data: pd.DataFrame, count: int = 30, **kwargs) -> Tuple[List[int], List[float]]:
        """2é˜¶é©¬å°”å¯å¤«é“¾é¢„æµ‹ - åŸºäºé¢‘ç‡å’Œä½ç½®çš„æ”¹è¿›é¢„æµ‹"""
        print(f"ğŸ”„ æ‰§è¡Œ2é˜¶é©¬å°”å¯å¤«é“¾é¢„æµ‹...")
        print(f"åˆ†ææ•°æ®: {len(data)}æœŸ")

        # ç»Ÿè®¡æ¯ä¸ªå·ç åœ¨ä¸åŒä½ç½®çš„å‡ºç°é¢‘ç‡
        position_frequencies = np.zeros((20, 80))  # 20ä¸ªä½ç½®ï¼Œ80ä¸ªå·ç 

        for _, row in data.iterrows():
            numbers = [int(row[f'num{i}']) for i in range(1, 21)]
            for pos, num in enumerate(numbers):
                position_frequencies[pos][num - 1] += 1

        # å½’ä¸€åŒ–ä½ç½®é¢‘ç‡
        for pos in range(20):
            total = np.sum(position_frequencies[pos])
            if total > 0:
                position_frequencies[pos] /= total
            else:
                position_frequencies[pos] = np.ones(80) / 80

        # ç»Ÿè®¡å·ç é—´çš„å…±ç°å…³ç³»
        cooccurrence_matrix = np.zeros((80, 80))

        for _, row in data.iterrows():
            numbers = [int(row[f'num{i}']) for i in range(1, 21)]
            for i in range(len(numbers)):
                for j in range(i + 1, len(numbers)):
                    num1, num2 = numbers[i] - 1, numbers[j] - 1
                    cooccurrence_matrix[num1][num2] += 1
                    cooccurrence_matrix[num2][num1] += 1

        # å½’ä¸€åŒ–å…±ç°çŸ©é˜µ
        for i in range(80):
            total = np.sum(cooccurrence_matrix[i])
            if total > 0:
                cooccurrence_matrix[i] /= total
            else:
                cooccurrence_matrix[i] = np.ones(80) / 80

        # è®¡ç®—ç»¼åˆæ¦‚ç‡ï¼šä½ç½®é¢‘ç‡ + å…±ç°å…³ç³»
        final_probs = np.zeros(80)

        # ä½ç½®é¢‘ç‡æƒé‡ (40%)
        for pos in range(20):
            final_probs += 0.4 * position_frequencies[pos] / 20

        # å…±ç°å…³ç³»æƒé‡ (60%)
        if len(data) > 0:
            recent_numbers = [int(data.iloc[0][f'num{i}']) for i in range(1, 21)]
            for num in recent_numbers:
                final_probs += 0.6 * cooccurrence_matrix[num - 1] / len(recent_numbers)
        else:
            final_probs += 0.6 * np.ones(80) / 80

        print(f"æ„å»ºäº† {len(data)} æœŸæ•°æ®çš„2é˜¶è½¬ç§»å…³ç³»")
        print(f"åˆå§‹çŠ¶æ€: åŸºäºæœ€è¿‘æœŸå·ç å…³ç³»")

        # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„å·ç 
        number_probs = [(i + 1, prob) for i, prob in enumerate(final_probs)]
        number_probs.sort(key=lambda x: x[1], reverse=True)

        predicted_numbers = [num for num, _ in number_probs[:count]]
        confidence_scores = [float(prob) for _, prob in number_probs[:count]]

        print(f"âœ… 2é˜¶é©¬å°”å¯å¤«é“¾é¢„æµ‹å®Œæˆ")
        print(f"é¢„æµ‹å·ç : {predicted_numbers[:10]}...")
        print(f"å¹³å‡ç½®ä¿¡åº¦: {np.mean(confidence_scores):.3f}")

        return predicted_numbers, confidence_scores


class Markov3rdPredictor:
    """3é˜¶é©¬å°”å¯å¤«é“¾é¢„æµ‹å™¨ - åŸºäºç‰¹å¾çŠ¶æ€è½¬ç§»"""

    def __init__(self, analyzer):
        self.analyzer = analyzer

    def predict(self, data: pd.DataFrame, count: int = 30, **kwargs) -> Tuple[List[int], List[float]]:
        """3é˜¶é©¬å°”å¯å¤«é“¾é¢„æµ‹ - åŸºäºç‰¹å¾è½¬ç§»è€Œéå…·ä½“å·ç è½¬ç§»"""
        print(f"ğŸ”„ æ‰§è¡Œ3é˜¶é©¬å°”å¯å¤«é“¾é¢„æµ‹ï¼ˆç‰¹å¾åŒ–çŠ¶æ€ç©ºé—´ï¼‰...")
        print(f"åˆ†ææ•°æ®: {len(data)}æœŸ")

        # æå–æ¯æœŸçš„ç‰¹å¾
        features_history = []
        for _, row in data.iterrows():
            numbers = [int(row[f'num{i}']) for i in range(1, 21)]
            features = self._extract_features(numbers)
            features_history.append(features)

        # æ„å»º3é˜¶ç‰¹å¾çŠ¶æ€è½¬ç§»
        transition_counts = {}
        state_counts = {}

        for i in range(3, len(features_history)):
            # å‰ä¸‰æœŸçš„ç‰¹å¾ä½œä¸ºçŠ¶æ€
            state1 = tuple(features_history[i-3])
            state2 = tuple(features_history[i-2])
            state3 = tuple(features_history[i-1])
            next_features = tuple(features_history[i])

            state_triple = (state1, state2, state3)

            if state_triple not in transition_counts:
                transition_counts[state_triple] = {}
                state_counts[state_triple] = 0

            if next_features not in transition_counts[state_triple]:
                transition_counts[state_triple][next_features] = 0

            transition_counts[state_triple][next_features] += 1
            state_counts[state_triple] += 1

        print(f"æ„å»ºäº† {len(transition_counts)} ä¸ª3é˜¶ç‰¹å¾çŠ¶æ€è½¬ç§»")

        # è·å–æœ€è¿‘ä¸‰æœŸçš„ç‰¹å¾ä½œä¸ºå½“å‰çŠ¶æ€
        if len(features_history) >= 3:
            current_state = (
                tuple(features_history[-3]),
                tuple(features_history[-2]),
                tuple(features_history[-1])
            )
        else:
            # æ•°æ®ä¸è¶³ï¼Œä½¿ç”¨é»˜è®¤ç‰¹å¾
            default_features = self._extract_features(list(range(1, 21)))
            current_state = (tuple(default_features),) * 3

        # é¢„æµ‹ä¸‹ä¸€æœŸç‰¹å¾
        predicted_features = self._predict_next_features(
            transition_counts, state_counts, current_state
        )

        # æ ¹æ®é¢„æµ‹ç‰¹å¾ç”Ÿæˆå·ç 
        predicted_numbers, confidence_scores = self._features_to_numbers(
            predicted_features, data, count
        )

        print(f"âœ… 3é˜¶é©¬å°”å¯å¤«é“¾é¢„æµ‹å®Œæˆ")
        print(f"é¢„æµ‹ç‰¹å¾: å’Œå€¼={predicted_features[0]:.1f}, å¥‡å¶æ¯”={predicted_features[1]:.2f}")
        print(f"é¢„æµ‹å·ç : {predicted_numbers[:10]}...")
        print(f"å¹³å‡ç½®ä¿¡åº¦: {np.mean(confidence_scores):.3f}")

        return predicted_numbers, confidence_scores

    def _extract_features(self, numbers: List[int]) -> List[float]:
        """æå–å·ç ç‰¹å¾"""
        # å’Œå€¼ç‰¹å¾
        sum_value = sum(numbers) / 20  # å½’ä¸€åŒ–

        # å¥‡å¶æ¯”ç‰¹å¾
        odd_count = sum(1 for num in numbers if num % 2 == 1)
        odd_ratio = odd_count / 20

        # å¤§å°æ¯”ç‰¹å¾ (>40ä¸ºå¤§å·)
        big_count = sum(1 for num in numbers if num > 40)
        big_ratio = big_count / 20

        # åŒºåŸŸåˆ†å¸ƒç‰¹å¾ (8ä¸ªåŒºåŸŸ)
        zone_counts = [0] * 8
        for num in numbers:
            zone_idx = (num - 1) // 10
            zone_counts[zone_idx] += 1
        zone_ratios = [count / 20 for count in zone_counts]

        return [sum_value, odd_ratio, big_ratio] + zone_ratios

    def _predict_next_features(self, transition_counts, state_counts, current_state):
        """é¢„æµ‹ä¸‹ä¸€æœŸç‰¹å¾"""
        alpha = 0.01  # æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘å‚æ•°

        if current_state in transition_counts:
            # æ‰¾åˆ°æœ€å¯èƒ½çš„ä¸‹ä¸€ç‰¹å¾çŠ¶æ€
            feature_probs = {}
            total_count = state_counts[current_state]

            for next_features, count in transition_counts[current_state].items():
                prob = (count + alpha) / (total_count + alpha * len(transition_counts[current_state]))
                feature_probs[next_features] = prob

            # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ç‰¹å¾
            best_features = max(feature_probs.items(), key=lambda x: x[1])[0]
            return list(best_features)
        else:
            # å¦‚æœæ²¡æœ‰åŒ¹é…çš„çŠ¶æ€ï¼Œä½¿ç”¨å†å²å¹³å‡ç‰¹å¾
            return self._get_average_features(transition_counts)

    def _get_average_features(self, transition_counts):
        """è·å–å†å²å¹³å‡ç‰¹å¾"""
        all_features = []
        for state_triple in transition_counts:
            for next_features in transition_counts[state_triple]:
                all_features.append(list(next_features))

        if all_features:
            avg_features = np.mean(all_features, axis=0)
            return avg_features.tolist()
        else:
            # é»˜è®¤ç‰¹å¾
            return [10.5, 0.5, 0.5] + [0.125] * 8

    def _features_to_numbers(self, predicted_features, data, count):
        """æ ¹æ®é¢„æµ‹ç‰¹å¾ç”Ÿæˆå·ç """
        target_sum = predicted_features[0] * 20
        target_odd_ratio = predicted_features[1]
        target_big_ratio = predicted_features[2]
        target_zone_ratios = predicted_features[3:11]

        # ä½¿ç”¨é—ä¼ ç®—æ³•æˆ–å¯å‘å¼æ–¹æ³•ç”Ÿæˆç¬¦åˆç‰¹å¾çš„å·ç ç»„åˆ
        best_combination = self._generate_combination_by_features(
            target_sum, target_odd_ratio, target_big_ratio, target_zone_ratios, count
        )

        # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºç‰¹å¾åŒ¹é…åº¦ï¼‰
        confidence_scores = self._calculate_feature_confidence(
            best_combination, predicted_features
        )

        return best_combination, confidence_scores

    def _generate_combination_by_features(self, target_sum, target_odd_ratio,
                                        target_big_ratio, target_zone_ratios, count):
        """åŸºäºç›®æ ‡ç‰¹å¾ç”Ÿæˆå·ç ç»„åˆ"""
        best_combination = []
        best_score = float('-inf')

        # å¤šæ¬¡éšæœºå°è¯•ï¼Œé€‰æ‹©æœ€ç¬¦åˆç‰¹å¾çš„ç»„åˆ
        for _ in range(1000):
            combination = np.random.choice(range(1, 81), size=count, replace=False).tolist()
            score = self._evaluate_combination(
                combination, target_sum, target_odd_ratio, target_big_ratio, target_zone_ratios
            )

            if score > best_score:
                best_score = score
                best_combination = combination.copy()

        return sorted(best_combination)

    def _evaluate_combination(self, combination, target_sum, target_odd_ratio,
                            target_big_ratio, target_zone_ratios):
        """è¯„ä¼°å·ç ç»„åˆä¸ç›®æ ‡ç‰¹å¾çš„åŒ¹é…åº¦"""
        # å’Œå€¼åŒ¹é…åº¦
        actual_sum = sum(combination)
        sum_score = 1.0 / (1.0 + abs(actual_sum - target_sum))

        # å¥‡å¶æ¯”åŒ¹é…åº¦
        actual_odd_ratio = sum(1 for num in combination if num % 2 == 1) / len(combination)
        odd_score = 1.0 / (1.0 + abs(actual_odd_ratio - target_odd_ratio))

        # å¤§å°æ¯”åŒ¹é…åº¦
        actual_big_ratio = sum(1 for num in combination if num > 40) / len(combination)
        big_score = 1.0 / (1.0 + abs(actual_big_ratio - target_big_ratio))

        # åŒºåŸŸåˆ†å¸ƒåŒ¹é…åº¦
        actual_zone_counts = [0] * 8
        for num in combination:
            zone_idx = (num - 1) // 10
            actual_zone_counts[zone_idx] += 1
        actual_zone_ratios = [count / len(combination) for count in actual_zone_counts]

        zone_score = 0
        for i in range(8):
            zone_score += 1.0 / (1.0 + abs(actual_zone_ratios[i] - target_zone_ratios[i]))
        zone_score /= 8

        # ç»¼åˆè¯„åˆ†
        return (sum_score + odd_score + big_score + zone_score) / 4

    def _calculate_feature_confidence(self, combination, predicted_features):
        """è®¡ç®—åŸºäºç‰¹å¾çš„ç½®ä¿¡åº¦"""
        confidence = self._evaluate_combination(
            combination,
            predicted_features[0] * 20,
            predicted_features[1],
            predicted_features[2],
            predicted_features[3:11]
        )

        # ä¸ºæ¯ä¸ªå·ç åˆ†é…ç›¸åŒçš„ç½®ä¿¡åº¦
        return [confidence] * len(combination)


class AdaptiveMarkovPredictor:
    """è‡ªé€‚åº”é©¬å°”å¯å¤«é“¾é¢„æµ‹å™¨ - 1-5é˜¶æ™ºèƒ½èåˆ"""

    def __init__(self, analyzer):
        self.analyzer = analyzer
        self.base_predictors = {
            1: MarkovPredictor(analyzer),
            2: Markov2ndPredictor(analyzer),
            3: Markov3rdPredictor(analyzer)
        }

    def predict(self, data: pd.DataFrame, count: int = 30, **kwargs) -> Tuple[List[int], List[float]]:
        """è‡ªé€‚åº”é©¬å°”å¯å¤«é“¾é¢„æµ‹ - å¤šé˜¶èåˆ"""
        print(f"ğŸ”„ æ‰§è¡Œè‡ªé€‚åº”é©¬å°”å¯å¤«é“¾é¢„æµ‹...")
        print(f"åˆ†ææ•°æ®: {len(data)}æœŸ")

        # åŠ¨æ€æƒé‡åˆ†é…
        weights = self._calculate_adaptive_weights(data)
        print(f"åŠ¨æ€æƒé‡: {weights}")

        # æ”¶é›†å„é˜¶é¢„æµ‹ç»“æœ
        all_predictions = {}
        all_confidences = {}

        for order, weight in weights.items():
            if weight > 0:
                try:
                    if order in self.base_predictors:
                        numbers, confidences = self.base_predictors[order].predict(data, count * 2)
                        all_predictions[order] = numbers
                        all_confidences[order] = confidences
                        print(f"{order}é˜¶é¢„æµ‹å®Œæˆ: {len(numbers)}ä¸ªå·ç ")
                except Exception as e:
                    print(f"âš ï¸ {order}é˜¶é¢„æµ‹å¤±è´¥: {e}")
                    weights[order] = 0

        # é‡æ–°å½’ä¸€åŒ–æƒé‡
        total_weight = sum(weights.values())
        if total_weight > 0:
            weights = {k: v/total_weight for k, v in weights.items()}

        # èåˆé¢„æµ‹ç»“æœ
        final_numbers, final_confidences = self._fuse_predictions(
            all_predictions, all_confidences, weights, count
        )

        print(f"âœ… è‡ªé€‚åº”é©¬å°”å¯å¤«é“¾é¢„æµ‹å®Œæˆ")
        print(f"èåˆäº† {len([w for w in weights.values() if w > 0])} ä¸ªé¢„æµ‹å™¨")
        print(f"é¢„æµ‹å·ç : {final_numbers[:10]}...")
        print(f"å¹³å‡ç½®ä¿¡åº¦: {np.mean(final_confidences):.3f}")

        return final_numbers, final_confidences

    def _calculate_adaptive_weights(self, data):
        """è®¡ç®—è‡ªé€‚åº”æƒé‡"""
        # åŸºç¡€æƒé‡åˆ†é…
        base_weights = {
            1: 0.25,  # 1é˜¶æƒé‡
            2: 0.50,  # 2é˜¶æƒé‡
            3: 0.25   # 3é˜¶æƒé‡
        }

        # åŸºäºæ•°æ®é‡è°ƒæ•´æƒé‡
        data_size = len(data)
        data_factor = min(1.0, data_size / 100)  # 100æœŸä»¥ä¸Šæ•°æ®æ‰èƒ½å……åˆ†å‘æŒ¥é«˜é˜¶ä¼˜åŠ¿

        # è°ƒæ•´æƒé‡
        adjusted_weights = {}
        for order, base_weight in base_weights.items():
            if order == 1:
                # 1é˜¶é©¬å°”å¯å¤«é“¾åœ¨æ•°æ®å°‘æ—¶æƒé‡æ›´é«˜
                adjusted_weights[order] = base_weight * (2.0 - data_factor)
            elif order == 2:
                # 2é˜¶åœ¨ä¸­ç­‰æ•°æ®é‡æ—¶æƒé‡æœ€é«˜
                adjusted_weights[order] = base_weight * (1.0 + data_factor * 0.5)
            else:
                # é«˜é˜¶åœ¨æ•°æ®å……è¶³æ—¶æƒé‡æ›´é«˜
                adjusted_weights[order] = base_weight * data_factor

        # å½’ä¸€åŒ–æƒé‡
        total_weight = sum(adjusted_weights.values())
        if total_weight > 0:
            adjusted_weights = {k: v/total_weight for k, v in adjusted_weights.items()}

        return adjusted_weights

    def _fuse_predictions(self, all_predictions, all_confidences, weights, count):
        """èåˆå¤šä¸ªé¢„æµ‹ç»“æœ"""
        # æ”¶é›†æ‰€æœ‰å€™é€‰å·ç åŠå…¶åŠ æƒç½®ä¿¡åº¦
        number_scores = {}

        for order, numbers in all_predictions.items():
            weight = weights.get(order, 0)
            confidences = all_confidences.get(order, [])

            for i, number in enumerate(numbers):
                confidence = confidences[i] if i < len(confidences) else 0.1
                weighted_score = confidence * weight

                if number not in number_scores:
                    number_scores[number] = 0
                number_scores[number] += weighted_score

        # æŒ‰åŠ æƒå¾—åˆ†æ’åº
        sorted_numbers = sorted(number_scores.items(), key=lambda x: x[1], reverse=True)

        # é€‰æ‹©å‰countä¸ªå·ç 
        final_numbers = [num for num, score in sorted_numbers[:count]]
        final_confidences = [score for num, score in sorted_numbers[:count]]

        # å½’ä¸€åŒ–ç½®ä¿¡åº¦
        if final_confidences:
            max_conf = max(final_confidences)
            if max_conf > 0:
                final_confidences = [conf / max_conf for conf in final_confidences]

        return final_numbers, final_confidences


class LSTMPredictor:
    """LSTMç¥ç»ç½‘ç»œé¢„æµ‹å™¨"""

    def __init__(self, analyzer):
        self.analyzer = analyzer
        self.model = None
        self.scaler = StandardScaler()
    
    def predict(self, data: pd.DataFrame, count: int = 30, **kwargs) -> Tuple[List[int], List[float]]:
        """LSTMé¢„æµ‹"""
        print("ğŸ”„ æ‰§è¡ŒLSTMç¥ç»ç½‘ç»œé¢„æµ‹...")
        print(f"åˆ†ææ•°æ®: {len(data)}æœŸ")

        try:
            if not TF_AVAILABLE:
                print("âš ï¸ TensorFlowæœªå®‰è£…ï¼Œä½¿ç”¨é¢‘ç‡åˆ†æä½œä¸ºåå¤‡")
                frequency_predictor = FrequencyPredictor(self.analyzer)
                return frequency_predictor.predict(data, count)

            # å‡†å¤‡è®­ç»ƒæ•°æ®
            X, y = self._prepare_training_data(data)

            if X.size == 0:
                print("âš ï¸ è®­ç»ƒæ•°æ®ä¸è¶³ï¼Œä½¿ç”¨é¢‘ç‡åˆ†æä½œä¸ºåå¤‡")
                frequency_predictor = FrequencyPredictor(self.analyzer)
                return frequency_predictor.predict(data, count)

            # æ„å»ºå’Œè®­ç»ƒæ¨¡å‹
            self.model = self._build_model(X.shape)
            self._train_model(X, y)

            # æ‰§è¡Œé¢„æµ‹
            predicted_numbers, confidence_scores = self._predict_numbers(X, count)

            print(f"âœ… LSTMé¢„æµ‹å®Œæˆ")
            print(f"é¢„æµ‹å·ç : {predicted_numbers[:10]}...")
            print(f"å¹³å‡ç½®ä¿¡åº¦: {np.mean(confidence_scores):.3f}")

            return predicted_numbers, confidence_scores

        except Exception as e:
            print(f"âš ï¸ LSTMé¢„æµ‹å¤±è´¥: {e}")
            frequency_predictor = FrequencyPredictor(self.analyzer)
            return frequency_predictor.predict(data, count)
    
    def _prepare_training_data(self, data: pd.DataFrame, sequence_length: int = 10):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        if len(data) < sequence_length + 1:
            return np.array([]), np.array([])
        
        features = []
        targets = []
        
        for i in range(len(data) - sequence_length):
            # è¾“å…¥åºåˆ—
            sequence_data = data.iloc[i:i+sequence_length]
            sequence_features = []
            
            for _, row in sequence_data.iterrows():
                numbers = [row[f'num{j}'] for j in range(1, 21)]
                feature_vector = self._extract_features(numbers)
                sequence_features.append(feature_vector)
            
            features.append(sequence_features)
            
            # ç›®æ ‡ï¼šä¸‹ä¸€æœŸçš„å·ç 
            next_row = data.iloc[i + sequence_length]
            next_numbers = [next_row[f'num{j}'] for j in range(1, 21)]
            targets.append(self._encode_target(next_numbers))
        
        X = np.array(features)
        y = np.array(targets)
        
        return X, y
    
    def _extract_features(self, numbers: List[int]) -> List[float]:
        """æå–ç‰¹å¾å‘é‡"""
        features = []
        
        # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
        features.extend([
            sum(numbers) / 20,  # å¹³å‡å€¼
            (max(numbers) - min(numbers)) / 80,  # å½’ä¸€åŒ–è·¨åº¦
            sum(1 for n in numbers if n % 2 == 1) / 20,  # å¥‡æ•°æ¯”ä¾‹
            sum(1 for n in numbers if n >= 41) / 20,  # å¤§å·æ¯”ä¾‹
        ])
        
        # åŒºåŸŸåˆ†å¸ƒç‰¹å¾
        zone_counts = [0] * 8
        for num in numbers:
            zone_idx = (num - 1) // 10
            zone_counts[zone_idx] += 1
        
        features.extend([count / 20 for count in zone_counts])
        
        # å·ç åˆ†å¸ƒç‰¹å¾ (ç®€åŒ–ä¸º10ä¸ªåŒºé—´)
        interval_counts = [0] * 10
        for num in numbers:
            interval_idx = min((num - 1) // 8, 9)
            interval_counts[interval_idx] += 1
        
        features.extend([count / 20 for count in interval_counts])
        
        return features
    
    def _encode_target(self, numbers: List[int]) -> List[float]:
        """ç¼–ç ç›®æ ‡"""
        target = [0.0] * 80
        for num in numbers:
            target[num - 1] = 1.0
        return target
    
    def _build_model(self, input_shape):
        """æ„å»ºLSTMæ¨¡å‹"""
        model = tf.keras.Sequential([
            tf.keras.layers.LSTM(64, return_sequences=True, input_shape=input_shape[1:]),
            tf.keras.layers.Dropout(0.2),
            tf.keras.layers.LSTM(32, return_sequences=False),
            tf.keras.layers.Dropout(0.2),
            tf.keras.layers.Dense(64, activation='relu'),
            tf.keras.layers.Dropout(0.3),
            tf.keras.layers.Dense(80, activation='sigmoid')  # 80ä¸ªå·ç çš„æ¦‚ç‡
        ])
        
        model.compile(
            optimizer='adam',
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    def _train_model(self, X, y):
        """è®­ç»ƒæ¨¡å‹"""
        print("å¼€å§‹è®­ç»ƒLSTMæ¨¡å‹...")
        
        # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯æ•°æ®
        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # æ—©åœå›è°ƒ
        early_stopping = tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True
        )
        
        # è®­ç»ƒæ¨¡å‹
        history = self.model.fit(
            X_train, y_train,
            epochs=100,
            batch_size=32,
            validation_data=(X_val, y_val),
            callbacks=[early_stopping],
            verbose=0
        )
        
        print(f"LSTMæ¨¡å‹è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆéªŒè¯æŸå¤±: {min(history.history['val_loss']):.4f}")
    
    def _predict_numbers(self, X, count: int) -> Tuple[List[int], List[float]]:
        """é¢„æµ‹å·ç """
        # ä½¿ç”¨æœ€åä¸€ä¸ªåºåˆ—è¿›è¡Œé¢„æµ‹
        if len(X) > 0:
            last_sequence = X[-1:]
        else:
            # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œåˆ›å»ºé›¶åºåˆ—
            last_sequence = np.zeros((1, 10, 22))
        
        # é¢„æµ‹æ¦‚ç‡
        probabilities = self.model.predict(last_sequence, verbose=0)[0]
        
        # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„å·ç 
        number_probs = [(i + 1, prob) for i, prob in enumerate(probabilities)]
        number_probs.sort(key=lambda x: x[1], reverse=True)
        
        predicted_numbers = [num for num, _ in number_probs[:count]]
        confidence_scores = [prob for _, prob in number_probs[:count]]
        
        return predicted_numbers, confidence_scores


class TransformerPredictor:
    """Transformeræ¨¡å‹é¢„æµ‹å™¨ - åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„åºåˆ—é¢„æµ‹"""

    def __init__(self, analyzer):
        self.analyzer = analyzer
        self.model = None
        self.scaler = StandardScaler()
        self.vocab_size = 81  # 1-80å·ç  + padding token
        self.d_model = 64
        self.num_heads = 8
        self.num_layers = 3
        self.max_seq_length = 20

    def predict(self, data: pd.DataFrame, count: int = 30, **kwargs) -> Tuple[List[int], List[float]]:
        """Transformeré¢„æµ‹"""
        print(f"ğŸ”„ æ‰§è¡ŒTransformeræ¨¡å‹é¢„æµ‹...")
        print(f"åˆ†ææ•°æ®: {len(data)}æœŸ")

        try:
            import torch
            import torch.nn as nn
            import torch.nn.functional as F
            from torch.utils.data import DataLoader, TensorDataset

            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®
            if len(data) < 10:
                print("âš ï¸ æ•°æ®ä¸è¶³ï¼Œä½¿ç”¨é¢‘ç‡åˆ†æä½œä¸ºåå¤‡")
                frequency_predictor = FrequencyPredictor(self.analyzer)
                return frequency_predictor.predict(data, count)

            # å‡†å¤‡è®­ç»ƒæ•°æ®
            sequences, targets = self._prepare_sequences(data)

            if len(sequences) == 0:
                print("âš ï¸ æ— æ³•æ„å»ºåºåˆ—ï¼Œä½¿ç”¨é¢‘ç‡åˆ†æä½œä¸ºåå¤‡")
                frequency_predictor = FrequencyPredictor(self.analyzer)
                return frequency_predictor.predict(data, count)

            # æ„å»ºå’Œè®­ç»ƒæ¨¡å‹
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            print(f"ä½¿ç”¨è®¾å¤‡: {device}")

            model = self._build_transformer_model().to(device)

            # è®­ç»ƒæ¨¡å‹
            self._train_model(model, sequences, targets, device)

            # é¢„æµ‹
            predicted_numbers, confidence_scores = self._predict_with_model(
                model, sequences, device, count
            )

            print(f"âœ… Transformeré¢„æµ‹å®Œæˆ")
            print(f"é¢„æµ‹å·ç : {predicted_numbers[:10]}...")
            print(f"å¹³å‡ç½®ä¿¡åº¦: {np.mean(confidence_scores):.3f}")

            return predicted_numbers, confidence_scores

        except ImportError:
            print("âš ï¸ PyTorchæœªå®‰è£…ï¼Œä½¿ç”¨é¢‘ç‡åˆ†æä½œä¸ºåå¤‡")
            frequency_predictor = FrequencyPredictor(self.analyzer)
            return frequency_predictor.predict(data, count)
        except Exception as e:
            print(f"âš ï¸ Transformeré¢„æµ‹å¤±è´¥: {e}")
            frequency_predictor = FrequencyPredictor(self.analyzer)
            return frequency_predictor.predict(data, count)

    def _prepare_sequences(self, data: pd.DataFrame):
        """å‡†å¤‡åºåˆ—æ•°æ®"""
        sequences = []
        targets = []

        # å°†æ¯æœŸå·ç è½¬æ¢ä¸ºåºåˆ—
        all_numbers = []
        for _, row in data.iterrows():
            numbers = [int(row[f'num{i}']) for i in range(1, 21)]
            all_numbers.append(numbers)

        # åˆ›å»ºæ»‘åŠ¨çª—å£åºåˆ—
        seq_length = 10  # ä½¿ç”¨å‰10æœŸé¢„æµ‹ä¸‹ä¸€æœŸ

        for i in range(len(all_numbers) - seq_length):
            # è¾“å…¥åºåˆ—ï¼šå‰seq_lengthæœŸçš„å·ç 
            input_seq = []
            for j in range(seq_length):
                input_seq.extend(all_numbers[i + j])

            # ç›®æ ‡ï¼šä¸‹ä¸€æœŸçš„å·ç 
            target = all_numbers[i + seq_length]

            sequences.append(input_seq)
            targets.append(target)

        return sequences, targets

    def _build_transformer_model(self):
        """æ„å»ºTransformeræ¨¡å‹"""
        import torch
        import torch.nn as nn

        class TransformerModel(nn.Module):
            def __init__(self, vocab_size, d_model, num_heads, num_layers, max_seq_length):
                super(TransformerModel, self).__init__()
                self.d_model = d_model
                self.embedding = nn.Embedding(vocab_size, d_model)
                self.pos_encoding = self._create_positional_encoding(max_seq_length, d_model)

                encoder_layer = nn.TransformerEncoderLayer(
                    d_model=d_model,
                    nhead=num_heads,
                    dim_feedforward=256,
                    dropout=0.1,
                    batch_first=True
                )
                self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

                self.output_projection = nn.Linear(d_model, 80)  # è¾“å‡º80ä¸ªå·ç çš„æ¦‚ç‡
                self.dropout = nn.Dropout(0.1)

            def _create_positional_encoding(self, max_len, d_model):
                import torch
                import math

                pe = torch.zeros(max_len, d_model)
                position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
                div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                                   (-math.log(10000.0) / d_model))

                pe[:, 0::2] = torch.sin(position * div_term)
                pe[:, 1::2] = torch.cos(position * div_term)
                return pe.unsqueeze(0)

            def forward(self, x):
                import math
                # x shape: (batch_size, seq_len)
                seq_len = x.size(1)

                # åµŒå…¥å’Œä½ç½®ç¼–ç 
                x = self.embedding(x) * math.sqrt(self.d_model)
                x = x + self.pos_encoding[:, :seq_len, :].to(x.device)
                x = self.dropout(x)

                # Transformerç¼–ç 
                x = self.transformer(x)

                # å…¨å±€å¹³å‡æ± åŒ–
                x = torch.mean(x, dim=1)

                # è¾“å‡ºæŠ•å½±
                x = self.output_projection(x)
                return torch.sigmoid(x)

        return TransformerModel(
            self.vocab_size, self.d_model, self.num_heads,
            self.num_layers, self.max_seq_length * 20
        )

    def _train_model(self, model, sequences, targets, device):
        """è®­ç»ƒTransformeræ¨¡å‹"""
        import torch
        import torch.nn as nn
        from torch.utils.data import DataLoader, TensorDataset

        print("å¼€å§‹è®­ç»ƒTransformeræ¨¡å‹...")

        # å‡†å¤‡æ•°æ®
        X = torch.tensor(sequences, dtype=torch.long).to(device)

        # å°†ç›®æ ‡è½¬æ¢ä¸ºå¤šæ ‡ç­¾æ ¼å¼
        y = torch.zeros(len(targets), 80).to(device)
        for i, target_numbers in enumerate(targets):
            for num in target_numbers:
                if 1 <= num <= 80:
                    y[i, num - 1] = 1.0

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataset = TensorDataset(X, y)
        dataloader = DataLoader(dataset, batch_size=16, shuffle=True)

        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        criterion = nn.BCELoss()

        # è®­ç»ƒå¾ªç¯
        model.train()
        num_epochs = 50

        for epoch in range(num_epochs):
            total_loss = 0
            for batch_x, batch_y in dataloader:
                optimizer.zero_grad()

                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)

                loss.backward()
                optimizer.step()

                total_loss += loss.item()

            if (epoch + 1) % 10 == 0:
                avg_loss = total_loss / len(dataloader)
                print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}")

        print("Transformeræ¨¡å‹è®­ç»ƒå®Œæˆ")

    def _predict_with_model(self, model, sequences, device, count):
        """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        import torch

        model.eval()

        # ä½¿ç”¨æœ€åä¸€ä¸ªåºåˆ—è¿›è¡Œé¢„æµ‹
        if len(sequences) > 0:
            last_seq = torch.tensor([sequences[-1]], dtype=torch.long).to(device)
        else:
            # åˆ›å»ºéšæœºåºåˆ—ä½œä¸ºåå¤‡
            last_seq = torch.randint(1, 81, (1, 200)).to(device)

        with torch.no_grad():
            probabilities = model(last_seq)[0].cpu().numpy()

        # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„å·ç 
        number_probs = [(i + 1, prob) for i, prob in enumerate(probabilities)]
        number_probs.sort(key=lambda x: x[1], reverse=True)

        predicted_numbers = [num for num, _ in number_probs[:count]]
        confidence_scores = [float(prob) for _, prob in number_probs[:count]]

        return predicted_numbers, confidence_scores


class GraphNeuralNetworkPredictor:
    """å›¾ç¥ç»ç½‘ç»œé¢„æµ‹å™¨ - åŸºäºå·ç å…³ç³»å›¾çš„é¢„æµ‹"""

    def __init__(self, analyzer):
        self.analyzer = analyzer
        self.model = None

    def predict(self, data: pd.DataFrame, count: int = 30, **kwargs) -> Tuple[List[int], List[float]]:
        """å›¾ç¥ç»ç½‘ç»œé¢„æµ‹"""
        print(f"ğŸ”„ æ‰§è¡Œå›¾ç¥ç»ç½‘ç»œé¢„æµ‹...")
        print(f"åˆ†ææ•°æ®: {len(data)}æœŸ")

        try:
            import torch
            import torch.nn as nn
            import torch.nn.functional as F

            # æ£€æŸ¥æ•°æ®é‡
            if len(data) < 20:
                print("âš ï¸ æ•°æ®ä¸è¶³ï¼Œä½¿ç”¨é¢‘ç‡åˆ†æä½œä¸ºåå¤‡")
                frequency_predictor = FrequencyPredictor(self.analyzer)
                return frequency_predictor.predict(data, count)

            # æ„å»ºå·ç å…³ç³»å›¾
            adjacency_matrix, node_features = self._build_number_graph(data)

            # æ„å»ºå’Œè®­ç»ƒGNNæ¨¡å‹
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            print(f"ä½¿ç”¨è®¾å¤‡: {device}")

            model = self._build_gnn_model().to(device)

            # è®­ç»ƒæ¨¡å‹
            self._train_gnn_model(model, adjacency_matrix, node_features, data, device)

            # é¢„æµ‹
            predicted_numbers, confidence_scores = self._predict_with_gnn(
                model, adjacency_matrix, node_features, device, count
            )

            print(f"âœ… å›¾ç¥ç»ç½‘ç»œé¢„æµ‹å®Œæˆ")
            print(f"é¢„æµ‹å·ç : {predicted_numbers[:10]}...")
            print(f"å¹³å‡ç½®ä¿¡åº¦: {np.mean(confidence_scores):.3f}")

            return predicted_numbers, confidence_scores

        except ImportError:
            print("âš ï¸ PyTorchæœªå®‰è£…ï¼Œä½¿ç”¨é¢‘ç‡åˆ†æä½œä¸ºåå¤‡")
            frequency_predictor = FrequencyPredictor(self.analyzer)
            return frequency_predictor.predict(data, count)
        except Exception as e:
            print(f"âš ï¸ å›¾ç¥ç»ç½‘ç»œé¢„æµ‹å¤±è´¥: {e}")
            frequency_predictor = FrequencyPredictor(self.analyzer)
            return frequency_predictor.predict(data, count)

    def _build_number_graph(self, data: pd.DataFrame):
        """æ„å»ºå·ç å…³ç³»å›¾"""
        print("æ„å»ºå·ç å…³ç³»å›¾...")

        # åˆå§‹åŒ–é‚»æ¥çŸ©é˜µ (80x80)
        adjacency_matrix = np.zeros((80, 80))

        # ç»Ÿè®¡å·ç å…±ç°é¢‘ç‡
        for _, row in data.iterrows():
            numbers = [int(row[f'num{i}']) for i in range(1, 21)]

            # è®¡ç®—å·ç é—´çš„å…±ç°å…³ç³»
            for i in range(len(numbers)):
                for j in range(i + 1, len(numbers)):
                    num1, num2 = numbers[i] - 1, numbers[j] - 1  # è½¬æ¢ä¸º0-79ç´¢å¼•
                    adjacency_matrix[num1][num2] += 1
                    adjacency_matrix[num2][num1] += 1  # æ— å‘å›¾

        # å½’ä¸€åŒ–é‚»æ¥çŸ©é˜µ
        max_weight = np.max(adjacency_matrix)
        if max_weight > 0:
            adjacency_matrix = adjacency_matrix / max_weight

        # æ„å»ºèŠ‚ç‚¹ç‰¹å¾ (æ¯ä¸ªå·ç çš„ç»Ÿè®¡ç‰¹å¾)
        node_features = self._build_node_features(data)

        print(f"å›¾æ„å»ºå®Œæˆ: 80ä¸ªèŠ‚ç‚¹, {np.sum(adjacency_matrix > 0)//2}æ¡è¾¹")

        return adjacency_matrix, node_features

    def _build_node_features(self, data: pd.DataFrame):
        """æ„å»ºèŠ‚ç‚¹ç‰¹å¾"""
        node_features = np.zeros((80, 5))  # 5ç»´ç‰¹å¾

        # ç»Ÿè®¡æ¯ä¸ªå·ç çš„ç‰¹å¾
        for num in range(1, 81):
            # ç‰¹å¾1: å‡ºç°é¢‘ç‡
            frequency = 0
            # ç‰¹å¾2: æœ€è¿‘å‡ºç°ä½ç½®çš„å¹³å‡å€¼
            recent_positions = []
            # ç‰¹å¾3: ä¸å…¶ä»–å·ç çš„å¹³å‡å…±ç°åº¦
            cooccurrence = 0
            # ç‰¹å¾4: å¥‡å¶æ€§ (0=å¶æ•°, 1=å¥‡æ•°)
            parity = num % 2
            # ç‰¹å¾5: å¤§å° (å½’ä¸€åŒ–åˆ°0-1)
            size = (num - 1) / 79

            for idx, row in data.iterrows():
                numbers = [int(row[f'num{i}']) for i in range(1, 21)]
                if num in numbers:
                    frequency += 1
                    recent_positions.append(numbers.index(num))
                    # è®¡ç®—ä¸å…¶ä»–å·ç çš„å…±ç°
                    cooccurrence += len([n for n in numbers if n != num])

            # å½’ä¸€åŒ–ç‰¹å¾
            frequency = frequency / len(data) if len(data) > 0 else 0
            avg_position = np.mean(recent_positions) / 19 if recent_positions else 0.5
            cooccurrence = cooccurrence / (len(data) * 19) if len(data) > 0 else 0

            node_features[num - 1] = [frequency, avg_position, cooccurrence, parity, size]

        return node_features

    def _build_gnn_model(self):
        """æ„å»ºå›¾ç¥ç»ç½‘ç»œæ¨¡å‹"""
        import torch
        import torch.nn as nn
        import torch.nn.functional as F

        class GraphConvLayer(nn.Module):
            def __init__(self, in_features, out_features):
                super(GraphConvLayer, self).__init__()
                self.linear = nn.Linear(in_features, out_features)

            def forward(self, x, adj):
                # x: (num_nodes, in_features)
                # adj: (num_nodes, num_nodes)
                support = self.linear(x)
                output = torch.mm(adj, support)
                return F.relu(output)

        class GNNModel(nn.Module):
            def __init__(self, input_dim, hidden_dim, output_dim):
                super(GNNModel, self).__init__()
                self.gc1 = GraphConvLayer(input_dim, hidden_dim)
                self.gc2 = GraphConvLayer(hidden_dim, hidden_dim)
                self.gc3 = GraphConvLayer(hidden_dim, output_dim)
                self.dropout = nn.Dropout(0.2)

            def forward(self, x, adj):
                x = self.gc1(x, adj)
                x = self.dropout(x)
                x = self.gc2(x, adj)
                x = self.dropout(x)
                x = self.gc3(x, adj)
                return torch.sigmoid(x)

        return GNNModel(input_dim=5, hidden_dim=32, output_dim=1)

    def _train_gnn_model(self, model, adjacency_matrix, node_features, data, device):
        """è®­ç»ƒGNNæ¨¡å‹"""
        import torch
        import torch.nn as nn

        print("å¼€å§‹è®­ç»ƒå›¾ç¥ç»ç½‘ç»œæ¨¡å‹...")

        # è½¬æ¢ä¸ºå¼ é‡
        adj_tensor = torch.FloatTensor(adjacency_matrix).to(device)
        features_tensor = torch.FloatTensor(node_features).to(device)

        # æ„å»ºè®­ç»ƒç›®æ ‡ (æ¯æœŸå‡ºç°çš„å·ç ä¸ºæ­£æ ·æœ¬)
        targets = torch.zeros(80, len(data)).to(device)
        for idx, (_, row) in enumerate(data.iterrows()):
            numbers = [int(row[f'num{i}']) for i in range(1, 21)]
            for num in numbers:
                targets[num - 1, idx] = 1.0

        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
        criterion = nn.BCELoss()

        # è®­ç»ƒå¾ªç¯
        model.train()
        num_epochs = 100

        for epoch in range(num_epochs):
            optimizer.zero_grad()

            # å‰å‘ä¼ æ’­
            outputs = model(features_tensor, adj_tensor).squeeze()  # (80,)

            # è®¡ç®—å¹³å‡æŸå¤± (å¯¹æ‰€æœ‰æœŸçš„å¹³å‡)
            total_loss = 0
            for period_idx in range(targets.shape[1]):
                period_targets = targets[:, period_idx]
                loss = criterion(outputs, period_targets)
                total_loss += loss

            avg_loss = total_loss / targets.shape[1]
            avg_loss.backward()
            optimizer.step()

            if (epoch + 1) % 20 == 0:
                print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss.item():.4f}")

        print("å›¾ç¥ç»ç½‘ç»œæ¨¡å‹è®­ç»ƒå®Œæˆ")

    def _predict_with_gnn(self, model, adjacency_matrix, node_features, device, count):
        """ä½¿ç”¨GNNæ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        import torch

        model.eval()

        # è½¬æ¢ä¸ºå¼ é‡
        adj_tensor = torch.FloatTensor(adjacency_matrix).to(device)
        features_tensor = torch.FloatTensor(node_features).to(device)

        with torch.no_grad():
            output = model(features_tensor, adj_tensor)
            probabilities = output.squeeze().cpu().numpy()

            # ç¡®ä¿æ¦‚ç‡æ•°ç»„æœ‰80ä¸ªå…ƒç´ 
            if len(probabilities.shape) == 0:
                # å¦‚æœæ˜¯æ ‡é‡ï¼Œåˆ›å»ºéšæœºæ¦‚ç‡
                probabilities = np.random.random(80)
            elif len(probabilities) != 80:
                # å¦‚æœé•¿åº¦ä¸å¯¹ï¼Œä½¿ç”¨èŠ‚ç‚¹ç‰¹å¾çš„åŠ æƒå’Œä½œä¸ºæ¦‚ç‡
                probabilities = np.random.random(80)
                for i in range(min(len(probabilities), 80)):
                    # åŸºäºèŠ‚ç‚¹ç‰¹å¾è®¡ç®—æ¦‚ç‡
                    feature_sum = np.sum(node_features[i])
                    probabilities[i] = feature_sum / (1 + feature_sum)

        # æ·»åŠ éšæœºæ‰°åŠ¨é¿å…å®Œå…¨ç›¸åŒçš„æ¦‚ç‡
        probabilities += np.random.normal(0, 0.01, 80)
        probabilities = np.abs(probabilities)  # ç¡®ä¿éè´Ÿ

        # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„å·ç 
        number_probs = [(i + 1, prob) for i, prob in enumerate(probabilities)]
        number_probs.sort(key=lambda x: x[1], reverse=True)

        predicted_numbers = [num for num, _ in number_probs[:count]]
        confidence_scores = [float(prob) for _, prob in number_probs[:count]]

        return predicted_numbers, confidence_scores


class MonteCarloPredictor:
    """è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿé¢„æµ‹å™¨ - åŸºäºéšæœºé‡‡æ ·çš„æ¦‚ç‡é¢„æµ‹"""

    def __init__(self, analyzer):
        self.analyzer = analyzer
        self.num_simulations = 50000  # å¤§è§„æ¨¡éšæœºé‡‡æ ·

    def predict(self, data: pd.DataFrame, count: int = 30, **kwargs) -> Tuple[List[int], List[float]]:
        """è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿé¢„æµ‹"""
        print(f"ğŸ”„ æ‰§è¡Œè’™ç‰¹å¡æ´›æ¨¡æ‹Ÿé¢„æµ‹...")
        print(f"åˆ†ææ•°æ®: {len(data)}æœŸï¼Œæ¨¡æ‹Ÿæ¬¡æ•°: {self.num_simulations}")

        # åˆ†æå†å²æ•°æ®çš„ç»Ÿè®¡ç‰¹å¾
        historical_stats = self._analyze_historical_patterns(data)

        # æ‰§è¡Œè’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ
        simulation_results = self._run_monte_carlo_simulation(historical_stats)

        # ç»Ÿè®¡æ¨¡æ‹Ÿç»“æœ
        number_frequencies = self._analyze_simulation_results(simulation_results)

        # é€‰æ‹©æœ€ä¼˜å·ç 
        predicted_numbers, confidence_scores = self._select_optimal_numbers(
            number_frequencies, count
        )

        print(f"âœ… è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿé¢„æµ‹å®Œæˆ")
        print(f"é¢„æµ‹å·ç : {predicted_numbers[:10]}...")
        print(f"å¹³å‡ç½®ä¿¡åº¦: {np.mean(confidence_scores):.3f}")

        return predicted_numbers, confidence_scores

    def _analyze_historical_patterns(self, data: pd.DataFrame):
        """åˆ†æå†å²æ•°æ®æ¨¡å¼"""
        patterns = {
            'number_frequencies': np.zeros(80),
            'sum_distribution': [],
            'odd_even_ratios': [],
            'zone_distributions': [],
            'consecutive_patterns': []
        }

        for _, row in data.iterrows():
            numbers = [int(row[f'num{i}']) for i in range(1, 21)]

            # å·ç é¢‘ç‡
            for num in numbers:
                patterns['number_frequencies'][num - 1] += 1

            # å’Œå€¼åˆ†å¸ƒ
            patterns['sum_distribution'].append(sum(numbers))

            # å¥‡å¶æ¯”
            odd_count = sum(1 for num in numbers if num % 2 == 1)
            patterns['odd_even_ratios'].append(odd_count / 20)

            # åŒºåŸŸåˆ†å¸ƒ
            zone_counts = [0] * 8
            for num in numbers:
                zone_idx = (num - 1) // 10
                zone_counts[zone_idx] += 1
            patterns['zone_distributions'].append(zone_counts)

            # è¿ç»­å·ç æ¨¡å¼
            consecutive_count = self._count_consecutive_numbers(numbers)
            patterns['consecutive_patterns'].append(consecutive_count)

        # å½’ä¸€åŒ–é¢‘ç‡
        patterns['number_frequencies'] = patterns['number_frequencies'] / len(data)

        return patterns

    def _count_consecutive_numbers(self, numbers):
        """ç»Ÿè®¡è¿ç»­å·ç æ•°é‡"""
        sorted_numbers = sorted(numbers)
        consecutive_count = 0

        for i in range(len(sorted_numbers) - 1):
            if sorted_numbers[i + 1] - sorted_numbers[i] == 1:
                consecutive_count += 1

        return consecutive_count

    def _run_monte_carlo_simulation(self, historical_stats):
        """æ‰§è¡Œè’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ"""
        print("å¼€å§‹è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿ...")

        simulation_results = []

        # ä½¿ç”¨å¤šè¿›ç¨‹åŠ é€Ÿæ¨¡æ‹Ÿ
        import multiprocessing as mp
        from functools import partial

        # åˆ†æ‰¹å¤„ç†
        batch_size = self.num_simulations // mp.cpu_count()

        with mp.Pool() as pool:
            simulate_batch = partial(self._simulate_batch, historical_stats)
            batch_results = pool.map(simulate_batch, [batch_size] * mp.cpu_count())

        # åˆå¹¶ç»“æœ
        for batch_result in batch_results:
            simulation_results.extend(batch_result)

        print(f"æ¨¡æ‹Ÿå®Œæˆï¼Œç”Ÿæˆ {len(simulation_results)} ä¸ªæ ·æœ¬")

        return simulation_results

    def _simulate_batch(self, historical_stats, batch_size):
        """æ¨¡æ‹Ÿä¸€æ‰¹æ ·æœ¬"""
        batch_results = []

        for _ in range(batch_size):
            # åŸºäºå†å²ç»Ÿè®¡ç”Ÿæˆä¸€ç»„å·ç 
            simulated_numbers = self._generate_constrained_numbers(historical_stats)
            batch_results.append(simulated_numbers)

        return batch_results

    def _analyze_simulation_results(self, simulation_results):
        """åˆ†ææ¨¡æ‹Ÿç»“æœ"""
        number_frequencies = np.zeros(80)

        for numbers in simulation_results:
            for num in numbers:
                number_frequencies[num - 1] += 1

        # å½’ä¸€åŒ–é¢‘ç‡
        number_frequencies = number_frequencies / len(simulation_results)

        return number_frequencies

    def _select_optimal_numbers(self, number_frequencies, count):
        """é€‰æ‹©æœ€ä¼˜å·ç """
        # æŒ‰é¢‘ç‡æ’åº
        number_probs = [(i + 1, freq) for i, freq in enumerate(number_frequencies)]
        number_probs.sort(key=lambda x: x[1], reverse=True)

        predicted_numbers = [num for num, _ in number_probs[:count]]
        confidence_scores = [float(freq) for _, freq in number_probs[:count]]

        # å½’ä¸€åŒ–ç½®ä¿¡åº¦
        if confidence_scores:
            max_conf = max(confidence_scores)
            if max_conf > 0:
                confidence_scores = [conf / max_conf for conf in confidence_scores]

        return predicted_numbers, confidence_scores

    def _generate_constrained_numbers(self, historical_stats):
        """åŸºäºçº¦æŸæ¡ä»¶ç”Ÿæˆå·ç """
        max_attempts = 1000

        for _ in range(max_attempts):
            # åŸºäºé¢‘ç‡æƒé‡éšæœºé€‰æ‹©å·ç 
            weights = historical_stats['number_frequencies']
            weights = weights + 0.01  # é¿å…é›¶æƒé‡
            weights = weights / np.sum(weights)

            # éšæœºé€‰æ‹©20ä¸ªä¸é‡å¤å·ç 
            numbers = np.random.choice(
                range(1, 81), size=20, replace=False, p=weights
            ).tolist()

            # éªŒè¯çº¦æŸæ¡ä»¶
            if self._validate_constraints(numbers, historical_stats):
                return sorted(numbers)

        # å¦‚æœæ— æ³•æ»¡è¶³çº¦æŸï¼Œè¿”å›åŸºäºé¢‘ç‡çš„éšæœºé€‰æ‹©
        return sorted(np.random.choice(range(1, 81), size=20, replace=False).tolist())

    def _validate_constraints(self, numbers, historical_stats):
        """éªŒè¯å·ç ç»„åˆæ˜¯å¦æ»¡è¶³å†å²æ¨¡å¼çº¦æŸ"""
        # å’Œå€¼çº¦æŸ
        sum_value = sum(numbers)
        sum_mean = np.mean(historical_stats['sum_distribution'])
        sum_std = np.std(historical_stats['sum_distribution'])
        if abs(sum_value - sum_mean) > 2 * sum_std:
            return False

        # å¥‡å¶æ¯”çº¦æŸ
        odd_count = sum(1 for num in numbers if num % 2 == 1)
        odd_ratio = odd_count / 20
        odd_mean = np.mean(historical_stats['odd_even_ratios'])
        if abs(odd_ratio - odd_mean) > 0.3:
            return False

        # åŒºåŸŸåˆ†å¸ƒçº¦æŸ
        zone_counts = [0] * 8
        for num in numbers:
            zone_idx = (num - 1) // 10
            zone_counts[zone_idx] += 1

        # æ£€æŸ¥æ˜¯å¦æœ‰åŒºåŸŸå®Œå…¨ä¸ºç©ºï¼ˆä¸å¤ªç°å®ï¼‰
        if zone_counts.count(0) > 3:
            return False

        return True


class ClusteringPredictor:
    """èšç±»åˆ†æé¢„æµ‹å™¨ - åŸºäºæ•°æ®èšç±»çš„æ¨¡å¼è¯†åˆ«é¢„æµ‹"""

    def __init__(self, analyzer):
        self.analyzer = analyzer

    def predict(self, data: pd.DataFrame, count: int = 30, **kwargs) -> Tuple[List[int], List[float]]:
        """èšç±»åˆ†æé¢„æµ‹"""
        print(f"ğŸ”„ æ‰§è¡Œèšç±»åˆ†æé¢„æµ‹...")
        print(f"åˆ†ææ•°æ®: {len(data)}æœŸ")

        try:
            from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
            from sklearn.metrics import silhouette_score
            from sklearn.preprocessing import StandardScaler

            # ç‰¹å¾æå–
            features = self._extract_clustering_features(data)

            if len(features) < 10:
                print("âš ï¸ æ•°æ®ä¸è¶³ï¼Œä½¿ç”¨é¢‘ç‡åˆ†æä½œä¸ºåå¤‡")
                frequency_predictor = FrequencyPredictor(self.analyzer)
                return frequency_predictor.predict(data, count)

            # ç‰¹å¾æ ‡å‡†åŒ–
            scaler = StandardScaler()
            features_scaled = scaler.fit_transform(features)

            # å¤šç®—æ³•èšç±»èåˆ
            clustering_results = self._multi_algorithm_clustering(features_scaled)

            # èšç±»ä¸­å¿ƒé¢„æµ‹
            predicted_numbers, confidence_scores = self._predict_from_clusters(
                clustering_results, features, data, count
            )

            print(f"âœ… èšç±»åˆ†æé¢„æµ‹å®Œæˆ")
            print(f"é¢„æµ‹å·ç : {predicted_numbers[:10]}...")
            print(f"å¹³å‡ç½®ä¿¡åº¦: {np.mean(confidence_scores):.3f}")

            return predicted_numbers, confidence_scores

        except ImportError:
            print("âš ï¸ scikit-learnåŠŸèƒ½ä¸å®Œæ•´ï¼Œä½¿ç”¨é¢‘ç‡åˆ†æä½œä¸ºåå¤‡")
            frequency_predictor = FrequencyPredictor(self.analyzer)
            return frequency_predictor.predict(data, count)
        except Exception as e:
            print(f"âš ï¸ èšç±»åˆ†æå¤±è´¥: {e}")
            frequency_predictor = FrequencyPredictor(self.analyzer)
            return frequency_predictor.predict(data, count)

    def _extract_clustering_features(self, data: pd.DataFrame):
        """æå–èšç±»ç‰¹å¾"""
        features = []

        for _, row in data.iterrows():
            numbers = [int(row[f'num{i}']) for i in range(1, 21)]

            # å¤šç»´ç‰¹å¾æå–
            feature_vector = []

            # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
            feature_vector.extend([
                sum(numbers) / 20,  # å¹³å‡å€¼
                np.std(numbers),    # æ ‡å‡†å·®
                min(numbers),       # æœ€å°å€¼
                max(numbers),       # æœ€å¤§å€¼
                max(numbers) - min(numbers)  # èŒƒå›´
            ])

            # å¥‡å¶ç‰¹å¾
            odd_count = sum(1 for num in numbers if num % 2 == 1)
            feature_vector.extend([
                odd_count / 20,     # å¥‡æ•°æ¯”ä¾‹
                (20 - odd_count) / 20  # å¶æ•°æ¯”ä¾‹
            ])

            # å¤§å°ç‰¹å¾
            big_count = sum(1 for num in numbers if num > 40)
            feature_vector.extend([
                big_count / 20,     # å¤§å·æ¯”ä¾‹
                (20 - big_count) / 20  # å°å·æ¯”ä¾‹
            ])

            # åŒºåŸŸåˆ†å¸ƒç‰¹å¾
            zone_counts = [0] * 8
            for num in numbers:
                zone_idx = (num - 1) // 10
                zone_counts[zone_idx] += 1
            feature_vector.extend([count / 20 for count in zone_counts])

            # è¿ç»­æ€§ç‰¹å¾
            sorted_numbers = sorted(numbers)
            consecutive_pairs = sum(1 for i in range(len(sorted_numbers) - 1)
                                  if sorted_numbers[i + 1] - sorted_numbers[i] == 1)
            feature_vector.append(consecutive_pairs / 19)

            # é—´éš”ç‰¹å¾
            gaps = [sorted_numbers[i + 1] - sorted_numbers[i]
                   for i in range(len(sorted_numbers) - 1)]
            feature_vector.extend([
                np.mean(gaps),      # å¹³å‡é—´éš”
                np.std(gaps),       # é—´éš”æ ‡å‡†å·®
                max(gaps)           # æœ€å¤§é—´éš”
            ])

            features.append(feature_vector)

        return np.array(features)

    def _multi_algorithm_clustering(self, features_scaled):
        """å¤šç®—æ³•èšç±»èåˆ"""
        from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
        from sklearn.metrics import silhouette_score

        clustering_results = {}

        # K-meansèšç±»
        best_kmeans_score = -1
        best_kmeans_k = 2

        for k in range(2, min(8, len(features_scaled) // 2)):
            try:
                kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                labels = kmeans.fit_predict(features_scaled)

                if len(set(labels)) > 1:  # ç¡®ä¿æœ‰å¤šä¸ªèšç±»
                    score = silhouette_score(features_scaled, labels)
                    if score > best_kmeans_score:
                        best_kmeans_score = score
                        best_kmeans_k = k
            except:
                continue

        # ä½¿ç”¨æœ€ä½³Kå€¼è¿›è¡ŒK-meansèšç±»
        kmeans = KMeans(n_clusters=best_kmeans_k, random_state=42, n_init=10)
        clustering_results['kmeans'] = {
            'labels': kmeans.fit_predict(features_scaled),
            'centers': kmeans.cluster_centers_,
            'score': best_kmeans_score
        }

        print(f"èšç±»ç®—æ³•ç»“æœ: K-means (K={best_kmeans_k}, è½®å»“ç³»æ•°={best_kmeans_score:.3f})")

        return clustering_results

    def _predict_from_clusters(self, clustering_results, features, data, count):
        """åŸºäºèšç±»ç»“æœè¿›è¡Œé¢„æµ‹"""
        if not clustering_results:
            # å¦‚æœèšç±»å¤±è´¥ï¼Œä½¿ç”¨é¢‘ç‡åˆ†æ
            frequency_predictor = FrequencyPredictor(self.analyzer)
            return frequency_predictor.predict(data, count)

        # é€‰æ‹©æœ€ä½³èšç±»ç»“æœ
        best_clustering = max(clustering_results.items(),
                            key=lambda x: x[1]['score'])

        algorithm_name, result = best_clustering
        labels = result['labels']

        print(f"ä½¿ç”¨æœ€ä½³èšç±»ç®—æ³•: {algorithm_name} (è½®å»“ç³»æ•°: {result['score']:.3f})")

        # æ‰¾åˆ°æœ€è¿‘çš„èšç±»ä¸­å¿ƒ
        if 'centers' in result:
            # K-meansæœ‰èšç±»ä¸­å¿ƒ
            last_feature = features[-1]  # æœ€è¿‘ä¸€æœŸçš„ç‰¹å¾

            # è®¡ç®—åˆ°å„èšç±»ä¸­å¿ƒçš„è·ç¦»
            centers = result['centers']
            distances = [np.linalg.norm(last_feature - center) for center in centers]
            closest_cluster = np.argmin(distances)

            # æ‰¾åˆ°å±äºè¯¥èšç±»çš„æ‰€æœ‰æ ·æœ¬
            cluster_indices = [i for i, label in enumerate(labels) if label == closest_cluster]
        else:
            # å…¶ä»–ç®—æ³•ï¼Œæ‰¾åˆ°æœ€è¿‘æ ·æœ¬æ‰€å±çš„èšç±»
            last_feature = features[-1]
            distances = [np.linalg.norm(last_feature - features[i]) for i in range(len(features))]
            closest_sample_idx = np.argmin(distances)
            target_cluster = labels[closest_sample_idx]

            cluster_indices = [i for i, label in enumerate(labels) if label == target_cluster]

        # åŸºäºèšç±»æ ·æœ¬ç”Ÿæˆé¢„æµ‹
        predicted_numbers, confidence_scores = self._generate_cluster_prediction(
            cluster_indices, data, count
        )

        return predicted_numbers, confidence_scores

    def _generate_cluster_prediction(self, cluster_indices, data, count):
        """åŸºäºèšç±»æ ·æœ¬ç”Ÿæˆé¢„æµ‹"""
        # ç»Ÿè®¡èšç±»ä¸­å·ç çš„å‡ºç°é¢‘ç‡
        number_frequencies = np.zeros(80)

        for idx in cluster_indices:
            if idx < len(data):
                row = data.iloc[idx]
                numbers = [int(row[f'num{i}']) for i in range(1, 21)]
                for num in numbers:
                    number_frequencies[num - 1] += 1

        # å½’ä¸€åŒ–é¢‘ç‡
        if np.sum(number_frequencies) > 0:
            number_frequencies = number_frequencies / np.sum(number_frequencies) * 20

        # é€‰æ‹©é¢‘ç‡æœ€é«˜çš„å·ç 
        number_probs = [(i + 1, freq) for i, freq in enumerate(number_frequencies)]
        number_probs.sort(key=lambda x: x[1], reverse=True)

        predicted_numbers = [num for num, _ in number_probs[:count]]
        confidence_scores = [float(freq) for _, freq in number_probs[:count]]

        # å½’ä¸€åŒ–ç½®ä¿¡åº¦
        if confidence_scores:
            max_conf = max(confidence_scores) if max(confidence_scores) > 0 else 1
            confidence_scores = [conf / max_conf for conf in confidence_scores]

        return predicted_numbers, confidence_scores


class AdvancedEnsemblePredictor:
    """è‡ªé€‚åº”é›†æˆå­¦ä¹ é¢„æµ‹å™¨ - 2000è½®é›†æˆè®­ç»ƒ"""

    def __init__(self, analyzer):
        self.analyzer = analyzer
        self.num_rounds = 2000

    def predict(self, data: pd.DataFrame, count: int = 30, **kwargs) -> Tuple[List[int], List[float]]:
        """è‡ªé€‚åº”é›†æˆå­¦ä¹ é¢„æµ‹"""
        print(f"ğŸ”„ æ‰§è¡Œè‡ªé€‚åº”é›†æˆå­¦ä¹ é¢„æµ‹...")
        print(f"åˆ†ææ•°æ®: {len(data)}æœŸï¼Œé›†æˆè½®æ•°: {self.num_rounds}")

        try:
            from sklearn.ensemble import RandomForestClassifier
            from sklearn.svm import SVC
            from sklearn.model_selection import cross_val_score

            # å‡†å¤‡è®­ç»ƒæ•°æ®
            X, y = self._prepare_ensemble_data(data)

            if len(X) < 20:
                print("âš ï¸ æ•°æ®ä¸è¶³ï¼Œä½¿ç”¨é¢‘ç‡åˆ†æä½œä¸ºåå¤‡")
                frequency_predictor = FrequencyPredictor(self.analyzer)
                return frequency_predictor.predict(data, count)

            # å¤šæ¨¡å‹èåˆè®­ç»ƒ
            ensemble_results = self._train_ensemble_models(X, y)

            # è‡ªé€‚åº”æƒé‡æ›´æ–°
            model_weights = self._calculate_adaptive_weights(ensemble_results, X, y)

            # é›†æˆé¢„æµ‹
            predicted_numbers, confidence_scores = self._ensemble_predict(
                ensemble_results, model_weights, X, count
            )

            print(f"âœ… è‡ªé€‚åº”é›†æˆå­¦ä¹ é¢„æµ‹å®Œæˆ")
            print(f"é¢„æµ‹å·ç : {predicted_numbers[:10]}...")
            print(f"å¹³å‡ç½®ä¿¡åº¦: {np.mean(confidence_scores):.3f}")

            return predicted_numbers, confidence_scores

        except ImportError:
            print("âš ï¸ scikit-learnåŠŸèƒ½ä¸å®Œæ•´ï¼Œä½¿ç”¨é¢‘ç‡åˆ†æä½œä¸ºåå¤‡")
            frequency_predictor = FrequencyPredictor(self.analyzer)
            return frequency_predictor.predict(data, count)
        except Exception as e:
            print(f"âš ï¸ è‡ªé€‚åº”é›†æˆå­¦ä¹ å¤±è´¥: {e}")
            frequency_predictor = FrequencyPredictor(self.analyzer)
            return frequency_predictor.predict(data, count)

    def _prepare_ensemble_data(self, data: pd.DataFrame):
        """å‡†å¤‡é›†æˆå­¦ä¹ æ•°æ®"""
        X = []
        y = []

        # ä½¿ç”¨æ»‘åŠ¨çª—å£åˆ›å»ºè®­ç»ƒæ ·æœ¬
        window_size = 5

        for i in range(window_size, len(data)):
            # ç‰¹å¾ï¼šå‰window_sizeæœŸçš„ç»Ÿè®¡ä¿¡æ¯
            features = []

            for j in range(window_size):
                period_data = data.iloc[i - window_size + j]
                numbers = [int(period_data[f'num{k}']) for k in range(1, 21)]

                # æœŸé—´ç‰¹å¾
                features.extend([
                    sum(numbers) / 20,  # å¹³å‡å€¼
                    len([n for n in numbers if n % 2 == 1]) / 20,  # å¥‡æ•°æ¯”
                    len([n for n in numbers if n > 40]) / 20,  # å¤§å·æ¯”
                ])

            # ç›®æ ‡ï¼šå½“å‰æœŸçš„å·ç ï¼ˆå¤šæ ‡ç­¾ï¼‰
            current_numbers = [int(data.iloc[i][f'num{k}']) for k in range(1, 21)]
            target = [0] * 80
            for num in current_numbers:
                target[num - 1] = 1

            X.append(features)
            y.append(target)

        return np.array(X), np.array(y)

    def _train_ensemble_models(self, X, y):
        """è®­ç»ƒå¤šä¸ªåŸºç¡€æ¨¡å‹"""
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.multioutput import MultiOutputClassifier
        from sklearn.linear_model import LogisticRegression

        models = {}

        print("è®­ç»ƒé›†æˆæ¨¡å‹...")

        # éšæœºæ£®æ—
        try:
            rf = MultiOutputClassifier(RandomForestClassifier(
                n_estimators=100, random_state=42, n_jobs=-1
            ))
            rf.fit(X, y)
            models['random_forest'] = rf
            print("âœ… éšæœºæ£®æ—è®­ç»ƒå®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ éšæœºæ£®æ—è®­ç»ƒå¤±è´¥: {e}")

        # é€»è¾‘å›å½’
        try:
            lr = MultiOutputClassifier(LogisticRegression(
                random_state=42, max_iter=1000
            ))
            lr.fit(X, y)
            models['logistic_regression'] = lr
            print("âœ… é€»è¾‘å›å½’è®­ç»ƒå®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ é€»è¾‘å›å½’è®­ç»ƒå¤±è´¥: {e}")

        return models

    def _calculate_adaptive_weights(self, models, X, y):
        """è®¡ç®—è‡ªé€‚åº”æƒé‡"""
        from sklearn.model_selection import cross_val_score
        from sklearn.metrics import accuracy_score

        weights = {}

        for name, model in models.items():
            try:
                # ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹æ€§èƒ½
                # ç”±äºæ˜¯å¤šæ ‡ç­¾é—®é¢˜ï¼Œä½¿ç”¨ç®€åŒ–çš„è¯„ä¼°æ–¹æ³•
                predictions = model.predict(X)

                # è®¡ç®—å¹³å‡å‡†ç¡®ç‡
                accuracies = []
                for i in range(y.shape[1]):  # å¯¹æ¯ä¸ªè¾“å‡ºç»´åº¦
                    acc = accuracy_score(y[:, i], predictions[:, i])
                    accuracies.append(acc)

                avg_accuracy = np.mean(accuracies)
                weights[name] = max(avg_accuracy, 0.1)  # æœ€å°æƒé‡0.1

                print(f"{name} å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.3f}")

            except Exception as e:
                print(f"âš ï¸ {name} æƒé‡è®¡ç®—å¤±è´¥: {e}")
                weights[name] = 0.1

        # å½’ä¸€åŒ–æƒé‡
        total_weight = sum(weights.values())
        if total_weight > 0:
            weights = {k: v / total_weight for k, v in weights.items()}

        print(f"æ¨¡å‹æƒé‡: {weights}")
        return weights

    def _ensemble_predict(self, models, weights, X, count):
        """é›†æˆé¢„æµ‹"""
        if not models:
            print("âš ï¸ æ²¡æœ‰å¯ç”¨æ¨¡å‹ï¼Œä½¿ç”¨é¢‘ç‡åˆ†æ")
            frequency_predictor = FrequencyPredictor(self.analyzer)
            return frequency_predictor.predict(pd.DataFrame(), count)

        # æ„å»ºé¢„æµ‹ç‰¹å¾ï¼šåŸºäºå†å²æ•°æ®çš„ç»Ÿè®¡ç‰¹å¾
        if len(X) > 0:
            # ä½¿ç”¨æœ€åä¸€ä¸ªæ ·æœ¬çš„ç‰¹å¾
            last_sample = X[-1:].reshape(1, -1)
        else:
            # å¦‚æœæ²¡æœ‰è®­ç»ƒæ•°æ®ï¼Œåˆ›å»ºé»˜è®¤ç‰¹å¾
            last_sample = np.zeros((1, 15))  # 5ä¸ªçª—å£ * 3ä¸ªç‰¹å¾

        print(f"é¢„æµ‹ç‰¹å¾ç»´åº¦: {last_sample.shape}")

        # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹ç»“æœ
        ensemble_predictions = np.zeros(80)
        successful_predictions = 0

        for name, model in models.items():
            try:
                # å¯¹æ¯ä¸ªå·ç è¿›è¡ŒäºŒåˆ†ç±»é¢„æµ‹
                model_predictions = np.zeros(80)

                # å¦‚æœæ˜¯å¤šè¾“å‡ºæ¨¡å‹ï¼Œç›´æ¥é¢„æµ‹
                if hasattr(model, 'predict'):
                    prediction = model.predict(last_sample)[0]
                    if len(prediction) == 80:
                        model_predictions = prediction
                    else:
                        # å¦‚æœé¢„æµ‹ç»´åº¦ä¸åŒ¹é…ï¼Œä½¿ç”¨æ¦‚ç‡é¢„æµ‹
                        if hasattr(model, 'predict_proba'):
                            proba = model.predict_proba(last_sample)
                            if len(proba) == 80:
                                model_predictions = [p[1] if len(p) > 1 else p[0] for p in proba]
                            else:
                                model_predictions = np.random.random(80)
                        else:
                            model_predictions = np.random.random(80)

                weight = weights.get(name, 0.1)
                ensemble_predictions += np.array(model_predictions) * weight
                successful_predictions += 1

                print(f"âœ… {name} é¢„æµ‹æˆåŠŸï¼Œæƒé‡: {weight:.3f}")

            except Exception as e:
                print(f"âš ï¸ {name} é¢„æµ‹å¤±è´¥: {e}")
                # ä½¿ç”¨éšæœºé¢„æµ‹ä½œä¸ºåå¤‡
                weight = weights.get(name, 0.1)
                ensemble_predictions += np.random.random(80) * weight * 0.1

        if successful_predictions == 0:
            print("âš ï¸ æ‰€æœ‰æ¨¡å‹é¢„æµ‹å¤±è´¥ï¼Œä½¿ç”¨éšæœºé¢„æµ‹")
            ensemble_predictions = np.random.random(80)

        # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„å·ç 
        number_probs = [(i + 1, prob) for i, prob in enumerate(ensemble_predictions)]
        number_probs.sort(key=lambda x: x[1], reverse=True)

        predicted_numbers = [num for num, _ in number_probs[:count]]
        confidence_scores = [float(prob) for _, prob in number_probs[:count]]

        # å½’ä¸€åŒ–ç½®ä¿¡åº¦åˆ°0-1èŒƒå›´
        if confidence_scores and max(confidence_scores) > 0:
            max_conf = max(confidence_scores)
            confidence_scores = [conf / max_conf for conf in confidence_scores]
        else:
            confidence_scores = [0.1] * len(predicted_numbers)

        return predicted_numbers, confidence_scores


class BayesianPredictor:
    """è´å¶æ–¯æ¨ç†é¢„æµ‹å™¨ - åŠ¨æ€è´å¶æ–¯ç½‘ç»œå’ŒMCMCé‡‡æ ·"""

    def __init__(self, analyzer):
        self.analyzer = analyzer
        self.num_samples = 1000  # MCMCé‡‡æ ·æ¬¡æ•°

    def predict(self, data: pd.DataFrame, count: int = 30, **kwargs) -> Tuple[List[int], List[float]]:
        """è´å¶æ–¯æ¨ç†é¢„æµ‹"""
        print(f"ğŸ”„ æ‰§è¡Œè´å¶æ–¯æ¨ç†é¢„æµ‹...")
        print(f"åˆ†ææ•°æ®: {len(data)}æœŸï¼ŒMCMCé‡‡æ ·: {self.num_samples}æ¬¡")

        # æ„å»ºå…ˆéªŒåˆ†å¸ƒ
        prior_distribution = self._build_prior_distribution(data)

        # MCMCé‡‡æ ·
        posterior_samples = self._mcmc_sampling(data, prior_distribution)

        # åéªŒæ¦‚ç‡è®¡ç®—
        posterior_probabilities = self._calculate_posterior_probabilities(posterior_samples)

        # é€‰æ‹©æœ€ä¼˜å·ç 
        predicted_numbers, confidence_scores = self._bayesian_selection(
            posterior_probabilities, count
        )

        print(f"âœ… è´å¶æ–¯æ¨ç†é¢„æµ‹å®Œæˆ")
        print(f"é¢„æµ‹å·ç : {predicted_numbers[:10]}...")
        print(f"å¹³å‡ç½®ä¿¡åº¦: {np.mean(confidence_scores):.3f}")

        return predicted_numbers, confidence_scores

    def _build_prior_distribution(self, data: pd.DataFrame):
        """æ„å»ºå…ˆéªŒåˆ†å¸ƒ"""
        # ä½¿ç”¨Dirichletåˆ†å¸ƒä½œä¸ºå…ˆéªŒ
        alpha = np.ones(80) + 0.1  # å¹³æ»‘å‚æ•°

        # åŸºäºå†å²æ•°æ®æ›´æ–°å…ˆéªŒ
        for _, row in data.iterrows():
            numbers = [int(row[f'num{i}']) for i in range(1, 21)]
            for num in numbers:
                alpha[num - 1] += 1

        return alpha

    def _mcmc_sampling(self, data: pd.DataFrame, prior_alpha):
        """MCMCé‡‡æ · - Gibbsé‡‡æ ·"""
        print("å¼€å§‹MCMCé‡‡æ ·...")

        samples = []

        # åˆå§‹åŒ–å‚æ•°
        current_theta = np.random.dirichlet(prior_alpha)

        for i in range(self.num_samples):
            # Gibbsé‡‡æ ·æ­¥éª¤

            # 1. åŸºäºå½“å‰å‚æ•°é‡‡æ ·å·ç ç»„åˆ
            sampled_numbers = self._sample_numbers_from_theta(current_theta)

            # 2. åŸºäºé‡‡æ ·ç»“æœæ›´æ–°å‚æ•°
            updated_alpha = prior_alpha.copy()
            for num in sampled_numbers:
                updated_alpha[num - 1] += 1

            # 3. ä»åéªŒåˆ†å¸ƒé‡‡æ ·æ–°å‚æ•°
            current_theta = np.random.dirichlet(updated_alpha)

            # 4. è®°å½•æ ·æœ¬
            samples.append({
                'theta': current_theta.copy(),
                'numbers': sampled_numbers
            })

            if (i + 1) % 200 == 0:
                print(f"MCMCé‡‡æ ·è¿›åº¦: {i + 1}/{self.num_samples}")

        print("MCMCé‡‡æ ·å®Œæˆ")
        return samples

    def _sample_numbers_from_theta(self, theta):
        """åŸºäºå‚æ•°thetaé‡‡æ ·å·ç ç»„åˆ"""
        # ç¡®ä¿thetaæ˜¯æœ‰æ•ˆçš„æ¦‚ç‡åˆ†å¸ƒ
        theta = theta / np.sum(theta)

        # é‡‡æ ·20ä¸ªä¸é‡å¤å·ç 
        sampled_numbers = []
        remaining_theta = theta.copy()

        for _ in range(20):
            # å½’ä¸€åŒ–å‰©ä½™æ¦‚ç‡
            if np.sum(remaining_theta) > 0:
                prob = remaining_theta / np.sum(remaining_theta)

                # é‡‡æ ·ä¸€ä¸ªå·ç 
                sampled_idx = np.random.choice(80, p=prob)
                sampled_numbers.append(sampled_idx + 1)

                # ç§»é™¤å·²é‡‡æ ·çš„å·ç 
                remaining_theta[sampled_idx] = 0
            else:
                # å¦‚æœæ¦‚ç‡ç”¨å®Œï¼Œéšæœºé€‰æ‹©å‰©ä½™å·ç 
                remaining_numbers = [i + 1 for i in range(80) if (i + 1) not in sampled_numbers]
                if remaining_numbers:
                    sampled_numbers.append(np.random.choice(remaining_numbers))

        return sorted(sampled_numbers)

    def _calculate_posterior_probabilities(self, samples):
        """è®¡ç®—åéªŒæ¦‚ç‡"""
        # ç»Ÿè®¡æ¯ä¸ªå·ç åœ¨æ ·æœ¬ä¸­çš„å‡ºç°é¢‘ç‡
        number_counts = np.zeros(80)

        for sample in samples:
            for num in sample['numbers']:
                number_counts[num - 1] += 1

        # è®¡ç®—åéªŒæ¦‚ç‡
        posterior_probs = number_counts / len(samples)

        return posterior_probs

    def _bayesian_selection(self, posterior_probs, count):
        """è´å¶æ–¯é€‰æ‹©æœ€ä¼˜å·ç """
        # æŒ‰åéªŒæ¦‚ç‡æ’åº
        number_probs = [(i + 1, prob) for i, prob in enumerate(posterior_probs)]
        number_probs.sort(key=lambda x: x[1], reverse=True)

        predicted_numbers = [num for num, _ in number_probs[:count]]
        confidence_scores = [float(prob) for _, prob in number_probs[:count]]

        # å½’ä¸€åŒ–ç½®ä¿¡åº¦
        if confidence_scores:
            max_conf = max(confidence_scores) if max(confidence_scores) > 0 else 1
            confidence_scores = [conf / max_conf for conf in confidence_scores]

        return predicted_numbers, confidence_scores


class SuperPredictor:
    """è¶…çº§é¢„æµ‹å™¨ - æ‰€æœ‰ç®—æ³•çš„æ™ºèƒ½èåˆ"""

    def __init__(self, analyzer):
        self.analyzer = analyzer
        self.predictors = {
            'frequency': FrequencyPredictor(analyzer),
            'hot_cold': HotColdPredictor(analyzer),
            'missing': MissingPredictor(analyzer),
            'adaptive_markov': AdaptiveMarkovPredictor(analyzer),
            'transformer': TransformerPredictor(analyzer),
            'gnn': GraphNeuralNetworkPredictor(analyzer),
            'monte_carlo': MonteCarloPredictor(analyzer),
            'clustering': ClusteringPredictor(analyzer),
            'advanced_ensemble': AdvancedEnsemblePredictor(analyzer),
            'bayesian': BayesianPredictor(analyzer)
        }

    def predict(self, data: pd.DataFrame, count: int = 30, **kwargs) -> Tuple[List[int], List[float]]:
        """è¶…çº§é¢„æµ‹å™¨ - 15+ç§ç®—æ³•æ™ºèƒ½èåˆ"""
        print(f"ğŸ”„ æ‰§è¡Œè¶…çº§é¢„æµ‹å™¨...")
        print(f"åˆ†ææ•°æ®: {len(data)}æœŸï¼Œèåˆç®—æ³•: {len(self.predictors)}ç§")

        # æ”¶é›†æ‰€æœ‰é¢„æµ‹ç»“æœ
        all_predictions = {}
        all_confidences = {}
        execution_times = {}

        for name, predictor in self.predictors.items():
            try:
                import time
                start_time = time.time()

                numbers, confidences = predictor.predict(data, count * 2)  # è·å–æ›´å¤šå€™é€‰

                execution_time = time.time() - start_time

                all_predictions[name] = numbers
                all_confidences[name] = confidences
                execution_times[name] = execution_time

                print(f"âœ… {name}: {len(numbers)}ä¸ªå·ç , å¹³å‡ç½®ä¿¡åº¦={np.mean(confidences):.3f}, è€—æ—¶={execution_time:.2f}s")

            except Exception as e:
                print(f"âš ï¸ {name} é¢„æµ‹å¤±è´¥: {e}")
                continue

        # åŠ¨æ€æƒé‡åˆ†é…
        weights = self._calculate_dynamic_weights(all_predictions, all_confidences, execution_times, data)

        # æ™ºèƒ½èåˆ
        final_numbers, final_confidences = self._intelligent_fusion(
            all_predictions, all_confidences, weights, count
        )

        print(f"âœ… è¶…çº§é¢„æµ‹å™¨å®Œæˆ")
        print(f"èåˆäº† {len([w for w in weights.values() if w > 0])} ä¸ªæœ‰æ•ˆé¢„æµ‹å™¨")
        print(f"é¢„æµ‹å·ç : {final_numbers[:10]}...")
        print(f"å¹³å‡ç½®ä¿¡åº¦: {np.mean(final_confidences):.3f}")

        return final_numbers, final_confidences

    def _calculate_dynamic_weights(self, all_predictions, all_confidences, execution_times, data):
        """è®¡ç®—åŠ¨æ€æƒé‡"""
        weights = {}

        for name in all_predictions.keys():
            weight = 1.0

            # åŸºäºç½®ä¿¡åº¦çš„æƒé‡
            if name in all_confidences:
                avg_confidence = np.mean(all_confidences[name])
                weight *= (1.0 + avg_confidence)

            # åŸºäºæ‰§è¡Œæ—¶é—´çš„æƒé‡ï¼ˆå¿«é€Ÿç®—æ³•è·å¾—è½»å¾®åŠ åˆ†ï¼‰
            if name in execution_times:
                exec_time = execution_times[name]
                time_factor = 1.0 / (1.0 + exec_time / 10.0)  # 10ç§’ä»¥å†…çš„ç®—æ³•è·å¾—åŠ åˆ†
                weight *= time_factor

            # åŸºäºæ•°æ®é‡çš„æƒé‡è°ƒæ•´
            data_size = len(data)
            if name in ['transformer', 'gnn', 'advanced_ensemble']:
                # æ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨æ•°æ®å……è¶³æ—¶æƒé‡æ›´é«˜
                weight *= min(2.0, data_size / 50.0)
            elif name in ['frequency', 'hot_cold']:
                # ç®€å•æ–¹æ³•åœ¨æ•°æ®ä¸è¶³æ—¶æƒé‡æ›´é«˜
                weight *= max(0.5, 2.0 - data_size / 50.0)

            weights[name] = weight

        # å½’ä¸€åŒ–æƒé‡
        total_weight = sum(weights.values())
        if total_weight > 0:
            weights = {k: v / total_weight for k, v in weights.items()}

        print(f"åŠ¨æ€æƒé‡åˆ†é…: {weights}")
        return weights

    def _intelligent_fusion(self, all_predictions, all_confidences, weights, count):
        """æ™ºèƒ½èåˆé¢„æµ‹ç»“æœ"""
        # æ”¶é›†æ‰€æœ‰å€™é€‰å·ç åŠå…¶åŠ æƒå¾—åˆ†
        number_scores = {}

        for name, numbers in all_predictions.items():
            weight = weights.get(name, 0)
            confidences = all_confidences.get(name, [])

            for i, number in enumerate(numbers):
                confidence = confidences[i] if i < len(confidences) else 0.1
                weighted_score = confidence * weight

                if number not in number_scores:
                    number_scores[number] = 0
                number_scores[number] += weighted_score

        # æŒ‰åŠ æƒå¾—åˆ†æ’åº
        sorted_numbers = sorted(number_scores.items(), key=lambda x: x[1], reverse=True)

        # é€‰æ‹©å‰countä¸ªå·ç 
        final_numbers = [num for num, score in sorted_numbers[:count]]
        final_confidences = [score for num, score in sorted_numbers[:count]]

        # å½’ä¸€åŒ–ç½®ä¿¡åº¦
        if final_confidences:
            max_conf = max(final_confidences)
            if max_conf > 0:
                final_confidences = [conf / max_conf for conf in final_confidences]

        return final_numbers, final_confidences


class HighConfidencePredictor:
    """é«˜ç½®ä¿¡åº¦é¢„æµ‹ç³»ç»Ÿ - é€‰æ‹©æ€§é¢„æµ‹æœºåˆ¶"""

    def __init__(self, analyzer):
        self.analyzer = analyzer
        self.confidence_threshold = 0.90  # 90%ç½®ä¿¡åº¦é˜ˆå€¼
        self.super_predictor = SuperPredictor(analyzer)

    def predict(self, data: pd.DataFrame, count: int = 30, **kwargs) -> Tuple[List[int], List[float]]:
        """é«˜ç½®ä¿¡åº¦é¢„æµ‹ - åªåœ¨é«˜ç½®ä¿¡åº¦æ—¶è¾“å‡º"""
        print(f"ğŸ”„ æ‰§è¡Œé«˜ç½®ä¿¡åº¦é¢„æµ‹ç³»ç»Ÿ...")
        print(f"ç½®ä¿¡åº¦é˜ˆå€¼: {self.confidence_threshold:.1%}")

        # ä½¿ç”¨è¶…çº§é¢„æµ‹å™¨è·å¾—åˆå§‹é¢„æµ‹
        numbers, confidences = self.super_predictor.predict(data, count)

        # 6ç»´ç½®ä¿¡åº¦è¯„ä¼°
        confidence_dimensions = self._evaluate_confidence_dimensions(data, numbers, confidences)

        # 4å±‚éªŒè¯æœºåˆ¶
        validation_results = self._four_layer_validation(data, numbers, confidence_dimensions)

        # ç»¼åˆç½®ä¿¡åº¦è®¡ç®—
        overall_confidence = self._calculate_overall_confidence(confidence_dimensions, validation_results)

        print(f"ç»¼åˆç½®ä¿¡åº¦: {overall_confidence:.1%}")

        if overall_confidence >= self.confidence_threshold:
            print(f"âœ… ç½®ä¿¡åº¦è¾¾æ ‡ï¼Œè¾“å‡ºé¢„æµ‹ç»“æœ")
            return numbers, confidences
        else:
            print(f"âš ï¸ ç½®ä¿¡åº¦ä¸è¶³ ({overall_confidence:.1%} < {self.confidence_threshold:.1%})")
            print(f"å»ºè®®ç­‰å¾…æ›´å¥½çš„é¢„æµ‹æ—¶æœº")

            # è¿”å›ç©ºç»“æœæˆ–é™çº§é¢„æµ‹
            return [], []

    def _evaluate_confidence_dimensions(self, data, numbers, confidences):
        """6ç»´ç½®ä¿¡åº¦è¯„ä¼°"""
        dimensions = {}

        # 1. æ¨¡å‹ä¸€è‡´æ€§
        dimensions['model_consistency'] = np.mean(confidences) if confidences else 0

        # 2. æ•°æ®è´¨é‡
        data_quality = min(1.0, len(data) / 100.0)  # 100æœŸä¸ºæ»¡åˆ†
        dimensions['data_quality'] = data_quality

        # 3. æ¨¡å¼å¼ºåº¦
        pattern_strength = self._calculate_pattern_strength(data)
        dimensions['pattern_strength'] = pattern_strength

        # 4. å†å²éªŒè¯
        historical_accuracy = self._calculate_historical_accuracy(data, numbers)
        dimensions['historical_accuracy'] = historical_accuracy

        # 5. ç»Ÿè®¡æ˜¾è‘—æ€§
        statistical_significance = self._calculate_statistical_significance(data, numbers)
        dimensions['statistical_significance'] = statistical_significance

        # 6. é¢„æµ‹ç¨³å®šæ€§
        prediction_stability = self._calculate_prediction_stability(data, numbers)
        dimensions['prediction_stability'] = prediction_stability

        print(f"6ç»´ç½®ä¿¡åº¦è¯„ä¼°: {dimensions}")
        return dimensions

    def _calculate_pattern_strength(self, data):
        """è®¡ç®—æ¨¡å¼å¼ºåº¦"""
        if len(data) < 10:
            return 0.1

        # åˆ†æå·ç å‡ºç°çš„è§„å¾‹æ€§
        number_frequencies = np.zeros(80)
        for _, row in data.iterrows():
            numbers = [int(row[f'num{i}']) for i in range(1, 21)]
            for num in numbers:
                number_frequencies[num - 1] += 1

        # è®¡ç®—é¢‘ç‡åˆ†å¸ƒçš„æ–¹å·®ï¼ˆæ–¹å·®è¶Šå¤§ï¼Œæ¨¡å¼è¶Šå¼ºï¼‰
        freq_variance = np.var(number_frequencies)
        pattern_strength = min(1.0, freq_variance / 100.0)

        return pattern_strength

    def _calculate_historical_accuracy(self, data, predicted_numbers):
        """è®¡ç®—å†å²å‡†ç¡®æ€§"""
        if len(data) < 5:
            return 0.5

        # ä½¿ç”¨å‰80%çš„æ•°æ®è®­ç»ƒï¼Œå20%éªŒè¯
        split_point = int(len(data) * 0.8)
        train_data = data.iloc[:split_point]
        test_data = data.iloc[split_point:]

        if len(test_data) == 0:
            return 0.5

        # ç®€åŒ–çš„å†å²éªŒè¯
        total_accuracy = 0
        for _, test_row in test_data.iterrows():
            actual_numbers = [int(test_row[f'num{i}']) for i in range(1, 21)]

            # è®¡ç®—é¢„æµ‹å·ç ä¸å®é™…å·ç çš„é‡å åº¦
            overlap = len(set(predicted_numbers) & set(actual_numbers))
            accuracy = overlap / min(len(predicted_numbers), len(actual_numbers))
            total_accuracy += accuracy

        return total_accuracy / len(test_data)

    def _calculate_statistical_significance(self, data, predicted_numbers):
        """è®¡ç®—ç»Ÿè®¡æ˜¾è‘—æ€§"""
        if len(data) < 10:
            return 0.3

        # è®¡ç®—é¢„æµ‹å·ç çš„ç»Ÿè®¡ç‰¹å¾ä¸å†å²æ•°æ®çš„ä¸€è‡´æ€§
        historical_avg = []
        for _, row in data.iterrows():
            numbers = [int(row[f'num{i}']) for i in range(1, 21)]
            historical_avg.append(np.mean(numbers))

        predicted_avg = np.mean(predicted_numbers) if predicted_numbers else 40
        historical_mean = np.mean(historical_avg)
        historical_std = np.std(historical_avg)

        if historical_std == 0:
            return 0.5

        # Z-scoreè®¡ç®—
        z_score = abs(predicted_avg - historical_mean) / historical_std
        significance = max(0, 1.0 - z_score / 3.0)  # 3ä¸ªæ ‡å‡†å·®å†…ä¸ºæ˜¾è‘—

        return significance

    def _calculate_prediction_stability(self, data, predicted_numbers):
        """è®¡ç®—é¢„æµ‹ç¨³å®šæ€§"""
        # å¤šæ¬¡é¢„æµ‹çš„ä¸€è‡´æ€§ï¼ˆç®€åŒ–å®ç°ï¼‰
        if len(predicted_numbers) < 5:
            return 0.2

        # æ£€æŸ¥é¢„æµ‹å·ç çš„åˆ†å¸ƒæ˜¯å¦åˆç†
        if len(set(predicted_numbers)) != len(predicted_numbers):
            return 0.1  # æœ‰é‡å¤å·ç ï¼Œç¨³å®šæ€§å·®

        # æ£€æŸ¥å·ç èŒƒå›´åˆ†å¸ƒ
        zones = [0] * 8
        for num in predicted_numbers:
            zone_idx = (num - 1) // 10
            zones[zone_idx] += 1

        # åˆ†å¸ƒè¶Šå‡åŒ€ï¼Œç¨³å®šæ€§è¶Šé«˜
        zone_variance = np.var(zones)
        stability = max(0.2, 1.0 - zone_variance / 10.0)

        return stability

    def _four_layer_validation(self, data, numbers, confidence_dimensions):
        """4å±‚éªŒè¯æœºåˆ¶"""
        validation_results = {}

        # ç¬¬1å±‚ï¼šåŸºç¡€æ•°æ®éªŒè¯
        validation_results['data_validation'] = self._validate_data_quality(data)

        # ç¬¬2å±‚ï¼šæ¨¡å‹è¾“å‡ºéªŒè¯
        validation_results['model_validation'] = self._validate_model_output(numbers)

        # ç¬¬3å±‚ï¼šç»Ÿè®¡ä¸€è‡´æ€§éªŒè¯
        validation_results['statistical_validation'] = self._validate_statistical_consistency(data, numbers)

        # ç¬¬4å±‚ï¼šä¸šåŠ¡é€»è¾‘éªŒè¯
        validation_results['business_validation'] = self._validate_business_logic(numbers)

        print(f"4å±‚éªŒè¯ç»“æœ: {validation_results}")
        return validation_results

    def _validate_data_quality(self, data):
        """éªŒè¯æ•°æ®è´¨é‡"""
        if len(data) < 20:
            return 0.3
        elif len(data) < 50:
            return 0.6
        else:
            return 1.0

    def _validate_model_output(self, numbers):
        """éªŒè¯æ¨¡å‹è¾“å‡º"""
        if not numbers:
            return 0.0

        # æ£€æŸ¥å·ç èŒƒå›´
        if any(num < 1 or num > 80 for num in numbers):
            return 0.0

        # æ£€æŸ¥é‡å¤
        if len(set(numbers)) != len(numbers):
            return 0.3

        return 1.0

    def _validate_statistical_consistency(self, data, numbers):
        """éªŒè¯ç»Ÿè®¡ä¸€è‡´æ€§"""
        if not numbers or len(data) == 0:
            return 0.0

        # æ£€æŸ¥å’Œå€¼æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
        predicted_sum = sum(numbers)

        historical_sums = []
        for _, row in data.iterrows():
            period_numbers = [int(row[f'num{i}']) for i in range(1, 21)]
            historical_sums.append(sum(period_numbers))

        if historical_sums:
            mean_sum = np.mean(historical_sums)
            std_sum = np.std(historical_sums)

            if std_sum > 0:
                z_score = abs(predicted_sum - mean_sum) / std_sum
                consistency = max(0, 1.0 - z_score / 2.0)
                return consistency

        return 0.5

    def _validate_business_logic(self, numbers):
        """éªŒè¯ä¸šåŠ¡é€»è¾‘"""
        if not numbers:
            return 0.0

        # æ£€æŸ¥å·ç åˆ†å¸ƒçš„åˆç†æ€§
        score = 1.0

        # å¥‡å¶æ¯”ä¾‹æ£€æŸ¥
        odd_count = sum(1 for num in numbers if num % 2 == 1)
        odd_ratio = odd_count / len(numbers)
        if odd_ratio < 0.3 or odd_ratio > 0.7:
            score *= 0.8

        # å¤§å°æ¯”ä¾‹æ£€æŸ¥
        big_count = sum(1 for num in numbers if num > 40)
        big_ratio = big_count / len(numbers)
        if big_ratio < 0.3 or big_ratio > 0.7:
            score *= 0.8

        return score

    def _calculate_overall_confidence(self, confidence_dimensions, validation_results):
        """è®¡ç®—ç»¼åˆç½®ä¿¡åº¦"""
        # 6ç»´ç½®ä¿¡åº¦æƒé‡
        dimension_weights = {
            'model_consistency': 0.25,
            'data_quality': 0.15,
            'pattern_strength': 0.15,
            'historical_accuracy': 0.20,
            'statistical_significance': 0.15,
            'prediction_stability': 0.10
        }

        # 4å±‚éªŒè¯æƒé‡
        validation_weights = {
            'data_validation': 0.20,
            'model_validation': 0.30,
            'statistical_validation': 0.25,
            'business_validation': 0.25
        }

        # è®¡ç®—ç»´åº¦å¾—åˆ†
        dimension_score = sum(
            confidence_dimensions.get(dim, 0) * weight
            for dim, weight in dimension_weights.items()
        )

        # è®¡ç®—éªŒè¯å¾—åˆ†
        validation_score = sum(
            validation_results.get(val, 0) * weight
            for val, weight in validation_weights.items()
        )

        # ç»¼åˆç½®ä¿¡åº¦ï¼ˆç»´åº¦å¾—åˆ†70%ï¼ŒéªŒè¯å¾—åˆ†30%ï¼‰
        overall_confidence = dimension_score * 0.7 + validation_score * 0.3

        return overall_confidence


class EnsemblePredictor:
    """é›†æˆå­¦ä¹ é¢„æµ‹å™¨"""
    
    def __init__(self, analyzer):
        self.analyzer = analyzer
        self.base_predictors = {
            'frequency': FrequencyPredictor(analyzer),
            'hot_cold': HotColdPredictor(analyzer),
            'missing': MissingPredictor(analyzer),
            'markov': MarkovPredictor(analyzer)
        }
    
    def predict(self, data: pd.DataFrame, count: int = 30, **kwargs) -> Tuple[List[int], List[float]]:
        """é›†æˆé¢„æµ‹"""
        print("æ‰§è¡Œé›†æˆå­¦ä¹ é¢„æµ‹...")
        
        # æ”¶é›†å„ä¸ªé¢„æµ‹å™¨çš„ç»“æœ
        all_predictions = {}
        weights = {'frequency': 0.3, 'hot_cold': 0.25, 'missing': 0.2, 'markov': 0.25}
        
        for name, predictor in self.base_predictors.items():
            try:
                numbers, scores = predictor.predict(data, count=count * 2)  # è·å–æ›´å¤šå€™é€‰
                all_predictions[name] = list(zip(numbers, scores))
            except Exception as e:
                print(f"é¢„æµ‹å™¨ {name} æ‰§è¡Œå¤±è´¥: {e}")
                all_predictions[name] = []
        
        # èåˆé¢„æµ‹ç»“æœ
        final_scores = {}
        
        for name, predictions in all_predictions.items():
            weight = weights.get(name, 0.1)
            for num, score in predictions:
                if num not in final_scores:
                    final_scores[num] = 0
                final_scores[num] += weight * score
        
        # æ’åºå¹¶é€‰æ‹©å‰countä¸ª
        sorted_predictions = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)
        
        predicted_numbers = [num for num, _ in sorted_predictions[:count]]
        confidence_scores = [score for _, score in sorted_predictions[:count]]
        
        return predicted_numbers, confidence_scores


class PredictionEngine:
    """é¢„æµ‹å¼•æ“"""
    
    def __init__(self, analyzer):
        self.analyzer = analyzer
        self.predictors = {
            'frequency': FrequencyPredictor(analyzer),
            'hot_cold': HotColdPredictor(analyzer),
            'missing': MissingPredictor(analyzer),
            'markov': MarkovPredictor(analyzer),
            'markov_2nd': Markov2ndPredictor(analyzer),
            'markov_3rd': Markov3rdPredictor(analyzer),
            'adaptive_markov': AdaptiveMarkovPredictor(analyzer),
            'transformer': TransformerPredictor(analyzer),
            'gnn': GraphNeuralNetworkPredictor(analyzer),
            'monte_carlo': MonteCarloPredictor(analyzer),
            'clustering': ClusteringPredictor(analyzer),
            'advanced_ensemble': AdvancedEnsemblePredictor(analyzer),
            'bayesian': BayesianPredictor(analyzer),
            'super_predictor': SuperPredictor(analyzer),
            'high_confidence': HighConfidencePredictor(analyzer),
            'lstm': LSTMPredictor(analyzer),
            'ensemble': EnsemblePredictor(analyzer)
        }
    
    def predict(self,
                data: pd.DataFrame,
                target_issue: str,
                count: int,
                method: str,
                **kwargs) -> PredictionResult:
        """æ‰§è¡Œé¢„æµ‹"""
        
        if method not in self.predictors:
            raise ValueError(f"ä¸æ”¯æŒçš„é¢„æµ‹æ–¹æ³•: {method}")
        
        predictor = self.predictors[method]
        
        # æ‰§è¡Œé¢„æµ‹
        start_time = time.time()
        predicted_numbers, confidence_scores = predictor.predict(
            data=data,
            count=count,
            **kwargs
        )
        execution_time = time.time() - start_time
        
        return PredictionResult(
            target_issue=target_issue,
            analysis_periods=len(data),
            method=method,
            predicted_numbers=predicted_numbers,
            confidence_scores=confidence_scores,
            generation_time=datetime.now(),
            execution_time=execution_time,
            parameters=kwargs
        )
    
    def get_available_methods(self) -> List[str]:
        """è·å–å¯ç”¨çš„é¢„æµ‹æ–¹æ³•"""
        return list(self.predictors.keys())


class ComparisonEngine:
    """ç»“æœå¯¹æ¯”å¼•æ“"""
    
    def __init__(self, analyzer):
        self.analyzer = analyzer
    
    def compare(self,
                target_issue: str,
                predicted_numbers: List[int],
                actual_numbers: List[int]) -> ComparisonResult:
        """å¯¹æ¯”é¢„æµ‹ç»“æœ"""
        
        # è®¡ç®—å‘½ä¸­æƒ…å†µ
        hit_numbers = [num for num in predicted_numbers if num in actual_numbers]
        miss_numbers = [num for num in predicted_numbers if num not in actual_numbers]
        
        hit_count = len(hit_numbers)
        total_predicted = len(predicted_numbers)
        hit_rate = hit_count / total_predicted if total_predicted > 0 else 0
        
        # åˆ†æå‘½ä¸­åˆ†å¸ƒ
        hit_distribution = self._analyze_hit_distribution(hit_numbers)
        
        return ComparisonResult(
            target_issue=target_issue,
            predicted_numbers=predicted_numbers,
            actual_numbers=actual_numbers,
            hit_numbers=hit_numbers,
            miss_numbers=miss_numbers,
            hit_count=hit_count,
            total_predicted=total_predicted,
            hit_rate=hit_rate,
            hit_distribution=hit_distribution,
            comparison_time=datetime.now()
        )
    
    def _analyze_hit_distribution(self, hit_numbers: List[int]) -> Dict[str, int]:
        """åˆ†æå‘½ä¸­åˆ†å¸ƒ"""
        distribution = {
            'small_numbers': sum(1 for n in hit_numbers if n <= 40),
            'big_numbers': sum(1 for n in hit_numbers if n >= 41),
            'odd_numbers': sum(1 for n in hit_numbers if n % 2 == 1),
            'even_numbers': sum(1 for n in hit_numbers if n % 2 == 0)
        }
        
        # åŒºåŸŸåˆ†å¸ƒ
        for i in range(8):
            start = i * 10 + 1
            end = (i + 1) * 10
            zone_hits = sum(1 for n in hit_numbers if start <= n <= end)
            distribution[f'zone_{i+1}'] = zone_hits
        
        return distribution


class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.prediction_history = []
        self.performance_stats = {}
    
    def record_prediction(self, method: str, execution_time: float, hit_rate: float = None):
        """è®°å½•é¢„æµ‹æ€§èƒ½"""
        record = {
            'method': method,
            'execution_time': execution_time,
            'hit_rate': hit_rate,
            'timestamp': datetime.now()
        }
        self.prediction_history.append(record)
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        if method not in self.performance_stats:
            self.performance_stats[method] = {
                'total_predictions': 0,
                'total_time': 0,
                'total_hit_rate': 0,
                'count_with_hit_rate': 0
            }
        
        stats = self.performance_stats[method]
        stats['total_predictions'] += 1
        stats['total_time'] += execution_time
        
        if hit_rate is not None:
            stats['total_hit_rate'] += hit_rate
            stats['count_with_hit_rate'] += 1
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        summary = {}
        
        for method, stats in self.performance_stats.items():
            avg_time = stats['total_time'] / stats['total_predictions']
            avg_hit_rate = (stats['total_hit_rate'] / stats['count_with_hit_rate'] 
                           if stats['count_with_hit_rate'] > 0 else 0)
            
            summary[method] = {
                'avg_execution_time': avg_time,
                'avg_hit_rate': avg_hit_rate,
                'total_predictions': stats['total_predictions']
            }
        
        return summary


class Happy8Analyzer:
    """å¿«ä¹8åˆ†æå™¨æ ¸å¿ƒç±»"""
    
    def __init__(self, data_dir: str = "data"):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.data_dir = Path(data_dir)
        self.data_manager = DataManager(data_dir)
        self.prediction_engine = PredictionEngine(self)
        self.comparison_engine = ComparisonEngine(self)
        self.performance_monitor = PerformanceMonitor()
        self.pair_frequency_analyzer = PairFrequencyAnalyzer(self.data_manager)
        
        # æ•°æ®ç¼“å­˜
        self.historical_data = None
        
        print("å¿«ä¹8æ™ºèƒ½é¢„æµ‹ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def load_data(self, periods: Optional[int] = None) -> pd.DataFrame:
        """åŠ è½½å†å²æ•°æ®"""
        if self.historical_data is None:
            self.historical_data = self.data_manager.load_historical_data()

        if periods and periods > 0:
            return self.historical_data.head(periods)  # æ”¹ä¸ºheadï¼Œå› ä¸ºæ•°æ®å·²ç»æŒ‰æœ€æ–°æœŸå·æ’åº
        return self.historical_data

    def crawl_latest_data(self, limit: int = 100) -> pd.DataFrame:
        """çˆ¬å–æœ€æ–°æ•°æ®"""
        try:
            self.data_manager.crawl_initial_data(limit)
            # é‡æ–°åŠ è½½æ•°æ®
            self.historical_data = None
            return self.load_data()
        except Exception as e:
            print(f"çˆ¬å–æœ€æ–°æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()

    def crawl_all_historical_data(self) -> int:
        """çˆ¬å–æ‰€æœ‰å†å²æ•°æ®"""
        try:
            total_crawled = self.data_manager.crawl_all_historical_data()
            # é‡æ–°åŠ è½½æ•°æ®
            self.historical_data = None
            return total_crawled
        except Exception as e:
            print(f"çˆ¬å–æ‰€æœ‰å†å²æ•°æ®å¤±è´¥: {e}")
            return 0
    
    def predict(self,
                target_issue: str,
                periods: int = 300,
                count: int = 30,
                method: str = 'frequency',
                **kwargs) -> PredictionResult:
        """æ™ºèƒ½é¢„æµ‹ - è‡ªåŠ¨åˆ¤æ–­å†å²éªŒè¯æ¨¡å¼æˆ–æœªæ¥é¢„æµ‹æ¨¡å¼"""

        # åŠ è½½æ•°æ®
        data = self.load_data(periods)

        if len(data) == 0:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„å†å²æ•°æ®")

        # æ‰§è¡Œé¢„æµ‹
        result = self.prediction_engine.predict(
            data=data,
            target_issue=target_issue,
            count=count,
            method=method,
            **kwargs
        )

        # è®°å½•æ€§èƒ½
        self.performance_monitor.record_prediction(method, result.execution_time)

        return result

    def predict_with_smart_mode(self,
                               target_issue: str,
                               periods: int = 300,
                               count: int = 30,
                               method: str = 'frequency',
                               **kwargs) -> Dict[str, Any]:
        """æ™ºèƒ½é¢„æµ‹æ¨¡å¼ - è‡ªåŠ¨åˆ¤æ–­å¹¶æ‰§è¡Œç›¸åº”çš„é¢„æµ‹æ¨¡å¼

        Returns:
            DictåŒ…å«:
            - prediction_result: PredictionResultå¯¹è±¡
            - comparison_result: ComparisonResultå¯¹è±¡ï¼ˆä»…å†å²éªŒè¯æ¨¡å¼ï¼‰
            - mode: 'historical_validation' æˆ– 'future_prediction'
            - mode_description: æ¨¡å¼æè¿°
        """

        print(f"ğŸ¯ å¼€å§‹æ™ºèƒ½é¢„æµ‹åˆ†æ...")
        print(f"ç›®æ ‡æœŸå·: {target_issue}")
        print(f"é¢„æµ‹æ–¹æ³•: {method}")
        print(f"ç”Ÿæˆæ•°é‡: {count}ä¸ªå·ç ")
        print("-" * 50)

        # æ£€æŸ¥ç›®æ ‡æœŸå·æ˜¯å¦å­˜åœ¨äºå†å²æ•°æ®ä¸­
        is_historical = self._check_issue_exists(target_issue)

        if is_historical:
            # æ¨¡å¼1: å†å²éªŒè¯æ¨¡å¼
            print("ğŸ“Š æ£€æµ‹åˆ°å†å²æœŸå·ï¼Œå¯åŠ¨ã€å†å²éªŒè¯æ¨¡å¼ã€‘")
            print("æ‰§è¡Œæµç¨‹: é¢„æµ‹åˆ†æ â†’ è·å–å®é™…ç»“æœ â†’ å¯¹æ¯”åˆ†æ")
            mode = 'historical_validation'
            mode_description = 'å†å²éªŒè¯æ¨¡å¼ï¼šå¯¹å·²çŸ¥æœŸå·è¿›è¡Œé¢„æµ‹å¹¶éªŒè¯å‡†ç¡®æ€§'

            # æ‰§è¡Œé¢„æµ‹
            prediction_result = self.predict(
                target_issue=target_issue,
                periods=periods,
                count=count,
                method=method,
                **kwargs
            )

            print(f"âœ… é¢„æµ‹å®Œæˆï¼Œç”Ÿæˆ {len(prediction_result.predicted_numbers)} ä¸ªå·ç ")
            print(f"é¢„æµ‹å·ç : {prediction_result.predicted_numbers}")

            # æ‰§è¡Œå¯¹æ¯”åˆ†æ
            try:
                comparison_result = self.compare_results(
                    target_issue=target_issue,
                    predicted_numbers=prediction_result.predicted_numbers
                )

                print(f"âœ… å¯¹æ¯”åˆ†æå®Œæˆ")
                print(f"å‘½ä¸­ç‡: {comparison_result.hit_rate:.1%}")
                print(f"å‘½ä¸­å·ç : {comparison_result.hit_numbers}")
                print(f"æœªå‘½ä¸­å·ç : {comparison_result.miss_numbers}")

                return {
                    'prediction_result': prediction_result,
                    'comparison_result': comparison_result,
                    'mode': mode,
                    'mode_description': mode_description,
                    'success': True
                }

            except Exception as e:
                print(f"âš ï¸ å¯¹æ¯”åˆ†æå¤±è´¥: {e}")
                return {
                    'prediction_result': prediction_result,
                    'comparison_result': None,
                    'mode': mode,
                    'mode_description': mode_description,
                    'success': False,
                    'error': str(e)
                }

        else:
            # æ¨¡å¼2: æœªæ¥é¢„æµ‹æ¨¡å¼
            print("ğŸ”® æ£€æµ‹åˆ°æœªæ¥æœŸå·ï¼Œå¯åŠ¨ã€æœªæ¥é¢„æµ‹æ¨¡å¼ã€‘")
            print("æ‰§è¡Œæµç¨‹: é¢„æµ‹åˆ†æ â†’ è¿”å›é¢„æµ‹ç»“æœ")
            mode = 'future_prediction'
            mode_description = 'æœªæ¥é¢„æµ‹æ¨¡å¼ï¼šå¯¹æœªçŸ¥æœŸå·è¿›è¡Œé¢„æµ‹åˆ†æ'

            # æ‰§è¡Œé¢„æµ‹
            prediction_result = self.predict(
                target_issue=target_issue,
                periods=periods,
                count=count,
                method=method,
                **kwargs
            )

            print(f"âœ… é¢„æµ‹å®Œæˆï¼Œç”Ÿæˆ {len(prediction_result.predicted_numbers)} ä¸ªå·ç ")
            print(f"é¢„æµ‹å·ç : {prediction_result.predicted_numbers}")
            print("ğŸ’¡ æç¤º: è¿™æ˜¯æœªæ¥æœŸå·é¢„æµ‹ï¼Œæ— æ³•è¿›è¡Œå‡†ç¡®æ€§éªŒè¯")

            return {
                'prediction_result': prediction_result,
                'comparison_result': None,
                'mode': mode,
                'mode_description': mode_description,
                'success': True
            }

    def _check_issue_exists(self, target_issue: str) -> bool:
        """æ£€æŸ¥ç›®æ ‡æœŸå·æ˜¯å¦å­˜åœ¨äºå†å²æ•°æ®ä¸­"""
        try:
            actual_result = self.data_manager.get_issue_result(target_issue)
            return actual_result is not None
        except Exception:
            return False
    
    def compare_results(self, 
                       target_issue: str,
                       predicted_numbers: List[int]) -> ComparisonResult:
        """å¯¹æ¯”é¢„æµ‹ç»“æœ"""
        
        # è·å–å¼€å¥–ç»“æœ
        actual_result = self.data_manager.get_issue_result(target_issue)
        if not actual_result:
            raise ValueError(f"æœªæ‰¾åˆ°æœŸå· {target_issue} çš„å¼€å¥–ç»“æœ")
        
        # æ‰§è¡Œå¯¹æ¯”
        comparison = self.comparison_engine.compare(
            target_issue=target_issue,
            predicted_numbers=predicted_numbers,
            actual_numbers=actual_result.numbers
        )
        
        # æ›´æ–°æ€§èƒ½è®°å½•
        method = getattr(self, '_last_prediction_method', 'unknown')
        self.performance_monitor.record_prediction(
            method, 0, comparison.hit_rate
        )
        
        return comparison
    
    def analyze_statistics(self, data: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†ææ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        if data.empty:
            return {}

        stats = {}

        # åŸºæœ¬ç»Ÿè®¡
        stats['total_periods'] = len(data)
        stats['date_range'] = {
            'start': data['issue'].iloc[0] if len(data) > 0 else None,
            'end': data['issue'].iloc[-1] if len(data) > 0 else None
        }

        # å·ç é¢‘ç‡ç»Ÿè®¡
        all_numbers = []
        for _, row in data.iterrows():
            numbers = [row[f'num{i}'] for i in range(1, 21)]
            all_numbers.extend(numbers)

        from collections import Counter
        number_freq = Counter(all_numbers)
        stats['number_frequency'] = dict(number_freq.most_common(10))

        # åŒºåŸŸåˆ†å¸ƒç»Ÿè®¡
        stats['zone_distribution'] = ZoneAnalyzer.analyze_zone_distribution(data)

        # å’Œå€¼åˆ†å¸ƒç»Ÿè®¡
        stats['sum_distribution'] = SumAnalyzer.analyze_sum_distribution(data)

        # å†·çƒ­å·ç»Ÿè®¡
        hot_numbers = [num for num, freq in number_freq.most_common(10)]
        cold_numbers = [num for num, freq in number_freq.most_common()[-10:]]
        stats['hot_numbers'] = hot_numbers
        stats['cold_numbers'] = cold_numbers

        return stats

    def analyze_and_predict(self,
                           target_issue: str,
                           periods: int = 300,
                           count: int = 30,
                           method: str = 'frequency',
                           **kwargs) -> Tuple[PredictionResult, ComparisonResult]:
        """åˆ†æé¢„æµ‹å¹¶å¯¹æ¯”ç»“æœ"""
        
        # è®°å½•é¢„æµ‹æ–¹æ³•
        self._last_prediction_method = method
        
        # æ‰§è¡Œé¢„æµ‹
        prediction_result = self.predict(
            target_issue=target_issue,
            periods=periods,
            count=count,
            method=method,
            **kwargs
        )
        
        # å¯¹æ¯”ç»“æœ
        comparison_result = self.compare_results(
            target_issue=target_issue,
            predicted_numbers=prediction_result.predicted_numbers
        )
        
        return prediction_result, comparison_result
    
    def get_available_methods(self) -> List[str]:
        """è·å–å¯ç”¨çš„é¢„æµ‹æ–¹æ³•"""
        return self.prediction_engine.get_available_methods()
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        return self.performance_monitor.get_performance_summary()
    
    # æ•°å­—å¯¹é¢‘ç‡åˆ†ææ–¹æ³•
    def analyze_pair_frequency(
        self, 
        target_issue: str, 
        period_count: int,
        use_cache: bool = True
    ) -> PairFrequencyResult:
        """
        åˆ†ææ•°å­—å¯¹é¢‘ç‡
        
        Args:
            target_issue: ç›®æ ‡æœŸå·ï¼ˆå¦‚"2025238"ï¼‰
            period_count: ç»Ÿè®¡æœŸæ•°
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            å®Œæ•´çš„åˆ†æç»“æœ
            
        Example:
            >>> analyzer = Happy8Analyzer()
            >>> result = analyzer.analyze_pair_frequency("2025238", 20)
            >>> print(f"åˆ†æäº†{result.actual_periods}æœŸæ•°æ®")
        """
        return self.pair_frequency_analyzer.analyze_pair_frequency(
            target_issue, period_count, use_cache
        )
    
    def batch_analyze_pair_frequency(
        self, 
        requests: List[Tuple[str, int]], 
        use_cache: bool = True
    ) -> List[PairFrequencyResult]:
        """
        æ‰¹é‡åˆ†ææ•°å­—å¯¹é¢‘ç‡
        
        Args:
            requests: è¯·æ±‚åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º(target_issue, period_count)
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            åˆ†æç»“æœåˆ—è¡¨
        """
        return self.pair_frequency_analyzer.batch_analyze(requests, use_cache)
    
    def get_top_pairs_across_periods(
        self, 
        target_issue: str, 
        period_counts: List[int], 
        top_n: int = 10
    ) -> Dict[int, List[PairFrequencyItem]]:
        """
        è·å–ä¸åŒæœŸæ•°ä¸‹çš„å‰Nä¸ªé«˜é¢‘æ•°å­—å¯¹
        
        Args:
            target_issue: ç›®æ ‡æœŸå·
            period_counts: æœŸæ•°åˆ—è¡¨
            top_n: è¿”å›å‰Nä¸ªæ•°å­—å¯¹
            
        Returns:
            å­—å…¸ï¼Œé”®ä¸ºæœŸæ•°ï¼Œå€¼ä¸ºå‰Nä¸ªæ•°å­—å¯¹åˆ—è¡¨
        """
        return self.pair_frequency_analyzer.get_top_pairs_across_periods(
            target_issue, period_counts, top_n
        )
    
    def find_consistent_pairs(
        self, 
        target_issue: str, 
        period_counts: List[int], 
        min_frequency: float = 30.0
    ) -> List[Tuple[int, int]]:
        """
        æŸ¥æ‰¾åœ¨ä¸åŒæœŸæ•°ä¸‹éƒ½ä¿æŒé«˜é¢‘çš„æ•°å­—å¯¹
        
        Args:
            target_issue: ç›®æ ‡æœŸå·
            period_counts: æœŸæ•°åˆ—è¡¨
            min_frequency: æœ€å°é¢‘ç‡ç™¾åˆ†æ¯”
            
        Returns:
            ä¸€è‡´é«˜é¢‘çš„æ•°å­—å¯¹åˆ—è¡¨
        """
        return self.pair_frequency_analyzer.find_consistent_pairs(
            target_issue, period_counts, min_frequency
        )
    
    def clear_pair_frequency_cache(self):
        """æ¸…ç©ºæ•°å­—å¯¹é¢‘ç‡åˆ†æç¼“å­˜"""
        self.pair_frequency_analyzer.clear_cache()
    
    def get_pair_frequency_cache_info(self) -> Dict[str, Any]:
        """è·å–æ•°å­—å¯¹é¢‘ç‡åˆ†æç¼“å­˜ä¿¡æ¯"""
        return self.pair_frequency_analyzer.get_cache_info()


class Happy8CLI:
    """å¿«ä¹8å‘½ä»¤è¡Œç•Œé¢"""
    
    def __init__(self):
        self.analyzer = Happy8Analyzer()
        self.parser = self._create_parser()
    
    def _create_parser(self):
        """åˆ›å»ºå‘½ä»¤è¡Œè§£æå™¨"""
        parser = argparse.ArgumentParser(
            description="å¿«ä¹8æ™ºèƒ½é¢„æµ‹ç³»ç»Ÿ",
            formatter_class=argparse.RawDescriptionHelpFormatter,
            epilog="""
ç¤ºä¾‹ç”¨æ³•:
  %(prog)s crawl --count 1000
  %(prog)s predict --target 2025238 --periods 300 --count 30 --method frequency
  %(prog)s compare --target 2025238 --periods 300 --count 30 --method ensemble
            """
        )
        
        subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
        
        # æ•°æ®ç®¡ç†å‘½ä»¤
        crawl_parser = subparsers.add_parser('crawl', help='çˆ¬å–å†å²æ•°æ®')
        crawl_parser.add_argument('--count', type=int, default=1000, help='çˆ¬å–æœŸæ•°')
        
        update_parser = subparsers.add_parser('update', help='æ›´æ–°æœ€æ–°æ•°æ®')
        
        validate_parser = subparsers.add_parser('validate', help='éªŒè¯æ•°æ®å®Œæ•´æ€§')
        
        # é¢„æµ‹å‘½ä»¤
        predict_parser = subparsers.add_parser('predict', help='æ‰§è¡Œé¢„æµ‹')
        predict_parser.add_argument('--target', required=True, help='ç›®æ ‡æœŸå·')
        predict_parser.add_argument('--periods', type=int, default=300, help='åˆ†ææœŸæ•°')
        predict_parser.add_argument('--count', type=int, default=30, help='ç”Ÿæˆå·ç æ•°')
        predict_parser.add_argument('--method', default='frequency', 
                                   choices=['frequency', 'hot_cold', 'markov', 'lstm', 'ensemble'],
                                   help='é¢„æµ‹æ–¹æ³•')
        predict_parser.add_argument('--explain', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†è¿‡ç¨‹')
        
        # å¯¹æ¯”å‘½ä»¤
        compare_parser = subparsers.add_parser('compare', help='é¢„æµ‹å¹¶å¯¹æ¯”ç»“æœ')
        compare_parser.add_argument('--target', required=True, help='ç›®æ ‡æœŸå·')
        compare_parser.add_argument('--periods', type=int, default=300, help='åˆ†ææœŸæ•°')
        compare_parser.add_argument('--count', type=int, default=30, help='ç”Ÿæˆå·ç æ•°')
        compare_parser.add_argument('--method', default='frequency',
                                   choices=['frequency', 'hot_cold', 'markov', 'lstm', 'ensemble'],
                                   help='é¢„æµ‹æ–¹æ³•')
        compare_parser.add_argument('--output', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
        
        return parser
    
    def run(self, args=None):
        """è¿è¡ŒCLI"""
        args = self.parser.parse_args(args)
        
        if args.command == 'crawl':
            self._handle_crawl(args)
        elif args.command == 'update':
            self._handle_update(args)
        elif args.command == 'validate':
            self._handle_validate(args)
        elif args.command == 'predict':
            self._handle_predict(args)
        elif args.command == 'compare':
            self._handle_compare(args)
        else:
            self.parser.print_help()
    
    def _handle_crawl(self, args):
        """å¤„ç†çˆ¬å–å‘½ä»¤"""
        print(f"å¼€å§‹çˆ¬å– {args.count} æœŸå†å²æ•°æ®...")
        self.analyzer.data_manager.crawl_initial_data(args.count)
        print("æ•°æ®çˆ¬å–å®Œæˆ!")
    
    def _handle_update(self, args):
        """å¤„ç†æ›´æ–°å‘½ä»¤"""
        print("æ›´æ–°æœ€æ–°æ•°æ®...")
        # è¿™é‡Œå¯ä»¥å®ç°æ•°æ®æ›´æ–°é€»è¾‘
        print("æ•°æ®æ›´æ–°å®Œæˆ!")
    
    def _handle_validate(self, args):
        """å¤„ç†éªŒè¯å‘½ä»¤"""
        print("éªŒè¯æ•°æ®å®Œæ•´æ€§...")
        data = self.analyzer.load_data()
        validation_result = self.analyzer.data_manager.validator.validate_happy8_data(data)
        
        print(f"éªŒè¯ç»“æœ:")
        print(f"- æ€»è®°å½•æ•°: {validation_result['total_records']}")
        print(f"- é‡å¤æœŸå·: {validation_result['duplicate_issues']}")
        print(f"- æ— æ•ˆå·ç èŒƒå›´: {validation_result['invalid_ranges']}")
        print(f"- æ— æ•ˆå·ç æ•°é‡: {validation_result['invalid_number_counts']}")
        
        if validation_result['errors']:
            print(f"- é”™è¯¯: {validation_result['errors']}")
        else:
            print("- æ•°æ®éªŒè¯é€šè¿‡!")
    
    def _handle_predict(self, args):
        """å¤„ç†é¢„æµ‹å‘½ä»¤"""
        print("å¿«ä¹8æ™ºèƒ½é¢„æµ‹ç³»ç»Ÿ")
        print("=" * 50)
        print()
        
        print("é¢„æµ‹å‚æ•°:")
        print(f"- ç›®æ ‡æœŸå·: {args.target}")
        print(f"- åˆ†ææœŸæ•°: {args.periods}æœŸ")
        print(f"- ç”Ÿæˆæ•°é‡: {args.count}ä¸ªå·ç ")
        print(f"- é¢„æµ‹æ–¹æ³•: {args.method}")
        print()
        
        try:
            # æ‰§è¡Œé¢„æµ‹
            print("æ­£åœ¨æ‰§è¡Œé¢„æµ‹... ", end="", flush=True)
            result = self.analyzer.predict(
                target_issue=args.target,
                periods=args.periods,
                count=args.count,
                method=args.method
            )
            print("âœ“")
            
            # æ˜¾ç¤ºç»“æœ
            self._display_prediction_result(result)
            
        except Exception as e:
            print(f"âœ—\né”™è¯¯: {str(e)}")
    
    def _handle_compare(self, args):
        """å¤„ç†å¯¹æ¯”å‘½ä»¤"""
        print("å¿«ä¹8æ™ºèƒ½é¢„æµ‹ç³»ç»Ÿ")
        print("=" * 50)
        print()
        
        print("é¢„æµ‹å‚æ•°:")
        print(f"- ç›®æ ‡æœŸå·: {args.target}")
        print(f"- åˆ†ææœŸæ•°: {args.periods}æœŸ")
        print(f"- ç”Ÿæˆæ•°é‡: {args.count}ä¸ªå·ç ")
        print(f"- é¢„æµ‹æ–¹æ³•: {args.method}")
        print()
        
        try:
            # æ‰§è¡Œé¢„æµ‹å’Œå¯¹æ¯”
            print("æ­£åœ¨æ‰§è¡Œé¢„æµ‹å’Œå¯¹æ¯”... ", end="", flush=True)
            prediction_result, comparison_result = self.analyzer.analyze_and_predict(
                target_issue=args.target,
                periods=args.periods,
                count=args.count,
                method=args.method
            )
            print("âœ“")
            
            # æ˜¾ç¤ºç»“æœ
            self._display_comparison_results(prediction_result, comparison_result)
            
            # ä¿å­˜ç»“æœ
            if args.output:
                self._save_results(prediction_result, comparison_result, args.output)
                print(f"\nç»“æœå·²ä¿å­˜åˆ°: {args.output}")
            
        except Exception as e:
            print(f"âœ—\né”™è¯¯: {str(e)}")
    
    def _display_prediction_result(self, result: PredictionResult):
        """æ˜¾ç¤ºé¢„æµ‹ç»“æœ"""
        print("\né¢„æµ‹ç»“æœ:")
        print("=" * 50)
        
        # é¢„æµ‹å·ç 
        predicted_numbers = result.predicted_numbers
        print(f"é¢„æµ‹å·ç  ({len(predicted_numbers)}ä¸ª):")
        
        # æŒ‰è¡Œæ˜¾ç¤ºï¼Œæ¯è¡Œ10ä¸ª
        for i in range(0, len(predicted_numbers), 10):
            line_numbers = predicted_numbers[i:i+10]
            formatted_numbers = [f"{num:02d}" for num in line_numbers]
            print(" ".join(formatted_numbers))
        
        print(f"\né¢„æµ‹å®Œæˆ! ç”¨æ—¶: {result.execution_time:.2f}ç§’")
    
    def _display_comparison_results(self, prediction_result: PredictionResult, comparison_result: ComparisonResult):
        """æ˜¾ç¤ºå¯¹æ¯”ç»“æœ"""
        print("\né¢„æµ‹ç»“æœ:")
        print("=" * 50)
        
        # é¢„æµ‹å·ç 
        predicted_numbers = prediction_result.predicted_numbers
        print(f"é¢„æµ‹å·ç  ({len(predicted_numbers)}ä¸ª):")
        
        for i in range(0, len(predicted_numbers), 10):
            line_numbers = predicted_numbers[i:i+10]
            formatted_numbers = []
            
            for num in line_numbers:
                if num in comparison_result.hit_numbers:
                    formatted_numbers.append(f"\033[91m[{num:02d}]\033[0m")  # çº¢è‰²æ ‡è®°
                else:
                    formatted_numbers.append(f"{num:02d}")
            
            print(" ".join(formatted_numbers))
        
        print()
        
        # å¼€å¥–å·ç 
        actual_numbers = comparison_result.actual_numbers
        print(f"å¼€å¥–å·ç  ({len(actual_numbers)}ä¸ª):")
        
        for i in range(0, len(actual_numbers), 10):
            line_numbers = actual_numbers[i:i+10]
            formatted_numbers = [f"\033[92m[{num:02d}]\033[0m" for num in line_numbers]  # ç»¿è‰²
            print(" ".join(formatted_numbers))
        
        print()
        
        # å‘½ä¸­åˆ†æ
        print("å‘½ä¸­åˆ†æ:")
        print("=" * 50)
        hit_numbers_str = " ".join([f"\033[91m{num:02d}\033[0m" for num in sorted(comparison_result.hit_numbers)])
        print(f"å‘½ä¸­å·ç : {hit_numbers_str}")
        print(f"å‘½ä¸­æ•°é‡: {comparison_result.hit_count}/{len(predicted_numbers)}")
        print(f"å‘½ä¸­ç‡: {comparison_result.hit_rate:.2%}")
        
        # è¯¦ç»†åˆ†æ
        self._display_detailed_analysis(comparison_result)
        
        print(f"\né¢„æµ‹å®Œæˆ! ç”¨æ—¶: {prediction_result.execution_time:.2f}ç§’")
    
    def _display_detailed_analysis(self, comparison_result: ComparisonResult):
        """æ˜¾ç¤ºè¯¦ç»†åˆ†æ"""
        hit_numbers = comparison_result.hit_numbers
        distribution = comparison_result.hit_distribution
        
        print("\nè¯¦ç»†åˆ†æ:")
        print(f"- å°å·å‘½ä¸­: {distribution.get('small_numbers', 0)}ä¸ª (1-40å·æ®µ)")
        print(f"- å¤§å·å‘½ä¸­: {distribution.get('big_numbers', 0)}ä¸ª (41-80å·æ®µ)")
        print(f"- å¥‡æ•°å‘½ä¸­: {distribution.get('odd_numbers', 0)}ä¸ª")
        print(f"- å¶æ•°å‘½ä¸­: {distribution.get('even_numbers', 0)}ä¸ª")
        
        # åŒºåŸŸåˆ†å¸ƒ
        zone_hits = [distribution.get(f'zone_{i}', 0) for i in range(1, 9)]
        print(f"- å„åŒºåŸŸå‘½ä¸­åˆ†å¸ƒ: {zone_hits}")
    
    def _save_results(self, prediction_result: PredictionResult, comparison_result: ComparisonResult, output_path: str):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        results = {
            'prediction': asdict(prediction_result),
            'comparison': asdict(comparison_result)
        }
        
        # å¤„ç†datetimeå¯¹è±¡
        results['prediction']['generation_time'] = results['prediction']['generation_time'].isoformat()
        results['comparison']['comparison_time'] = results['comparison']['comparison_time'].isoformat()
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)


def main():
    """ä¸»å‡½æ•°"""
    cli = Happy8CLI()
    cli.run()


if __name__ == "__main__":
    main()
